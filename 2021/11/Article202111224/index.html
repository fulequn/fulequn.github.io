<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="本小节我们将使用 RNN 完成图像说明( Image Captioning)任务。该任务需要联合 CNN 与 RNN 一同学习，CNN 用于提取图像特征，RNN 用于生成图像特征所对应的说明文字。 在训练阶段，首先我们将图片放入卷积网络中进行特征提取，将其作为隐藏层h0的输入；然后将图片对应的文字描述一个单词接一个单词的输入到 RNN 中，而 RNN 输出则是当前单词的下一个预测单词。 在测试阶段"><meta property="og:type" content="article"><meta property="og:title" content="《深度学习实战》第7章 循环神经网络"><meta property="og:url" content="https://fulequn.github.io/2021/11/Article202111224/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="本小节我们将使用 RNN 完成图像说明( Image Captioning)任务。该任务需要联合 CNN 与 RNN 一同学习，CNN 用于提取图像特征，RNN 用于生成图像特征所对应的说明文字。 在训练阶段，首先我们将图片放入卷积网络中进行特征提取，将其作为隐藏层h0的输入；然后将图片对应的文字描述一个单词接一个单词的输入到 RNN 中，而 RNN 输出则是当前单词的下一个预测单词。 在测试阶段"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957175.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957176.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957177.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957178.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957179.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271952187.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957180.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957181.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957182.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271951942.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271951359.png"><meta property="article:published_time" content="2021-11-22T05:26:14.000Z"><meta property="article:modified_time" content="2024-05-18T14:35:07.683Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957175.png"><title>《深度学习实战》第7章 循环神经网络 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="《深度学习实战》第7章 循环神经网络"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-11-22 13:26" pubdate>2021年11月22日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>4.8k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>41 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">《深度学习实战》第7章 循环神经网络</h1><div class="markdown-body"><p>本小节我们将使用 RNN 完成图像说明( Image Captioning)任务。该任务需要联合 CNN 与 RNN 一同学习，CNN 用于提取图像特征，RNN 用于生成图像特征所对应的说明文字。</p><p>在训练阶段，首先我们将图片放入卷积网络中进行特征提取，将其作为隐藏层h0的输入；然后将图片对应的文字描述一个单词接一个单词的输入到 RNN 中，而 RNN 输出则是当前单词的下一个预测单词。</p><p>在测试阶段，我们将 RNN 预测的当前单词，作为下一时间片段的输入数据输入到 RNN 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> time, os, json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> classifiers.chapter7 <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> *<br><br>%matplotlib inline<br>plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] =(<span class="hljs-number">10.0</span>, <span class="hljs-number">8.0</span>) <br>plt.rcParams[<span class="hljs-string">&#x27;image.interpolation&#x27;</span>] = <span class="hljs-string">&#x27;nearest&#x27;</span><br>plt.rcParams[<span class="hljs-string">&#x27;image.cmap&#x27;</span>] = <span class="hljs-string">&#x27;gray&#x27;</span><br>%load_ext autoreload<br>%autoreload <span class="hljs-number">2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rel_error</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;相对误差&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">max</span>(np.<span class="hljs-built_in">abs</span>(x - y) /(np.maximum(<span class="hljs-number">1e-8</span>, np.<span class="hljs-built_in">abs</span>(x) + np.<span class="hljs-built_in">abs</span>(y))))<br></code></pre></td></tr></table></figure><h3 id="microsoft-coco"><a class="markdownIt-Anchor" href="#microsoft-coco"></a> Microsoft COCO</h3><p>本练习将使用2014年发布的 <a target="_blank" rel="noopener" href="http://mscoco.org/">Microsoft COCO dataset</a>，其作为标准的图像说明测试数据，包含有80000个训练数据以及40000验证数据，使用该网址&quot;<a target="_blank" rel="noopener" href="http://cs231n.stanford.edu/coco_captioning.zip">http://cs231n.stanford.edu/coco_captioning.zip</a>&quot; 下载数据集，下载完毕后解压到datasets文件目录。</p><p>该套数据集已经对数据使用VGG-16卷积网络进行了特征提取，分别存放在<code>train2014_vgg16_fc7.h5</code>和<code>val2014_vgg16_fc7.h5</code></p><p>如果想要缩短运行时间，其也将特征从4096降到了512维，分别存放在<code>train2014_vgg16_fc7_pca.h5</code>以及<code>val2014_vgg16_fc7_pca.h5</code>中</p><p>原始图像大约有20GB，你可以使用<code>train2014_urls.txt</code>和<code>val2014_urls.txt</code>访问(需要翻墙)。</p><p>直接处理字符串非常不高效，因此我们给每个词分配了字典ID方便我们查阅使用。因此，输入的序列数据其实是一条字典ID串，该文件存放在 <code>coco2014_vocab.json</code>。你需要使用<code>coco_utils.py</code>文件中的 <code>decode_captions</code>函数将ID转换为词向量。</p><p>在训练RNN前，你需要使用记号 <code>&lt;START&gt;</code> 和 <code>&lt;END&gt;</code>标记句子的开头与结束。对于一些不常用的词我们使用<code>&lt;UNK&gt;</code> 进行替换。为了使句子长度相同，对于短句子，我们在 <code>&lt;END&gt;</code>标记后，填充<code>&lt;NULL&gt;</code> ，在训练时，不计算<code>&lt;NULL&gt;</code> 的损失与梯度。</p><p>以上内容可能过于繁杂，我们已经默默处理好了，你知道即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">data = load_coco_data(pca_features=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> data.items():<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(v) == np.ndarray:<br>        <span class="hljs-built_in">print</span>(k, <span class="hljs-built_in">type</span>(v), v.shape, v.dtype)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(k, <span class="hljs-built_in">type</span>(v), <span class="hljs-built_in">len</span>(v))<br></code></pre></td></tr></table></figure><pre><code class="hljs">train_captions &lt;class 'numpy.ndarray'&gt; (400135, 17) int32
train_image_idxs &lt;class 'numpy.ndarray'&gt; (400135,) int32
val_captions &lt;class 'numpy.ndarray'&gt; (195954, 17) int32
val_image_idxs &lt;class 'numpy.ndarray'&gt; (195954,) int32
train_features &lt;class 'numpy.ndarray'&gt; (82783, 512) float32
val_features &lt;class 'numpy.ndarray'&gt; (40504, 512) float32
idx_to_word &lt;class 'list'&gt; 1004
word_to_idx &lt;class 'dict'&gt; 1004
train_urls &lt;class 'numpy.ndarray'&gt; (82783,) &lt;U63
val_urls &lt;class 'numpy.ndarray'&gt; (40504,) &lt;U63
</code></pre><h3 id="rnn单步前向传播"><a class="markdownIt-Anchor" href="#rnn单步前向传播"></a> RNN单步前向传播</h3><p>打开 <code>classifiers\chapter7\rnn_layers.py</code>文件。完成<code>rnn_step_forward</code>函数，实现RNN的单步前向传播。</p><p>完成后运行下列代码，检验你的实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, H = <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">4</span><br><br>x = np.linspace(-<span class="hljs-number">0.4</span>, <span class="hljs-number">0.7</span>, num=N*D).reshape(N, D)<br>prev_h = np.linspace(-<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>, num=N*H).reshape(N, H)<br>Wx = np.linspace(-<span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>, num=D*H).reshape(D, H)<br>Wh = np.linspace(-<span class="hljs-number">0.3</span>, <span class="hljs-number">0.7</span>, num=H*H).reshape(H, H)<br>b = np.linspace(-<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, num=H)<br><br>next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)<br>expected_next_h = np.asarray([<br>    [-<span class="hljs-number">0.58172089</span>, -<span class="hljs-number">0.50182032</span>, -<span class="hljs-number">0.41232771</span>, -<span class="hljs-number">0.31410098</span>],<br>    [ <span class="hljs-number">0.66854692</span>,    <span class="hljs-number">0.79562378</span>,    <span class="hljs-number">0.87755553</span>,    <span class="hljs-number">0.92795967</span>],<br>    [ <span class="hljs-number">0.97934501</span>,    <span class="hljs-number">0.99144213</span>,    <span class="hljs-number">0.99646691</span>,    <span class="hljs-number">0.99854353</span>]])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;next_h 误差: &#x27;</span>, rel_error(expected_next_h, next_h))<br></code></pre></td></tr></table></figure><pre><code class="hljs">next_h 误差:  6.292421426471037e-09
</code></pre><h3 id="rnn单步反向传播"><a class="markdownIt-Anchor" href="#rnn单步反向传播"></a> RNN单步反向传播</h3><p>打开 classifiers\chapter7\rnn_layers.py文件。完成<code>rnn_step_backward</code>函数，实现RNN的单步反向传播。</p><p>完成后运行下列代码，检验你的实现。你的相对误差应该小于<code>1e-8</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, H = <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span><br>x = np.random.randn(N, D)<br>h = np.random.randn(N, H)<br>Wx = np.random.randn(D, H)<br>Wh = np.random.randn(H, H)<br>b = np.random.randn(H)<br><br>out, cache = rnn_step_forward(x, h, Wx, Wh, b)<br><br>dnext_h = np.random.randn(*out.shape)<br><br><span class="hljs-comment"># 计算前向传播后的各个值</span><br>fx = <span class="hljs-keyword">lambda</span> x: rnn_step_forward(x, h, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fh = <span class="hljs-keyword">lambda</span> prev_h: rnn_step_forward(x, h, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWx = <span class="hljs-keyword">lambda</span> Wx: rnn_step_forward(x, h, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWh = <span class="hljs-keyword">lambda</span> Wh: rnn_step_forward(x, h, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fb = <span class="hljs-keyword">lambda</span> b: rnn_step_forward(x, h, Wx, Wh, b)[<span class="hljs-number">0</span>]<br><br>dx_num = eval_numerical_gradient_array(fx, x, dnext_h)<br>dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)<br>dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)<br>dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)<br>db_num = eval_numerical_gradient_array(fb, b, dnext_h)<br><br>dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dprev_h 误差: &#x27;</span>, rel_error(dprev_h_num, dprev_h))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWx 误差: &#x27;</span>, rel_error(dWx_num, dWx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWh 误差: &#x27;</span>, rel_error(dWh_num, dWh))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;db 误差: &#x27;</span>, rel_error(db_num, db))<br></code></pre></td></tr></table></figure><pre><code class="hljs">dx 误差:  1.310256557537144e-09
dprev_h 误差:  2.1172970274127977e-10
dWx 误差:  1.6514816613293213e-08
dWh 误差:  1.770717277127576e-09
db 误差:  3.152117259170483e-10
</code></pre><h3 id="rnn时序前向传播"><a class="markdownIt-Anchor" href="#rnn时序前向传播"></a> RNN时序前向传播</h3><p>现在你需要将单步RNN组合起来，实现完整的时序RNN前向传播过程</p><p>打开 <code>classifiers\chapter7\rnn_layers.py</code> ,完成 <code>rnn_forward</code>函数编码。你的相对误差应该要小于<code>1e-7</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">N, T, D, H = <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span><br><br>x = np.linspace(-<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, num=N*T*D).reshape(N, T, D)<br>h0 = np.linspace(-<span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>, num=N*H).reshape(N, H)<br>Wx = np.linspace(-<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, num=D*H).reshape(D, H)<br>Wh = np.linspace(-<span class="hljs-number">0.4</span>, <span class="hljs-number">0.1</span>, num=H*H).reshape(H, H)<br>b = np.linspace(-<span class="hljs-number">0.7</span>, <span class="hljs-number">0.1</span>, num=H)<br><br>h, _ = rnn_forward(x, h0, Wx, Wh, b)<br>expected_h = np.asarray([<br>    [<br>        [-<span class="hljs-number">0.42070749</span>, -<span class="hljs-number">0.27279261</span>, -<span class="hljs-number">0.11074945</span>,    <span class="hljs-number">0.05740409</span>,    <span class="hljs-number">0.22236251</span>],<br>        [-<span class="hljs-number">0.39525808</span>, -<span class="hljs-number">0.22554661</span>, -<span class="hljs-number">0.0409454</span>,     <span class="hljs-number">0.14649412</span>,    <span class="hljs-number">0.32397316</span>],<br>        [-<span class="hljs-number">0.42305111</span>, -<span class="hljs-number">0.24223728</span>, -<span class="hljs-number">0.04287027</span>,    <span class="hljs-number">0.15997045</span>,    <span class="hljs-number">0.35014525</span>],<br>    ],<br>    [<br>        [-<span class="hljs-number">0.55857474</span>, -<span class="hljs-number">0.39065825</span>, -<span class="hljs-number">0.19198182</span>,    <span class="hljs-number">0.02378408</span>,    <span class="hljs-number">0.23735671</span>],<br>        [-<span class="hljs-number">0.27150199</span>, -<span class="hljs-number">0.07088804</span>,    <span class="hljs-number">0.13562939</span>,    <span class="hljs-number">0.33099728</span>,    <span class="hljs-number">0.50158768</span>],<br>        [-<span class="hljs-number">0.51014825</span>, -<span class="hljs-number">0.30524429</span>, -<span class="hljs-number">0.06755202</span>,    <span class="hljs-number">0.17806392</span>,    <span class="hljs-number">0.40333043</span>]]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;h 误差: &#x27;</span>, rel_error(expected_h, h))<br></code></pre></td></tr></table></figure><pre><code class="hljs">h 误差:  7.728466180186066e-08
</code></pre><h3 id="rnn时序反向传播"><a class="markdownIt-Anchor" href="#rnn时序反向传播"></a> RNN时序反向传播</h3><p>现在你需要将单步RNN反向传播组合起来，实现完整的时序RNN反向传播过程</p><p>打开 <code>classifiers\chapter7\rnn_layers.py</code> ,完成 <code>rnn_backward</code> 函数编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, T, H = <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">5</span><br><br>x = np.random.randn(N, T, D)<br>h0 = np.random.randn(N, H)<br>Wx = np.random.randn(D, H)<br>Wh = np.random.randn(H, H)<br>b = np.random.randn(H)<br><br>out, cache = rnn_forward(x, h0, Wx, Wh, b)<br><br>dout = np.random.randn(*out.shape)<br><br>dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)<br><br>fx = <span class="hljs-keyword">lambda</span> x: rnn_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fh0 = <span class="hljs-keyword">lambda</span> h0: rnn_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWx = <span class="hljs-keyword">lambda</span> Wx: rnn_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWh = <span class="hljs-keyword">lambda</span> Wh: rnn_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fb = <span class="hljs-keyword">lambda</span> b: rnn_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br><br>dx_num = eval_numerical_gradient_array(fx, x, dout)<br>dh0_num = eval_numerical_gradient_array(fh0, h0, dout)<br>dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)<br>dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)<br>db_num = eval_numerical_gradient_array(fb, b, dout)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dh0 误差: &#x27;</span>, rel_error(dh0_num, dh0))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWx 误差: &#x27;</span>, rel_error(dWx_num, dWx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWh 误差: &#x27;</span>, rel_error(dWh_num, dWh))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;db 误差: &#x27;</span>, rel_error(db_num, db))<br></code></pre></td></tr></table></figure><pre><code class="hljs">dx 误差:  3.019690437635004e-09
dh0 误差:  2.1293713568630968e-10
dWx 误差:  1.227642349941746e-09
dWh 误差:  1.8455612502078363e-09
db 误差:  1.347211901044371e-10
</code></pre><h3 id="词嵌入前向传播"><a class="markdownIt-Anchor" href="#词嵌入前向传播"></a> 词嵌入前向传播</h3><p>现在，需要将时序数据（索引串）转换为词向量。</p><p>打开 <code>classifiers\chapter7\rnn_layers.py</code>文件，实现 <code>word_embedding_forward</code>函数，将单词索引，转化为词向量。</p><p>运行下列代码，你的实现误差大约在范围<code>1e-8</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">N, T, V, D = <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span><br><br>x = np.asarray([[<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>]])<br>W = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, num=V*D).reshape(V, D)<br><br>out, _ = word_embedding_forward(x, W)<br>expected_out = np.asarray([<br> [[ <span class="hljs-number">0.</span>,                    <span class="hljs-number">0.07142857</span>,    <span class="hljs-number">0.14285714</span>],<br>    [ <span class="hljs-number">0.64285714</span>,    <span class="hljs-number">0.71428571</span>,    <span class="hljs-number">0.78571429</span>],<br>    [ <span class="hljs-number">0.21428571</span>,    <span class="hljs-number">0.28571429</span>,    <span class="hljs-number">0.35714286</span>],<br>    [ <span class="hljs-number">0.42857143</span>,    <span class="hljs-number">0.5</span>,                 <span class="hljs-number">0.57142857</span>]],<br> [[ <span class="hljs-number">0.42857143</span>,    <span class="hljs-number">0.5</span>,                 <span class="hljs-number">0.57142857</span>],<br>    [ <span class="hljs-number">0.21428571</span>,    <span class="hljs-number">0.28571429</span>,    <span class="hljs-number">0.35714286</span>],<br>    [ <span class="hljs-number">0.</span>,                    <span class="hljs-number">0.07142857</span>,    <span class="hljs-number">0.14285714</span>],<br>    [ <span class="hljs-number">0.64285714</span>,    <span class="hljs-number">0.71428571</span>,    <span class="hljs-number">0.78571429</span>]]])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;out 误差: &#x27;</span>, rel_error(expected_out, out))<br></code></pre></td></tr></table></figure><pre><code class="hljs">out 误差:  1.0000000094736443e-08
</code></pre><h3 id="词嵌入反向传播"><a class="markdownIt-Anchor" href="#词嵌入反向传播"></a> 词嵌入反向传播</h3><p>实现 <code>word_embedding_backward</code>函数，完成词嵌入的反向传播。</p><p>运行下列梯度检验代码，你的误差应该小于<code>1e-11</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">N, T, V, D = <span class="hljs-number">50</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span><br><br>x = np.random.randint(V, size=(N, T))<br>W = np.random.randn(V, D)<br><br>out, cache = word_embedding_forward(x, W)<br>dout = np.random.randn(*out.shape)<br>dW = word_embedding_backward(dout, cache)<br><br>f = <span class="hljs-keyword">lambda</span> W: word_embedding_forward(x, W)[<span class="hljs-number">0</span>]<br>dW_num = eval_numerical_gradient_array(f, W, dout)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dW 误差: &#x27;</span>, rel_error(dW, dW_num))<br></code></pre></td></tr></table></figure><pre><code class="hljs">dW 误差:  3.2759708946363888e-12
</code></pre><h3 id="时序输出仿射层"><a class="markdownIt-Anchor" href="#时序输出仿射层"></a> 时序输出仿射层</h3><p>RNN隐藏层到输出层传播和前馈网络类似，只是其数据为(N,T,D)的三维数据。</p><p>我们已经将其前向传播，反向传播分别实现在了<code>temporal_affine_forward</code>和 <code>temporal_affine_backward</code>函数中。</p><p>打开<code>rnn_layers.py</code>文件，阅读相关代码，并运行下列代码进行检验</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">N, T, D, M = <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span><br>x = np.random.randn(N, T, D)<br>w = np.random.randn(D, M)<br>b = np.random.randn(M)<br><br>out, cache = temporal_affine_forward(x, w, b)<br><br>dout = np.random.randn(*out.shape)<br><br>fx = <span class="hljs-keyword">lambda</span> x: temporal_affine_forward(x, w, b)[<span class="hljs-number">0</span>]<br>fw = <span class="hljs-keyword">lambda</span> w: temporal_affine_forward(x, w, b)[<span class="hljs-number">0</span>]<br>fb = <span class="hljs-keyword">lambda</span> b: temporal_affine_forward(x, w, b)[<span class="hljs-number">0</span>]<br><br>dx_num = eval_numerical_gradient_array(fx, x, dout)<br>dw_num = eval_numerical_gradient_array(fw, w, dout)<br>db_num = eval_numerical_gradient_array(fb, b, dout)<br><br>dx, dw, db = temporal_affine_backward(dout, cache)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dw 误差: &#x27;</span>, rel_error(dw_num, dw))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;db 误差: &#x27;</span>, rel_error(db_num, db))<br></code></pre></td></tr></table></figure><pre><code class="hljs">dx 误差:  1.1680190237299396e-09
dw 误差:  1.195903277097059e-11
db 误差:  1.0561424698310812e-11
</code></pre><h3 id="时序softmax损失函数"><a class="markdownIt-Anchor" href="#时序softmax损失函数"></a> 时序Softmax损失函数</h3><p>在使用RNN进行语言建模时，每一时间步我们都会预测下一个单词对应单词字典中的单词得分索引。每一时间片段我们使用softmax作为损失函数，而整个序列的损失值，我们是将每一时间片段的损失值加起来取平均值。但句子的长度是不相同的，为了使输入对齐，我们填充<code>&lt;NULL&gt;</code>补全短句子。但这些填充的 <code>&lt;NULL&gt;</code>标记不应该计算在损失值或梯度值内，我们需要使用 <code>mask</code>进行过滤。我们已经将其写进<code>rnn_layers.py</code>文件中的 <code>temporal_softmax_loss</code> 函数，阅读即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">N, T, V = <span class="hljs-number">100</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">check_loss</span>(<span class="hljs-params">N, T, V, p</span>):<br>    x = <span class="hljs-number">0.001</span> * np.random.randn(N, T, V)<br>    y = np.random.randint(V, size=(N, T))<br>    mask = np.random.rand(N, T) &lt;= p<br>    <span class="hljs-built_in">print</span>(temporal_softmax_loss(x, y, mask)[<span class="hljs-number">0</span>])<br>    <br>check_loss(<span class="hljs-number">100</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1.0</span>)     <span class="hljs-comment"># 损失值大约为 2.3</span><br>check_loss(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1.0</span>)    <span class="hljs-comment"># 损失值大约为 23</span><br>check_loss(<span class="hljs-number">5000</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>) <span class="hljs-comment"># 损失值大约为 2.3</span><br><br>N, T, V = <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span><br><br>x = np.random.randn(N, T, V)<br>y = np.random.randint(V, size=(N, T))<br>mask =(np.random.rand(N, T) &gt; <span class="hljs-number">0.5</span>)<br><br>loss, dx = temporal_softmax_loss(x, y, mask, verbose=<span class="hljs-literal">False</span>)<br><br>dx_num = eval_numerical_gradient(<br>        <span class="hljs-keyword">lambda</span> x: temporal_softmax_loss(x, y, mask)[<span class="hljs-number">0</span>], x, verbose=<span class="hljs-literal">False</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx 误差: &#x27;</span>, rel_error(dx, dx_num))<br></code></pre></td></tr></table></figure><pre><code class="hljs">2.302582382261073
23.02576008490865
2.34310410100737
dx 误差:  5.811536038658937e-08
</code></pre><h2 id="rnn图像说明任务"><a class="markdownIt-Anchor" href="#rnn图像说明任务"></a> RNN图像说明任务</h2><p>我们已经实现了我所需的各项零件，接下来我们就用RNN完成图片说明任务。打开<code>rnn.py</code>文件，阅读<code>CaptioningRNN</code>类。你需要实现RNN的<code>loss</code>函数，以及测试阶段使用的<code>sample</code>函数。目前你只需实现<code>cell_type = 'rnn'</code>即可，之后再实现LSTM部分内容。</p><p>完成后运行下列代码，你的损失误差应该小于 <code>1e-10</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, W, H = <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span><br>word_to_idx = &#123;<span class="hljs-string">&#x27;&lt;NULL&gt;&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;cat&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;dog&#x27;</span>: <span class="hljs-number">3</span>&#125;<br>V = <span class="hljs-built_in">len</span>(word_to_idx)<br>T = <span class="hljs-number">13</span><br><br>model = CaptioningRNN(word_to_idx,<br>                    input_dim=D,<br>                    wordvec_dim=W,<br>                    hidden_dim=H,<br>                    cell_type=<span class="hljs-string">&#x27;rnn&#x27;</span>)<br><br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> model.params.items():<br>    model.params[k] = np.linspace(-<span class="hljs-number">1.4</span>, <span class="hljs-number">1.3</span>, num=v.size).reshape(*v.shape)<br><br>features = np.linspace(-<span class="hljs-number">1.5</span>, <span class="hljs-number">0.3</span>, num=(N * D)).reshape(N, D)<br>captions =(np.arange(N * T) % V).reshape(N, T)<br><br>loss, grads = model.loss(features, captions)<br>expected_loss = <span class="hljs-number">9.83235591003</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;损失: &#x27;</span>, loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;期望损失: &#x27;</span>, expected_loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;误差: &#x27;</span>, <span class="hljs-built_in">abs</span>(loss - expected_loss))<br></code></pre></td></tr></table></figure><pre><code class="hljs">损失:  9.832355910027388
期望损失:  9.83235591003
误差:  2.611244553918368e-12
</code></pre><p><code>CaptioningRNN</code> 梯度检验，期望误差在 <code>1e-7</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">2</span><br>timesteps = <span class="hljs-number">3</span><br>input_dim = <span class="hljs-number">4</span><br>wordvec_dim = <span class="hljs-number">5</span><br>hidden_dim = <span class="hljs-number">6</span><br>word_to_idx = &#123;<span class="hljs-string">&#x27;&lt;NULL&gt;&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;cat&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;dog&#x27;</span>: <span class="hljs-number">3</span>&#125;<br>vocab_size = <span class="hljs-built_in">len</span>(word_to_idx)<br><br>captions = np.random.randint(vocab_size, size=(batch_size,<br>                                                                                             timesteps))<br>features = np.random.randn(batch_size, input_dim)<br><br>model = CaptioningRNN(word_to_idx,<br>                    input_dim=input_dim,<br>                    wordvec_dim=wordvec_dim,<br>                    hidden_dim=hidden_dim,<br>                    cell_type=<span class="hljs-string">&#x27;rnn&#x27;</span><br>                )<br><br>loss, grads = model.loss(features, captions)<br><br><span class="hljs-keyword">for</span> param_name <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(grads):<br>    f = <span class="hljs-keyword">lambda</span> _: model.loss(features, captions)[<span class="hljs-number">0</span>]<br>    param_grad_num = eval_numerical_gradient(f, model.params[param_name], <br>                         verbose=<span class="hljs-literal">False</span>, h=<span class="hljs-number">1e-6</span>)<br>    e = rel_error(param_grad_num, grads[param_name])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s 相对误差: %e&#x27;</span> %(param_name, e))<br></code></pre></td></tr></table></figure><pre><code class="hljs">W_embed 相对误差: 4.599221e-09
W_proj 相对误差: 5.548484e-09
W_vocab 相对误差: 3.479308e-08
Wh 相对误差: 1.954925e-08
Wx 相对误差: 8.715990e-06
b 相对误差: 2.182615e-09
b_proj 相对误差: 4.525645e-09
b_vocab 相对误差: 1.879290e-09
</code></pre><h3 id="过拟合小量数据"><a class="markdownIt-Anchor" href="#过拟合小量数据"></a> 过拟合小量数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">small_data = load_coco_data(max_train=<span class="hljs-number">50</span>)<br><br>small_rnn_model = CaptioningRNN(<br>                    cell_type=<span class="hljs-string">&#x27;rnn&#x27;</span>,<br>                    word_to_idx=data[<span class="hljs-string">&#x27;word_to_idx&#x27;</span>],<br>                    input_dim=data[<span class="hljs-string">&#x27;train_features&#x27;</span>].shape[<span class="hljs-number">1</span>],<br>                    hidden_dim=<span class="hljs-number">512</span>,<br>                    wordvec_dim=<span class="hljs-number">256</span>,<br>                )<br><br>small_rnn_solver = CaptioningTrainer(small_rnn_model, small_data,<br>                     update_rule=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>                     num_epochs=<span class="hljs-number">50</span>,<br>                     batch_size=<span class="hljs-number">25</span>,<br>                     updater_config=&#123;<br>                         <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">5e-3</span>,<br>                     &#125;,<br>                     lr_decay=<span class="hljs-number">0.95</span>,<br>                     verbose=<span class="hljs-literal">True</span>, print_every=<span class="hljs-number">10</span>,<br>                 )<br><br>small_rnn_solver.train()<br><br><span class="hljs-comment"># Plot the training losses</span><br>plt.plot(small_rnn_solver.loss_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Iteration&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training loss history&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><pre><code class="hljs">(Iteration 1 / 100) loss: 87.278615
(Iteration 11 / 100) loss: 24.241030
(Iteration 21 / 100) loss: 6.001595
(Iteration 31 / 100) loss: 1.049757
(Iteration 41 / 100) loss: 0.638685
(Iteration 51 / 100) loss: 0.245997
(Iteration 61 / 100) loss: 0.199264
(Iteration 71 / 100) loss: 0.145133
(Iteration 81 / 100) loss: 0.130924
(Iteration 91 / 100) loss: 0.118513
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957175.png" srcset="/img/loading.gif" lazyload alt="png"></p><h2 id="lstm"><a class="markdownIt-Anchor" href="#lstm"></a> LSTM</h2><h3 id="lstm单步前向传播"><a class="markdownIt-Anchor" href="#lstm单步前向传播"></a> LSTM单步前向传播</h3><p>打开 <code>rnn_layers.py</code>文件，完成lstm_step_forward代码，实现LSTM的单步前向传播。</p><p>运行下列代码，你的实现误差应该在<code>1e-8</code>以内。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, H = <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span><br>x = np.linspace(-<span class="hljs-number">0.4</span>, <span class="hljs-number">1.2</span>, num=N*D).reshape(N, D)<br>prev_h = np.linspace(-<span class="hljs-number">0.3</span>, <span class="hljs-number">0.7</span>, num=N*H).reshape(N, H)<br>prev_c = np.linspace(-<span class="hljs-number">0.4</span>, <span class="hljs-number">0.9</span>, num=N*H).reshape(N, H)<br>Wx = np.linspace(-<span class="hljs-number">2.1</span>, <span class="hljs-number">1.3</span>, num=<span class="hljs-number">4</span>*D*H).reshape(D, <span class="hljs-number">4</span> * H)<br>Wh = np.linspace(-<span class="hljs-number">0.7</span>, <span class="hljs-number">2.2</span>, num=<span class="hljs-number">4</span>*H*H).reshape(H, <span class="hljs-number">4</span> * H)<br>b = np.linspace(<span class="hljs-number">0.3</span>, <span class="hljs-number">0.7</span>, num=<span class="hljs-number">4</span>*H)<br><br>next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)<br><br>expected_next_h = np.asarray([<br>        [ <span class="hljs-number">0.24635157</span>,    <span class="hljs-number">0.28610883</span>,    <span class="hljs-number">0.32240467</span>,    <span class="hljs-number">0.35525807</span>,    <span class="hljs-number">0.38474904</span>],<br>        [ <span class="hljs-number">0.49223563</span>,    <span class="hljs-number">0.55611431</span>,    <span class="hljs-number">0.61507696</span>,    <span class="hljs-number">0.66844003</span>,    <span class="hljs-number">0.7159181</span> ],<br>        [ <span class="hljs-number">0.56735664</span>,    <span class="hljs-number">0.66310127</span>,    <span class="hljs-number">0.74419266</span>,    <span class="hljs-number">0.80889665</span>,    <span class="hljs-number">0.858299</span>    ]])<br>expected_next_c = np.asarray([<br>        [ <span class="hljs-number">0.32986176</span>,    <span class="hljs-number">0.39145139</span>,    <span class="hljs-number">0.451556</span>,        <span class="hljs-number">0.51014116</span>,    <span class="hljs-number">0.56717407</span>],<br>        [ <span class="hljs-number">0.66382255</span>,    <span class="hljs-number">0.76674007</span>,    <span class="hljs-number">0.87195994</span>,    <span class="hljs-number">0.97902709</span>,    <span class="hljs-number">1.08751345</span>],<br>        [ <span class="hljs-number">0.74192008</span>,    <span class="hljs-number">0.90592151</span>,    <span class="hljs-number">1.07717006</span>,    <span class="hljs-number">1.25120233</span>,    <span class="hljs-number">1.42395676</span>]])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;next_h 误差: &#x27;</span>, rel_error(expected_next_h, next_h))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;next_c 误差: &#x27;</span>, rel_error(expected_next_c, next_c))<br></code></pre></td></tr></table></figure><pre><code class="hljs">next_h 误差:  5.7054131185818695e-09
next_c 误差:  5.8143123088804145e-09
</code></pre><h3 id="lstm单步反向传播"><a class="markdownIt-Anchor" href="#lstm单步反向传播"></a> LSTM:单步反向传播</h3><p>打开 rnn_layers.py文件，完成lstm_step_backward函数代码，实现LSTM的单步反向传播。</p><p>运行下列代码，你的期望误差应该在1e-8以内</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, H = <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span><br>x = np.random.randn(N, D)<br>prev_h = np.random.randn(N, H)<br>prev_c = np.random.randn(N, H)<br>Wx = np.random.randn(D, <span class="hljs-number">4</span> * H)<br>Wh = np.random.randn(H, <span class="hljs-number">4</span> * H)<br>b = np.random.randn(<span class="hljs-number">4</span> * H)<br><br>next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)<br><br>dnext_h = np.random.randn(*next_h.shape)<br>dnext_c = np.random.randn(*next_c.shape)<br><br>fx_h = <span class="hljs-keyword">lambda</span> x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fh_h = <span class="hljs-keyword">lambda</span> h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fc_h = <span class="hljs-keyword">lambda</span> c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWx_h = <span class="hljs-keyword">lambda</span> Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWh_h = <span class="hljs-keyword">lambda</span> Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fb_h = <span class="hljs-keyword">lambda</span> b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">0</span>]<br><br>fx_c = <span class="hljs-keyword">lambda</span> x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">1</span>]<br>fh_c = <span class="hljs-keyword">lambda</span> h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">1</span>]<br>fc_c = <span class="hljs-keyword">lambda</span> c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">1</span>]<br>fWx_c = <span class="hljs-keyword">lambda</span> Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">1</span>]<br>fWh_c = <span class="hljs-keyword">lambda</span> Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">1</span>]<br>fb_c = <span class="hljs-keyword">lambda</span> b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[<span class="hljs-number">1</span>]<br><br>num_grad = eval_numerical_gradient_array<br><br>dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)<br>dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)<br>dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)<br>dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)<br>dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)<br>db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)<br><br>dx, dh, dc, dWx, dWh, db = lstm_step_backward(dnext_h, dnext_c, cache)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dh 误差: &#x27;</span>, rel_error(dh_num, dh))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dc 误差: &#x27;</span>, rel_error(dc_num, dc))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWx 误差: &#x27;</span>, rel_error(dWx_num, dWx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWh 误差: &#x27;</span>, rel_error(dWh_num, dWh))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;db 误差: &#x27;</span>, rel_error(db_num, db))<br></code></pre></td></tr></table></figure><pre><code class="hljs">dx 误差:  2.36114504649104e-10
dh 误差:  5.784107694134561e-10
dc 误差:  6.99069012739093e-10
dWx 误差:  6.291017711941009e-08
dWh 误差:  4.4809579254128135e-09
db 误差:  2.551473660645273e-10
</code></pre><h3 id="lstm前向传播"><a class="markdownIt-Anchor" href="#lstm前向传播"></a> LSTM前向传播</h3><p>现在将单步传播组合起来，完成完整的时序传播。打开<code>rnn_layers.py</code> 文件，完成<code>lstm_forward</code>函数代码，实现LSTM的前向传播。</p><p>运行下列代码，你的实现误差应该在1e-7以内</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, H, T = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span><br>x = np.linspace(-<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>, num=N*T*D).reshape(N, T, D)<br>h0 = np.linspace(-<span class="hljs-number">0.4</span>, <span class="hljs-number">0.8</span>, num=N*H).reshape(N, H)<br>Wx = np.linspace(-<span class="hljs-number">0.2</span>, <span class="hljs-number">0.9</span>, num=<span class="hljs-number">4</span>*D*H).reshape(D, <span class="hljs-number">4</span> * H)<br>Wh = np.linspace(-<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>, num=<span class="hljs-number">4</span>*H*H).reshape(H, <span class="hljs-number">4</span> * H)<br>b = np.linspace(<span class="hljs-number">0.2</span>, <span class="hljs-number">0.7</span>, num=<span class="hljs-number">4</span>*H)<br><br>h, cache = lstm_forward(x, h0, Wx, Wh, b)<br><br>expected_h = np.asarray([<br> [[ <span class="hljs-number">0.01764008</span>,    <span class="hljs-number">0.01823233</span>,    <span class="hljs-number">0.01882671</span>,    <span class="hljs-number">0.0194232</span> ],<br>    [ <span class="hljs-number">0.11287491</span>,    <span class="hljs-number">0.12146228</span>,    <span class="hljs-number">0.13018446</span>,    <span class="hljs-number">0.13902939</span>],<br>    [ <span class="hljs-number">0.31358768</span>,    <span class="hljs-number">0.33338627</span>,    <span class="hljs-number">0.35304453</span>,    <span class="hljs-number">0.37250975</span>]],<br> [[ <span class="hljs-number">0.45767879</span>,    <span class="hljs-number">0.4761092</span>,     <span class="hljs-number">0.4936887</span>,     <span class="hljs-number">0.51041945</span>],<br>    [ <span class="hljs-number">0.6704845</span>,     <span class="hljs-number">0.69350089</span>,    <span class="hljs-number">0.71486014</span>,    <span class="hljs-number">0.7346449</span> ],<br>    [ <span class="hljs-number">0.81733511</span>,    <span class="hljs-number">0.83677871</span>,    <span class="hljs-number">0.85403753</span>,    <span class="hljs-number">0.86935314</span>]]])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;h 误差: &#x27;</span>, rel_error(expected_h, h))<br></code></pre></td></tr></table></figure><pre><code class="hljs">h 误差:  8.610537452106624e-08
</code></pre><h3 id="lstm反向传播"><a class="markdownIt-Anchor" href="#lstm反向传播"></a> LSTM反向传播</h3><p>现在将单步反向传播组合起来，完成完整的时序反向传播。打开<code>rnn_layers.py</code> 文件，完成<code>lstm_backward</code> 函数代码，实现LSTM的反向传播。</p><p>运行下列代码。你的误差大约在<code>1e-8</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, T, H = <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span><br><br>x = np.random.randn(N, T, D)<br>h0 = np.random.randn(N, H)<br>Wx = np.random.randn(D, <span class="hljs-number">4</span> * H)<br>Wh = np.random.randn(H, <span class="hljs-number">4</span> * H)<br>b = np.random.randn(<span class="hljs-number">4</span> * H)<br><br>out, cache = lstm_forward(x, h0, Wx, Wh, b)<br><br>dout = np.random.randn(*out.shape)<br><br>dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)<br><br>fx = <span class="hljs-keyword">lambda</span> x: lstm_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fh0 = <span class="hljs-keyword">lambda</span> h0: lstm_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWx = <span class="hljs-keyword">lambda</span> Wx: lstm_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fWh = <span class="hljs-keyword">lambda</span> Wh: lstm_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br>fb = <span class="hljs-keyword">lambda</span> b: lstm_forward(x, h0, Wx, Wh, b)[<span class="hljs-number">0</span>]<br><br>dx_num = eval_numerical_gradient_array(fx, x, dout)<br>dh0_num = eval_numerical_gradient_array(fh0, h0, dout)<br>dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)<br>dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)<br>db_num = eval_numerical_gradient_array(fb, b, dout)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dh0 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWx 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dWh 误差: &#x27;</span>, rel_error(dx_num, dx))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;db 误差: &#x27;</span>, rel_error(dx_num, dx))<br></code></pre></td></tr></table></figure><pre><code class="hljs">dx 误差:  7.278901660157261e-10
dh0 误差:  7.278901660157261e-10
dWx 误差:  7.278901660157261e-10
dWh 误差:  7.278901660157261e-10
db 误差:  7.278901660157261e-10
</code></pre><h3 id="lstm图片说明任务"><a class="markdownIt-Anchor" href="#lstm图片说明任务"></a> LSTM图片说明任务</h3><p>打开<code>rnn.py</code>文件，完成LSTM模式下的损失计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">N, D, W, H = <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span><br>word_to_idx = &#123;<span class="hljs-string">&#x27;&lt;NULL&gt;&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;cat&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;dog&#x27;</span>: <span class="hljs-number">3</span>&#125;<br>V = <span class="hljs-built_in">len</span>(word_to_idx)<br>T = <span class="hljs-number">13</span><br><br>model = CaptioningRNN(word_to_idx,<br>                    input_dim=D,<br>                    wordvec_dim=W,<br>                    hidden_dim=H,<br>                    cell_type=<span class="hljs-string">&#x27;lstm&#x27;</span>)<br><br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> model.params.items():<br>    model.params[k] = np.linspace(-<span class="hljs-number">1.4</span>, <span class="hljs-number">1.3</span>, num=v.size).reshape(*v.shape)<br><br>features = np.linspace(-<span class="hljs-number">0.5</span>, <span class="hljs-number">1.7</span>, num=N*D).reshape(N, D)<br>captions =(np.arange(N * T) % V).reshape(N, T)<br><br>loss, grads = model.loss(features, captions)<br>expected_loss = <span class="hljs-number">9.82445935443</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;损失值: &#x27;</span>, loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;期望损失值: &#x27;</span>, expected_loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;误差: &#x27;</span>, <span class="hljs-built_in">abs</span>(loss - expected_loss))<br></code></pre></td></tr></table></figure><pre><code class="hljs">损失值:  9.82445935443226
期望损失值:  9.82445935443
误差:  2.261302256556519e-12
</code></pre><h3 id="lstm-过拟合测试"><a class="markdownIt-Anchor" href="#lstm-过拟合测试"></a> LSTM 过拟合测试</h3><p>和RNN类似，我们测试LSTM能否在小数据集上过拟合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">small_data = load_coco_data(max_train=<span class="hljs-number">50</span>)<br><br>small_lstm_model = CaptioningRNN(<br>                    cell_type=<span class="hljs-string">&#x27;lstm&#x27;</span>,<br>                    word_to_idx=data[<span class="hljs-string">&#x27;word_to_idx&#x27;</span>],<br>                    input_dim=data[<span class="hljs-string">&#x27;train_features&#x27;</span>].shape[<span class="hljs-number">1</span>],<br>                    hidden_dim=<span class="hljs-number">512</span>,<br>                    wordvec_dim=<span class="hljs-number">256</span>)<br><br>small_lstm_solver = CaptioningTrainer(small_lstm_model, small_data,<br>                     update_rule=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>                     num_epochs=<span class="hljs-number">50</span>,<br>                     batch_size=<span class="hljs-number">25</span>,<br>                     updater_config=&#123;<br>                         <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">5e-3</span>,<br>                     &#125;,<br>                     lr_decay=<span class="hljs-number">0.995</span>,<br>                     verbose=<span class="hljs-literal">True</span>, print_every=<span class="hljs-number">10</span>,<br>                 )<br><br>small_lstm_solver.train()<br><br>plt.plot(small_lstm_solver.loss_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Iteration&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training loss history&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><pre><code class="hljs">(Iteration 1 / 100) loss: 77.184947
(Iteration 11 / 100) loss: 39.224173
(Iteration 21 / 100) loss: 26.396812
(Iteration 31 / 100) loss: 14.473610
(Iteration 41 / 100) loss: 4.766169
(Iteration 51 / 100) loss: 1.583464
(Iteration 61 / 100) loss: 1.577735
(Iteration 71 / 100) loss: 0.271194
(Iteration 81 / 100) loss: 0.206668
(Iteration 91 / 100) loss: 0.110635
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957176.png" srcset="/img/loading.gif" lazyload alt="png"></p><h3 id="lstm-test-time-sampling"><a class="markdownIt-Anchor" href="#lstm-test-time-sampling"></a> LSTM test-time sampling</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>]:<br>    minibatch = sample_coco_minibatch(small_data, split=split, batch_size=<span class="hljs-number">2</span>)<br>    gt_captions, features, urls = minibatch<br>    gt_captions = decode_captions(gt_captions, data[<span class="hljs-string">&#x27;idx_to_word&#x27;</span>])<br><br>    sample_captions = small_lstm_model.sample(features)<br>    sample_captions = decode_captions(sample_captions, data[<span class="hljs-string">&#x27;idx_to_word&#x27;</span>])<br><br>    <span class="hljs-keyword">for</span> gt_caption, sample_caption, url <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(gt_captions, sample_captions, urls):<br>        <span class="hljs-comment"># plt.imshow(image_from_url(url))</span><br>        <span class="hljs-built_in">print</span>(url)<br>        plt.title(<span class="hljs-string">&#x27;%s\n%s\nGT:%s&#x27;</span> %(split, sample_caption, gt_caption))<br>        plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>        plt.show()<br></code></pre></td></tr></table></figure><pre><code class="hljs">http://farm7.staticflickr.com/6178/6167892245_3ef7bfd6c2_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957177.png" srcset="/img/loading.gif" lazyload alt="png"></p><pre><code class="hljs">http://farm5.staticflickr.com/4121/4769212854_0bcba64aa7_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957178.png" srcset="/img/loading.gif" lazyload alt="png"></p><pre><code class="hljs">http://farm2.staticflickr.com/1112/596804848_afb6e350a8_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957179.png" srcset="/img/loading.gif" lazyload alt="png"></p><pre><code class="hljs">http://farm1.staticflickr.com/164/439147986_2486920a79_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271952187.png" srcset="/img/loading.gif" lazyload alt="png"></p><h3 id="训练一个好的图片说明任务"><a class="markdownIt-Anchor" href="#训练一个好的图片说明任务"></a> 训练一个好的图片说明任务</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#pass</span><br>small_data2 = load_coco_data(max_train=<span class="hljs-number">5000</span>)<br>good_lstm_model = CaptioningRNN(<br>                    cell_type=<span class="hljs-string">&#x27;lstm&#x27;</span>,<br>                    word_to_idx=data[<span class="hljs-string">&#x27;word_to_idx&#x27;</span>],<br>                    input_dim=data[<span class="hljs-string">&#x27;train_features&#x27;</span>].shape[<span class="hljs-number">1</span>],<br>                    hidden_dim=<span class="hljs-number">200</span>,<br>                    wordvec_dim=<span class="hljs-number">256</span> )<br><br>good_lstm_solver = CaptioningTrainer(good_lstm_model, small_data2,<br>                     update_rule=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>                     num_epochs=<span class="hljs-number">50</span>,<br>                     batch_size=<span class="hljs-number">100</span>,<br>                     updater_config=&#123;<br>                         <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">5e-3</span>,<br>                     &#125;,<br>                     lr_decay=<span class="hljs-number">0.995</span>,<br>                     verbose=<span class="hljs-literal">True</span>, print_every=<span class="hljs-number">50</span>,<br>                 )<br><br>good_lstm_solver.train()<br><br>plt.plot(good_lstm_solver.loss_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Iteration&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training loss history&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><pre><code class="hljs">(Iteration 1 / 2500) loss: 77.557809
(Iteration 51 / 2500) loss: 41.835524
(Iteration 101 / 2500) loss: 35.659107
(Iteration 151 / 2500) loss: 33.714988
(Iteration 201 / 2500) loss: 30.808365
(Iteration 251 / 2500) loss: 28.904012
(Iteration 301 / 2500) loss: 26.059836
(Iteration 351 / 2500) loss: 26.031326
(Iteration 401 / 2500) loss: 22.815424
(Iteration 451 / 2500) loss: 22.772759
(Iteration 501 / 2500) loss: 20.736977
(Iteration 551 / 2500) loss: 21.022939
(Iteration 601 / 2500) loss: 19.118450
(Iteration 651 / 2500) loss: 18.663928
(Iteration 701 / 2500) loss: 16.895750
(Iteration 751 / 2500) loss: 16.610176
(Iteration 801 / 2500) loss: 16.800854
(Iteration 851 / 2500) loss: 15.282215
(Iteration 901 / 2500) loss: 13.853283
(Iteration 951 / 2500) loss: 12.862395
(Iteration 1001 / 2500) loss: 12.577979
(Iteration 1051 / 2500) loss: 11.035210
(Iteration 1101 / 2500) loss: 11.835750
(Iteration 1151 / 2500) loss: 11.085219
(Iteration 1201 / 2500) loss: 10.444645
(Iteration 1251 / 2500) loss: 9.716379
(Iteration 1301 / 2500) loss: 10.622007
(Iteration 1351 / 2500) loss: 9.389702
(Iteration 1401 / 2500) loss: 8.864774
(Iteration 1451 / 2500) loss: 8.837836
(Iteration 1501 / 2500) loss: 8.383492
(Iteration 1551 / 2500) loss: 7.997636
(Iteration 1601 / 2500) loss: 8.565062
(Iteration 1651 / 2500) loss: 8.156050
(Iteration 1701 / 2500) loss: 7.169216
(Iteration 1751 / 2500) loss: 6.436919
(Iteration 1801 / 2500) loss: 7.325675
(Iteration 1851 / 2500) loss: 6.806780
(Iteration 1901 / 2500) loss: 6.385995
(Iteration 1951 / 2500) loss: 6.291843
(Iteration 2001 / 2500) loss: 4.958617
(Iteration 2051 / 2500) loss: 5.567452
(Iteration 2101 / 2500) loss: 5.913960
(Iteration 2151 / 2500) loss: 5.399012
(Iteration 2201 / 2500) loss: 4.924958
(Iteration 2251 / 2500) loss: 4.796707
(Iteration 2301 / 2500) loss: 5.224964
(Iteration 2351 / 2500) loss: 4.845715
(Iteration 2401 / 2500) loss: 4.922645
(Iteration 2451 / 2500) loss: 4.887911
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957180.png" srcset="/img/loading.gif" lazyload alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#pass</span><br><span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>]:<br>    minibatch = sample_coco_minibatch(small_data2, split=split, batch_size=<span class="hljs-number">2</span>)<br>    gt_captions, features, urls = minibatch<br>    gt_captions = decode_captions(gt_captions, data[<span class="hljs-string">&#x27;idx_to_word&#x27;</span>])<br><br>    sample_captions = good_lstm_model.sample(features)<br>    sample_captions = decode_captions(sample_captions, data[<span class="hljs-string">&#x27;idx_to_word&#x27;</span>])<br><br>    <span class="hljs-keyword">for</span> gt_caption, sample_caption, url <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(gt_captions, sample_captions, urls):<br>        <span class="hljs-comment">#plt.imshow(image_from_url(url))</span><br>        <span class="hljs-built_in">print</span>(url)<br>        plt.title(<span class="hljs-string">&#x27;%s\n%s\nGT:%s&#x27;</span> %(split, sample_caption, gt_caption))<br>        plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>        plt.show()<br></code></pre></td></tr></table></figure><pre><code class="hljs">http://farm9.staticflickr.com/8102/8510027814_acffff06cd_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957181.png" srcset="/img/loading.gif" lazyload alt="png"></p><pre><code class="hljs">http://farm8.staticflickr.com/7035/6510880243_4086cbb06e_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271957182.png" srcset="/img/loading.gif" lazyload alt="png"></p><pre><code class="hljs">http://farm6.staticflickr.com/5328/9362192917_5324bd3d74_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271951942.png" srcset="/img/loading.gif" lazyload alt="png"></p><pre><code class="hljs">http://farm4.staticflickr.com/3316/5816070088_dfdce60093_z.jpg
</code></pre><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111271951359.png" srcset="/img/loading.gif" lazyload alt="png"></p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>《深度学习实战》第7章 循环神经网络</div><div>https://fulequn.github.io/2021/11/Article202111224/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年11月22日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2021/11/Article202111225/" title="编码实现RNN以及LSTM"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">编码实现RNN以及LSTM</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/11/Article202111223/" title="录制动态帧视频在PR中音画不同步问题"><span class="hidden-mobile">录制动态帧视频在PR中音画不同步问题</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>