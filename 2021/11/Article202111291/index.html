<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="参考链接： https:&#x2F;&#x2F;mofanpy.com&#x2F;tutorials&#x2F;machine-learning&#x2F;reinforcement-learning&#x2F;intro-q-learning&#x2F;   第2章 Q-learning 强化学习中有名的算法，Q-learning。由第一章可知，Q-learning的分类是model-free，基于价值，单步更新，离线学习。  2.1 什么是Q-Learning"><meta property="og:type" content="article"><meta property="og:title" content="莫烦强化学习-Q Leaning"><meta property="og:url" content="https://fulequn.github.io/2021/11/Article202111291/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="参考链接： https:&#x2F;&#x2F;mofanpy.com&#x2F;tutorials&#x2F;machine-learning&#x2F;reinforcement-learning&#x2F;intro-q-learning&#x2F;   第2章 Q-learning 强化学习中有名的算法，Q-learning。由第一章可知，Q-learning的分类是model-free，基于价值，单步更新，离线学习。  2.1 什么是Q-Learning"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300843576.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300916525.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300920463.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300930021.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300934738.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202112021050182.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202112031036521.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202112031037484.png"><meta property="article:published_time" content="2021-11-29T02:41:59.000Z"><meta property="article:modified_time" content="2024-05-18T14:35:07.696Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="动态规划"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300843576.png"><title>莫烦强化学习-Q Leaning - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="莫烦强化学习-Q Leaning"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-11-29 10:41" pubdate>2021年11月29日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>4.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>37 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">莫烦强化学习-Q Leaning</h1><div class="markdown-body"><blockquote><p>参考链接：</p><p><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning/">https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning/</a></p></blockquote><h1 id="第2章-q-learning"><a class="markdownIt-Anchor" href="#第2章-q-learning"></a> 第2章 Q-learning</h1><p>强化学习中有名的算法，Q-learning。由第一章可知，Q-learning的分类是model-free，基于价值，单步更新，离线学习。</p><h2 id="21-什么是q-learning"><a class="markdownIt-Anchor" href="#21-什么是q-learning"></a> 2.1 什么是Q-Learning</h2><h3 id="211-行为准则"><a class="markdownIt-Anchor" href="#211-行为准则"></a> 2.1.1 行为准则</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300843576.png" srcset="/img/loading.gif" lazyload alt="1"></p><p>我们做事情都会有一个自己的行为准则, 比如小时候爸妈常说”不写完作业就不准看电视”。所以我们在 写作业的<strong>这种状态</strong>下, <strong>好的行为</strong>就是继续写作业, 直到写完它, 我们还可以得到奖励, <strong>不好的行为</strong>就是没写完就跑去看电视了, 被爸妈发现, 后果很严重。小时候这种事情做多了, 也就变成我们不可磨灭的记忆。这和我们要提到的 Q learning 有什么关系呢? 原来 <strong>Q learning 也是一个决策过程</strong>, 和小时候的这种情况差不多。我们举例说明.</p><p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视, 所以现在我们有两种选择 , 1, 继续写作业, 2, 跑去看电视. 因为以前没有被罚过, 所以我选看电视, 然后现在的状态变成了看电视, 我又选了继续看电视, 接着我还是看电视, 最后爸妈回家, 发现我没写完作业就去看电视了, 狠狠地惩罚了我一次, 我也深刻地记下了这一次经历, 并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为, 我们在看看 Q learning 根据很多这样的经历是如何来决策的吧。</p><h3 id="212-qlearning-决策"><a class="markdownIt-Anchor" href="#212-qlearning-决策"></a> 2.1.2 QLearning 决策</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300916525.png" srcset="/img/loading.gif" lazyload alt="1"></p><p>假设我们的行为准则已经学习好了, 现在我们处于状态s1, 我在写作业, 我有两个行为 a1, a2, 分别是看电视和写作业, 根据我的经验, 在这种 s1 状态下, a2 写作业带来的潜在奖励要比 a1 看电视高（比较不同决策的价值）, 这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替, 在我的记忆Q表格中, Q(s1, a1)=-2 要小于 Q(s1, a2)=1, 所以我们判断要选择 a2 作为下一个行为。现在我们的状态更新成 s2 , 我们还是有两个同样的选择, 重复上面的过程, 在行为准则Q 表中寻找 Q(s2, a1)和Q(s2, a2) 的值, 并比较他们的大小, 选取较大的一个。接着根据 a2 我们到达 s3 并在此重复上面的决策过程. Q learning 的方法也就是这样决策的。看完决策, 我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改, 提升的。</p><h3 id="213-qlearning-更新"><a class="markdownIt-Anchor" href="#213-qlearning-更新"></a> 2.1.3 QLearning 更新</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300920463.png" srcset="/img/loading.gif" lazyload alt="3"></p><p>所以我们回到之前的流程, 根据 Q 表的估计, 因为在 s1 中, a2 的值比较大, 通过之前的决策方法, 我们在 s1 采取了 a2, 并到达 s2, 这时我们开始更新用于决策的 Q 表, 接着我们并没有在实际中采取任何行为, 而是<strong>再想象自己在 s2 上采取了每种行为</strong>, 分别看看两种行为哪一个的 Q 值大, 比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以<strong>我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0)</strong>, 因为会获取实实在在的奖励 R , 我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值。<strong>所以有了现实和估计值, 我们就能更新Q(s1, a2) , 根据估计与现实的差距, 将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值变成新的值</strong>。但时刻记住, 我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做。这就是 off-policy 的 Q learning 是如何决策和学习优化决策的过程。</p><h3 id="214-qlearning-整体算法"><a class="markdownIt-Anchor" href="#214-qlearning-整体算法"></a> 2.1.4 QLearning 整体算法</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300930021.png" srcset="/img/loading.gif" lazyload alt="4"></p><p>这一张图概括了我们之前所有的内容。这也是 <strong>Q learning 的算法, 每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实</strong>, 很奇妙吧。最后我们来说说这套算法中一些参数的意义。<strong>Epsilon greedy 是用在决策上的一种策略, 比如 epsilon = 0.9 时, 就说明有90% 的情况我会按照 Q 表的最优值选择行为, 10% 的时间使用随机选行为。alpha是学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1 的数。gamma 是对未来 reward 的衰减值。</strong></p><h3 id="215-qlearning-中的-gamma"><a class="markdownIt-Anchor" href="#215-qlearning-中的-gamma"></a> 2.1.5 QLearning 中的 Gamma</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111300934738.png" srcset="/img/loading.gif" lazyload alt="5"></p><p>我们重写一下 Q(s1) 的公式, 将 Q(s2) 拆开, 因为Q(s2)可以像 Q(s1)一样,是关于Q(s3) 的, 所以可以写成这样, 然后以此类推, 不停地这样写下去, 最后就能写成这样。**可以看出Q(s1) 是有关于之后所有的奖励, 但这些奖励正在衰减, 离 s1 越远的状态衰减越严重。**不好理解? 行, 我们想象 Qlearning 的机器人天生近视眼, gamma = 1 时, 机器人有了一副合适的眼镜, 在 s1 看到的 Q 是未来没有任何衰变的奖励, 也就是机器人能清清楚楚地看到之后所有步的全部价值, 但是当 gamma =0, 近视机器人没了眼镜, 只能摸到眼前的 reward, 同样也就只在乎最近的大奖励, 如果 gamma 从 0 变到 1, 眼镜的度数由浅变深, 对远处的价值看得越清楚, 所以机器人渐渐变得有远见, 不仅仅只看眼前的利益, 也为自己的未来着想。</p><h3 id="216-补充"><a class="markdownIt-Anchor" href="#216-补充"></a> 2.1.6 补充</h3><p>1.为什么不直接用现实值更新老的Q值呢？</p><p>Q值是未来发展情况的累计变量，不只有下一步的现实值</p><ol start="2"><li></li></ol><p>Q值的定义，从当前状态开始，之后每一次状态决策都采取最优解，直到最后一个状态（Game over）的动作质量(quality)。</p><p>Q值可以一眼看穿未来，这就是Q-learning 的迷人之处。</p><p>奖励表 R 是自然生成客观存在的。</p><h2 id="22-小例子"><a class="markdownIt-Anchor" href="#22-小例子"></a> 2.2 小例子</h2><h3 id="221-要点"><a class="markdownIt-Anchor" href="#221-要点"></a> 2.2.1 要点</h3><p>这一次我们会用 tabular Q-learning 的方法实现一个小例子, 例子的环境是一个一维世界, 在世界的右边有宝藏, 探索者只要得到宝藏尝到了甜头, 然后以后就记住了得到宝藏的方法, 这就是他用强化学习所学习到的行为。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">-o<span class="hljs-comment">---T</span><br><span class="hljs-comment"># T 就是宝藏的位置, o 是探索者的位置</span><br></code></pre></td></tr></table></figure><p><strong>Q-learning 是一种记录行为值 (Q value) 的方法, 每种在一定状态的行为都会有一个值 <code>Q(s, a)</code>, 就是说行为 <code>a</code> 在 <code>s</code> 状态的值是 <code>Q(s, a)</code>。 <code>s</code> 在上面的探索者游戏中, 就是 <code>o</code> 所在的地点了。 而每一个地点探索者都能做出两个行为 <code>left/right</code>, 这就是探索者的所有可行的 <code>a</code> 啦。</strong></p><p>如果在某个地点 <code>s1</code>, 探索者计算了他能有的两个行为, <code>a1/a2=left/right</code>, 计算结果是 <code>Q(s1, a1) &gt; Q(s1, a2)</code>, 那么探索者就会选择 <code>left</code> 这个行为。 这就是 Q learning 的行为选择简单规则。</p><p><strong>当然我们还会细说更具体的规则。 在之后的教程中, 我们会更加详细得讲解 RL 中的各种方法, 下面的内容, 大家大概看看就行, 有个大概的 RL 概念就行, 知道 RL 的一些关键步骤就行, 这节的算法不用仔细研究。</strong></p><h3 id="222-预设值"><a class="markdownIt-Anchor" href="#222-预设值"></a> 2.2.2 预设值</h3><p>这一次需要的模块和参数设置:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> time<br><br>N_STATES = <span class="hljs-number">6</span>   <span class="hljs-comment"># 1维世界的宽度</span><br>ACTIONS = [<span class="hljs-string">&#x27;left&#x27;</span>, <span class="hljs-string">&#x27;right&#x27;</span>]     <span class="hljs-comment"># 探索者的可用动作</span><br>EPSILON = <span class="hljs-number">0.9</span>   <span class="hljs-comment"># 贪婪度 greedy</span><br>ALPHA = <span class="hljs-number">0.1</span>     <span class="hljs-comment"># 学习率</span><br>GAMMA = <span class="hljs-number">0.9</span>    <span class="hljs-comment"># 奖励递减值</span><br>MAX_EPISODES = <span class="hljs-number">13</span>   <span class="hljs-comment"># 最大回合数</span><br>FRESH_TIME = <span class="hljs-number">0.3</span>    <span class="hljs-comment"># 移动间隔时间</span><br></code></pre></td></tr></table></figure><h3 id="223-q-表"><a class="markdownIt-Anchor" href="#223-q-表"></a> 2.2.3 Q 表</h3><p>对于 tabular Q learning, 我们必须将所有的 Q values (行为值) 放在 <code>q_table</code> 中, 更新 <code>q_table</code> 也是在更新他的行为准则。 <code>q_table</code> 的 index 是所有对应的 <code>state</code> (探索者位置), columns 是对应的 <code>action</code> (探索者行为)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_q_table</span>(<span class="hljs-params">n_states, actions</span>):<br>    table = pd.DataFrame(<br>        np.zeros((n_states, <span class="hljs-built_in">len</span>(actions))),     <span class="hljs-comment"># q_table 全 0 初始</span><br>        columns=actions,    <span class="hljs-comment"># columns 对应的是行为名称</span><br>    )<br>    <span class="hljs-keyword">return</span> table<br><br><span class="hljs-comment"># q_table:</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">   left  right</span><br><span class="hljs-string">0   0.0    0.0</span><br><span class="hljs-string">1   0.0    0.0</span><br><span class="hljs-string">2   0.0    0.0</span><br><span class="hljs-string">3   0.0    0.0</span><br><span class="hljs-string">4   0.0    0.0</span><br><span class="hljs-string">5   0.0    0.0</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><h3 id="224-定义动作"><a class="markdownIt-Anchor" href="#224-定义动作"></a> 2.2.4 定义动作</h3><p>接着定义探索者是如何挑选行为的。 这是我们引入 <code>epsilon greedy</code> 的概念。 因为在初始阶段, 随机的探索环境, 往往比固定的行为模式要好, 所以这也是累积经验的阶段, 我们希望探索者不会那么贪婪(greedy)。 所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。 <code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪), 不过在这个例子中, 我们就固定成 <code>EPSILON = 0.9</code>, 90% 的时间是选择最优策略, 10% 的时间来探索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在某个 state 地点, 选择行为</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">state, q_table</span>):<br>    state_actions = q_table.iloc[state, :]  <span class="hljs-comment"># 选出这个 state 的所有 action 值</span><br>    <span class="hljs-keyword">if</span> (np.random.uniform() &gt; EPSILON) <span class="hljs-keyword">or</span> (state_actions.<span class="hljs-built_in">all</span>() == <span class="hljs-number">0</span>):  <span class="hljs-comment"># 非贪婪 or 或者这个 state 还没有探索过</span><br>        action_name = np.random.choice(ACTIONS)<br>    <span class="hljs-keyword">else</span>:<br>        action_name = state_actions.argmax()    <span class="hljs-comment"># 贪婪模式</span><br>    <span class="hljs-keyword">return</span> action_name<br></code></pre></td></tr></table></figure><h3 id="225-环境反馈-s_-r"><a class="markdownIt-Anchor" href="#225-环境反馈-s_-r"></a> 2.2.5 环境反馈 S_, R</h3><p>做出行为后, 环境也要给我们的行为一个反馈, 反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward ®。 这里定义的规则就是, 只有当 <code>o</code> 移动到了 <code>T</code>, 探索者才会得到唯一的一个奖励, 奖励值 R=1, 其他情况都没有奖励。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_env_feedback</span>(<span class="hljs-params">S, A</span>):<br>    <span class="hljs-comment"># This is how agent will interact with the environment</span><br>    <span class="hljs-keyword">if</span> A == <span class="hljs-string">&#x27;right&#x27;</span>:    <span class="hljs-comment"># move right</span><br>        <span class="hljs-keyword">if</span> S == N_STATES - <span class="hljs-number">2</span>:   <span class="hljs-comment"># terminate</span><br>            S_ = <span class="hljs-string">&#x27;terminal&#x27;</span><br>            R = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            S_ = S + <span class="hljs-number">1</span><br>            R = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># move left</span><br>        R = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> S == <span class="hljs-number">0</span>:<br>            S_ = S  <span class="hljs-comment"># reach the wall</span><br>        <span class="hljs-keyword">else</span>:<br>            S_ = S - <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> S_, R<br></code></pre></td></tr></table></figure><h3 id="226-环境更新"><a class="markdownIt-Anchor" href="#226-环境更新"></a> 2.2.6 环境更新</h3><p>接下来就是环境的更新了, 不用细看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_env</span>(<span class="hljs-params">S, episode, step_counter</span>):<br>    <span class="hljs-comment"># This is how environment be updated</span><br>    env_list = [<span class="hljs-string">&#x27;-&#x27;</span>]*(N_STATES-<span class="hljs-number">1</span>) + [<span class="hljs-string">&#x27;T&#x27;</span>]   <span class="hljs-comment"># &#x27;---------T&#x27; our environment</span><br>    <span class="hljs-keyword">if</span> S == <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>        interaction = <span class="hljs-string">&#x27;Episode %s: total_steps = %s&#x27;</span> % (episode+<span class="hljs-number">1</span>, step_counter)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(interaction), end=<span class="hljs-string">&#x27;&#x27;</span>)<br>        time.sleep(<span class="hljs-number">2</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\r                                &#x27;</span>, end=<span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        env_list[S] = <span class="hljs-string">&#x27;o&#x27;</span><br>        interaction = <span class="hljs-string">&#x27;&#x27;</span>.join(env_list)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(interaction), end=<span class="hljs-string">&#x27;&#x27;</span>)<br>        time.sleep(FRESH_TIME)<br></code></pre></td></tr></table></figure><h3 id="227-强化学习主循环"><a class="markdownIt-Anchor" href="#227-强化学习主循环"></a> 2.2.7 强化学习主循环</h3><p>最重要的地方就在这里。 你定义的 RL 方法都在这里体现。 在之后的教程中, 我们会更加详细得讲解 RL 中的各种方法, 下面的内容, 大家大概看看就行, 这节内容不用仔细研究。</p><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202112021050182.png" srcset="/img/loading.gif" lazyload alt="6"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">rl</span>():<br>    q_table = build_q_table(N_STATES, ACTIONS)  <span class="hljs-comment"># 初始 q table</span><br>    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(MAX_EPISODES):     <span class="hljs-comment"># 回合</span><br>        step_counter = <span class="hljs-number">0</span><br>        S = <span class="hljs-number">0</span>   <span class="hljs-comment"># 回合初始位置</span><br>        is_terminated = <span class="hljs-literal">False</span>   <span class="hljs-comment"># 是否回合结束</span><br>        update_env(S, episode, step_counter)    <span class="hljs-comment"># 环境更新</span><br>        <br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> is_terminated:<br>            A = choose_action(S, q_table)   <span class="hljs-comment"># 选行为</span><br>            S_, R = get_env_feedback(S, A)  <span class="hljs-comment"># 实施行为并得到环境的反馈</span><br>            q_predict = q_table.loc[S, A]    <span class="hljs-comment"># 估算的(状态-行为)值</span><br>            <span class="hljs-keyword">if</span> S_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>                q_target = R + GAMMA * q_table.iloc[S_, :].<span class="hljs-built_in">max</span>()   <span class="hljs-comment">#  实际的(状态-行为)值 (回合没结束)</span><br>            <span class="hljs-keyword">else</span>:<br>                q_target = R     <span class="hljs-comment">#  实际的(状态-行为)值 (回合结束)</span><br>                is_terminated = <span class="hljs-literal">True</span>    <span class="hljs-comment"># terminate this episode</span><br><br>            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  <span class="hljs-comment">#  q_table 更新</span><br>            S = S_  <span class="hljs-comment"># 探索者移动到下一个 state</span><br><br>            update_env(S, episode, step_counter+<span class="hljs-number">1</span>)  <span class="hljs-comment"># 环境更新</span><br><br>            step_counter += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> q_table<br></code></pre></td></tr></table></figure><p>写好所有的评估和更新准则后, 我们就能开始训练了, 把探索者丢到环境中, 让它自己去玩吧。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    q_table = rl()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\r\nQ-table:\n&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(q_table)<br></code></pre></td></tr></table></figure><h3 id="228-补充"><a class="markdownIt-Anchor" href="#228-补充"></a> 2.2.8 补充</h3><p>1.运行到q_predict = q_table.loc[S, A]出现了一个KeyError。</p><p>解决方式：把action_name = state_actions.argmax()改成action_name = ACTIONS[state_actions.argmax()]就可以了。</p><h2 id="23-q-learning-算法更新"><a class="markdownIt-Anchor" href="#23-q-learning-算法更新"></a> 2.3 Q-learning 算法更新</h2><h3 id="231-要点"><a class="markdownIt-Anchor" href="#231-要点"></a> 2.3.1 要点</h3><p>上次我们知道了 RL 之中的 Q-learning 方法是在做什么事, 今天我们就来说说一个更具体的例子。让探索者学会走迷宫。黄色的是天堂 (reward 1), 黑色的地狱 (reward -1)。大多数 RL 是由 reward 导向的, 所以定义 reward 是 RL 中比较重要的一点。</p><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202112031036521.png" srcset="/img/loading.gif" lazyload alt="7"></p><h3 id="232-算法"><a class="markdownIt-Anchor" href="#232-算法"></a> 2.3.2 算法</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202112031037484.png" srcset="/img/loading.gif" lazyload alt="8"></p><p>**整个算法就是一直不断更新 Q table 里的值, 然后再根据新的值来判断要在某个 state 采取怎样的 action。**Qlearning 是一个 <strong>off-policy</strong> 的算法, 因为里面的 <code>max</code> action 让 Q table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验,甚至是学习他人的经验)。不过这一次的例子, 我们没有运用到 off-policy, 而是把 Qlearning 用在了 on-policy 上, 也就是现学现卖, 将现在经历的直接当场学习并运用。</p><h3 id="233-算法的代码形式"><a class="markdownIt-Anchor" href="#233-算法的代码形式"></a> 2.3.3 算法的代码形式</h3><p>首先我们先 import 两个模块, <code>maze_env</code> 是我们的环境模块, 已经编写好了, 大家可以直接在<a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py">这里下载</a>, <code>maze_env</code> 模块我们可以不深入研究, 如果你对编辑环境感兴趣, 可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境. 我也有<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的. 而 <code>RL_brain</code> 这个模块是 RL 的大脑部分, 我们下节会讲。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> maze_env <span class="hljs-keyword">import</span> Maze<br><span class="hljs-keyword">from</span> RL_brain <span class="hljs-keyword">import</span> QLearningTable<br></code></pre></td></tr></table></figure><p>下面的代码, 我们可以根据上面的图片中的算法对应起来, 这就是整个 Qlearning 最重要的迭代更新部分啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>():<br>    <span class="hljs-comment"># 学习 100 回合</span><br>    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        <span class="hljs-comment"># 初始化 state 的观测值</span><br>        observation = env.reset()<br><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 更新可视化环境</span><br>            env.render()<br><br>            <span class="hljs-comment"># RL 大脑根据 state 的观测值挑选 action</span><br>            action = RL.choose_action(<span class="hljs-built_in">str</span>(observation))<br><br>            <span class="hljs-comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span><br>            observation_, reward, done = env.step(action)<br><br>            <span class="hljs-comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span><br>            RL.learn(<span class="hljs-built_in">str</span>(observation), action, reward, <span class="hljs-built_in">str</span>(observation_))<br><br>            <span class="hljs-comment"># 将下一个 state 的值传到下一次循环</span><br>            observation = observation_<br><br>            <span class="hljs-comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span><br>            <span class="hljs-keyword">if</span> done:<br>                <span class="hljs-keyword">break</span><br><br>    <span class="hljs-comment"># 结束游戏并关闭窗口</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;game over&#x27;</span>)<br>    env.destroy()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># 定义环境 env 和 RL 方式</span><br>    env = Maze()<br>    RL = QLearningTable(actions=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(env.n_actions)))<br><br>    <span class="hljs-comment"># 开始可视化环境 env</span><br>    env.after(<span class="hljs-number">100</span>, update)<br>    env.mainloop()<br></code></pre></td></tr></table></figure><h3 id="234-补充"><a class="markdownIt-Anchor" href="#234-补充"></a> 2.3.4 补充</h3><p>1.为什么训练的时候，红色方框会斜着走？</p><p>应该是 tkinter 的跳帧问题，实际上还是走了 2 步</p><p>2.observation的初始值（5，5，35，35）对应成我们可以理解的坐标（x,y），这应该是怎么转换的呢？</p><p>是正方形的x1,y1,x2,y2 坐标。</p><p>3.您好，我看maze_env的代码时，没有看到针对不同state对action的限制。<br>比如当已经走到maze的边界，这时不需要限制方块的行为吗？运行run this后，发现方块在边界的移动刷新速率并不总相同，是否因为上述action未受限？</p><p>在maze_env.py的第95行有限制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> action == <span class="hljs-number">0</span>:<br>    <span class="hljs-keyword">if</span> s[<span class="hljs-number">1</span>] &gt; UNIT:<br>        base_action[<span class="hljs-number">1</span>] -= UNIT<br><span class="hljs-keyword">elif</span> action == <span class="hljs-number">1</span>:   <br>    <span class="hljs-keyword">if</span> s[<span class="hljs-number">1</span>] &lt; (MAZE_H - <span class="hljs-number">1</span>) * UNIT:<br>        base_action[<span class="hljs-number">1</span>] += UNIT<br><span class="hljs-keyword">elif</span> action == <span class="hljs-number">2</span>:   <br>    <span class="hljs-keyword">if</span> s[<span class="hljs-number">0</span>] &lt; (MAZE_W - <span class="hljs-number">1</span>) * UNIT:<br>        base_action[<span class="hljs-number">0</span>] += UNIT<br><span class="hljs-keyword">elif</span> action == <span class="hljs-number">3</span>:   <br>    <span class="hljs-keyword">if</span> s[<span class="hljs-number">0</span>] &gt; UNIT:<br>        base_action[<span class="hljs-number">0</span>] -= UNIT<br></code></pre></td></tr></table></figure><p>但是在rl的action输出上，不会对上下左右动作做限制。</p><h2 id="24-q-learning-思维决策"><a class="markdownIt-Anchor" href="#24-q-learning-思维决策"></a> 2.4 Q-learning 思维决策</h2><h3 id="241-代码主结构"><a class="markdownIt-Anchor" href="#241-代码主结构"></a> 2.4.1 代码主结构</h3><p>与上回不一样的地方是, 我们将要以一个 class 形式定义 Q learning, 并把这种 tabular q learning 方法叫做 <code>QLearningTable</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearningTable</span>:<br>    <span class="hljs-comment"># 初始化</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br><br>    <span class="hljs-comment"># 选行为</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br><br>    <span class="hljs-comment"># 学习更新参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br><br>    <span class="hljs-comment"># 检测 state 是否存在</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br></code></pre></td></tr></table></figure><h3 id="242-预设值"><a class="markdownIt-Anchor" href="#242-预设值"></a> 2.4.2 预设值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearningTable</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = actions  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate <span class="hljs-comment"># 学习率</span><br>        self.gamma = reward_decay   <span class="hljs-comment"># 奖励衰减</span><br>        self.epsilon = e_greedy     <span class="hljs-comment"># 贪婪度</span><br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   <span class="hljs-comment"># 初始 q_table</span><br></code></pre></td></tr></table></figure><h3 id="243-决定行为"><a class="markdownIt-Anchor" href="#243-决定行为"></a> 2.4.3 决定行为</h3><p>这里是定义如何根据所在的 state, 或者是在这个 state 上的 观测值 (observation) 来决策。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation) <span class="hljs-comment"># 检测本 state 是否在 q_table 中存在(见后面标题内容)</span><br><br>        <span class="hljs-comment"># 选择 action</span><br>        <span class="hljs-keyword">if</span> np.random.uniform() &lt; self.epsilon:  <span class="hljs-comment"># 选择 Q value 最高的 action</span><br>            state_action = self.q_table.loc[observation, :]<br><br>            <span class="hljs-comment"># 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br><br>        <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># 随机选择 action</span><br>            action = np.random.choice(self.actions)<br><br>        <span class="hljs-keyword">return</span> action<br></code></pre></td></tr></table></figure><h3 id="244-学习"><a class="markdownIt-Anchor" href="#244-学习"></a> 2.4.4 学习</h3><p>同上一个简单的 q learning 例子一样, 我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更行 <code>q_table</code>. 更新的方式是不是很熟悉呢:</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">update</span> <span class="hljs-operator">=</span> self.lr * (q_target - q_predict)<br></code></pre></td></tr></table></figure><p>这可以理解成神经网络中的更新方式, 学习率 * (真实值 - 预测值). 将判断误差传递回去, 有着和神经网络更新的异曲同工之处。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        self.check_state_exist(s_)  <span class="hljs-comment"># 检测 q_table 中是否存在 s_ (见后面标题内容)</span><br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="hljs-built_in">max</span>()  <span class="hljs-comment"># 下个 state 不是 终止符</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># 下个 state 是终止符</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># 更新对应的 state-action 值</span><br></code></pre></td></tr></table></figure><h3 id="245-检测-state-是否存在"><a class="markdownIt-Anchor" href="#245-检测-state-是否存在"></a> 2.4.5 检测 state 是否存在</h3><p>这个功能就是检测 <code>q_table</code> 中有没有当前 state 的步骤了, 如果还没有当前 state, 那我们就插入一组全 0 数据, 当做这个 state 的所有 action 初始 values。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" class="print-no-link">#动态规划</a></div></div><div class="license-box my-3"><div class="license-title"><div>莫烦强化学习-Q Leaning</div><div>https://fulequn.github.io/2021/11/Article202111291/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年11月29日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2021/11/Article202111292/" title="强化学习系列（一）：强化学习简介"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">强化学习系列（一）：强化学习简介</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/11/Article202111226/" title="《深度学习实战》汇总"><span class="hidden-mobile">《深度学习实战》汇总</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>