<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="本次涉及代码较多，部分重复代码不再重复发布。重复的代码详见以下链接： https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_37402392&#x2F;article&#x2F;details&#x2F;121468321?spm&#x3D;1001.2014.3001.5501 bn_layers.py cnn_layers.py dropout_layers.py layers.py updater.py 以下是本次添加的内容。 cap"><meta property="og:type" content="article"><meta property="og:title" content="编码实现RNN以及LSTM"><meta property="og:url" content="https://fulequn.github.io/2021/11/Article202111225/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="本次涉及代码较多，部分重复代码不再重复发布。重复的代码详见以下链接： https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_37402392&#x2F;article&#x2F;details&#x2F;121468321?spm&#x3D;1001.2014.3001.5501 bn_layers.py cnn_layers.py dropout_layers.py layers.py updater.py 以下是本次添加的内容。 cap"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2021-11-22T05:26:21.000Z"><meta property="article:modified_time" content="2024-05-18T14:35:07.690Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><title>编码实现RNN以及LSTM - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="编码实现RNN以及LSTM"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-11-22 13:26" pubdate>2021年11月22日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>5.4k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>46 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">编码实现RNN以及LSTM</h1><div class="markdown-body"><p>本次涉及代码较多，部分重复代码不再重复发布。重复的代码详见以下链接：</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37402392/article/details/121468321?spm=1001.2014.3001.5501">https://blog.csdn.net/qq_37402392/article/details/121468321?spm=1001.2014.3001.5501</a></p><p><strong>bn_layers.py</strong></p><p><strong>cnn_layers.py</strong></p><p><strong>dropout_layers.py</strong></p><p><strong><a target="_blank" rel="noopener" href="http://layers.py">layers.py</a></strong></p><p><strong><a target="_blank" rel="noopener" href="http://updater.py">updater.py</a></strong></p><p>以下是本次添加的内容。</p><p><strong>captioning_trainer.py</strong></p><p>训练器，与之前的训练器Trainer类似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">from</span> coco_utils <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> updater<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CaptioningTrainer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; </span><br><span class="hljs-string">    CaptioningTrainer大部分内容和前面的Trainer相同</span><br><span class="hljs-string">    使用方法：</span><br><span class="hljs-string">    data = load_coco_data()</span><br><span class="hljs-string">    model = MyAwesomeModel(hidden_dim=100)</span><br><span class="hljs-string">    trainer = CaptioningTrainer(model, data,</span><br><span class="hljs-string">                                    update_rule=&#x27;sgd&#x27;,</span><br><span class="hljs-string">                                    updater_config=&#123;</span><br><span class="hljs-string">                                        &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="hljs-string">                                    &#125;,</span><br><span class="hljs-string">                                    lr_decay=0.95,</span><br><span class="hljs-string">                                    num_epochs=10, batch_size=100,</span><br><span class="hljs-string">                                    print_every=100)</span><br><span class="hljs-string">    trainer.train()</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, data, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化CaptioningTrainer</span><br><span class="hljs-string">        所需参数:</span><br><span class="hljs-string">        - model: RNN模型</span><br><span class="hljs-string">        - data: coco数据集</span><br><span class="hljs-string"></span><br><span class="hljs-string">        可选参数:</span><br><span class="hljs-string">        - update_rule:更新规则，查看 updater.py.</span><br><span class="hljs-string">            默认为 &#x27;sgd&#x27;.</span><br><span class="hljs-string">        - updater_config: 更新器配置</span><br><span class="hljs-string">        - lr_decay:学习率衰减因子</span><br><span class="hljs-string">        - batch_size: 批量大小</span><br><span class="hljs-string">        - num_epochs: 迭代次数</span><br><span class="hljs-string">        - print_every:每训练多少次，打印训练结果</span><br><span class="hljs-string">        - verbose:是否打印训练中间结果</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.model = model<br>        self.data = data<br>        <br>        self.update_rule = kwargs.pop(<span class="hljs-string">&#x27;update_rule&#x27;</span>, <span class="hljs-string">&#x27;sgd&#x27;</span>)<br>        self.updater_config = kwargs.pop(<span class="hljs-string">&#x27;updater_config&#x27;</span>, &#123;&#125;)<br>        self.lr_decay = kwargs.pop(<span class="hljs-string">&#x27;lr_decay&#x27;</span>, <span class="hljs-number">1.0</span>)<br>        self.batch_size = kwargs.pop(<span class="hljs-string">&#x27;batch_size&#x27;</span>, <span class="hljs-number">100</span>)<br>        self.num_epochs = kwargs.pop(<span class="hljs-string">&#x27;num_epochs&#x27;</span>, <span class="hljs-number">10</span>)<br><br>        self.print_every = kwargs.pop(<span class="hljs-string">&#x27;print_every&#x27;</span>, <span class="hljs-number">10</span>)<br>        self.verbose = kwargs.pop(<span class="hljs-string">&#x27;verbose&#x27;</span>, <span class="hljs-literal">True</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(kwargs) &gt; <span class="hljs-number">0</span>:<br>            extra = <span class="hljs-string">&#x27;, &#x27;</span>.join(<span class="hljs-string">&#x27;&quot;%s&quot;&#x27;</span> % k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> kwargs.keys())<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Unrecognized arguments %s&#x27;</span> % extra)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(updater, self.update_rule):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % self.update_rule)<br>        self.update_rule = <span class="hljs-built_in">getattr</span>(updater, self.update_rule)<br><br>        self._reset()<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_reset</span>(<span class="hljs-params">self</span>):<br>        self.epoch = <span class="hljs-number">0</span><br>        self.best_val_acc = <span class="hljs-number">0</span><br>        self.best_params = &#123;&#125;<br>        self.loss_history = []<br>        self.train_acc_history = []<br>        self.val_acc_history = []<br><br>        self.updater_configs = &#123;&#125;<br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.model.params:<br>            d = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.updater_config.items()&#125;<br>            self.updater_configs[p] = d<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_step</span>(<span class="hljs-params">self</span>):<br>        minibatch = sample_coco_minibatch(self.data,<br>                                    batch_size=self.batch_size,<br>                                    split=<span class="hljs-string">&#x27;train&#x27;</span>)<br>        captions, features, urls = minibatch<br><br>        loss, grads = self.model.loss(features, captions)<br>        self.loss_history.append(loss)<br><br>        <span class="hljs-keyword">for</span> p, w <span class="hljs-keyword">in</span> self.model.params.items():<br>            dw = grads[p]<br>            config = self.updater_configs[p]<br>            next_w, next_config = self.update_rule(w, dw, config)<br>            self.model.params[p] = next_w<br>            self.updater_configs[p] = next_config<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        num_train = self.data[<span class="hljs-string">&#x27;train_captions&#x27;</span>].shape[<span class="hljs-number">0</span>]<br>        iterations_per_epoch = <span class="hljs-built_in">max</span>(num_train / self.batch_size, <span class="hljs-number">1</span>)<br>        num_iterations = <span class="hljs-built_in">int</span>(self.num_epochs * iterations_per_epoch)<br>        <br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>            self._step()<br><br><br>            <span class="hljs-keyword">if</span> self.verbose <span class="hljs-keyword">and</span> t % self.print_every == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(Iteration %d / %d) loss: %f&#x27;</span> % (<br>                             t + <span class="hljs-number">1</span>, num_iterations, self.loss_history[-<span class="hljs-number">1</span>]))<br><br>            epoch_end = (t + <span class="hljs-number">1</span>) % iterations_per_epoch == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> epoch_end:<br>                self.epoch += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> self.updater_configs:<br>                    self.updater_configs[k][<span class="hljs-string">&#x27;learning_rate&#x27;</span>] *= self.lr_decay<br><br><br></code></pre></td></tr></table></figure><p><strong>coco_utils.py</strong></p><p>对coco数据集进行处理，包括了读取数据文件、解码数据以及小批量读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os, json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> h5py<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_coco_data</span>(<span class="hljs-params">base_dir=<span class="hljs-string">&#x27;datasets/coco_captioning&#x27;</span>, max_train=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                         pca_features=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    读取CoCo训练文件</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    base_dir : TYPE, optional</span><br><span class="hljs-string">        数据文件位置. The default is &#x27;datasets/coco_captioning&#x27;.</span><br><span class="hljs-string">    max_train : TYPE, optional</span><br><span class="hljs-string">        是否对训练数据进行再抽样. The default is None.</span><br><span class="hljs-string">    pca_features : TYPE, optional</span><br><span class="hljs-string">        是否使用降维特征. The default is True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    data : TYPE</span><br><span class="hljs-string">        读取的数据文件.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 保存数据</span><br>    data = &#123;&#125;<br>    <span class="hljs-comment"># 获得文件</span><br>    caption_file = os.path.join(base_dir, <span class="hljs-string">&#x27;coco2014_captions.h5&#x27;</span>)<br>    <span class="hljs-comment"># 添加文件汇总的内容</span><br>    <span class="hljs-keyword">with</span> h5py.File(caption_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> f.items():<br>            data[k] = np.asarray(v)<br>    <br>    <span class="hljs-comment"># 是否使用降维特征</span><br>    <span class="hljs-keyword">if</span> pca_features:<br>        train_feat_file = os.path.join(base_dir, <span class="hljs-string">&#x27;train2014_vgg16_fc7_pca.h5&#x27;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        train_feat_file = os.path.join(base_dir, <span class="hljs-string">&#x27;train2014_vgg16_fc7.h5&#x27;</span>)<br>    <span class="hljs-keyword">with</span> h5py.File(train_feat_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        data[<span class="hljs-string">&#x27;train_features&#x27;</span>] = np.asarray(f[<span class="hljs-string">&#x27;features&#x27;</span>])<br><br>    <span class="hljs-keyword">if</span> pca_features:<br>        val_feat_file = os.path.join(base_dir, <span class="hljs-string">&#x27;val2014_vgg16_fc7_pca.h5&#x27;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        val_feat_file = os.path.join(base_dir, <span class="hljs-string">&#x27;val2014_vgg16_fc7.h5&#x27;</span>)<br>    <span class="hljs-keyword">with</span> h5py.File(val_feat_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        data[<span class="hljs-string">&#x27;val_features&#x27;</span>] = np.asarray(f[<span class="hljs-string">&#x27;features&#x27;</span>])<br><br>    dict_file = os.path.join(base_dir, <span class="hljs-string">&#x27;coco2014_vocab.json&#x27;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(dict_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        dict_data = json.load(f)<br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> dict_data.items():<br>            data[k] = v<br><br>    train_url_file = os.path.join(base_dir, <span class="hljs-string">&#x27;train2014_urls.txt&#x27;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(train_url_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        train_urls = np.asarray([line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f])<br>    data[<span class="hljs-string">&#x27;train_urls&#x27;</span>] = train_urls<br><br>    val_url_file = os.path.join(base_dir, <span class="hljs-string">&#x27;val2014_urls.txt&#x27;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(val_url_file, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        val_urls = np.asarray([line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f])<br>    data[<span class="hljs-string">&#x27;val_urls&#x27;</span>] = val_urls<br><br>    <span class="hljs-comment"># 也许对训练数据进行再抽样</span><br>    <span class="hljs-keyword">if</span> max_train <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        num_train = data[<span class="hljs-string">&#x27;train_captions&#x27;</span>].shape[<span class="hljs-number">0</span>]<br>        mask = np.random.randint(num_train, size=max_train)<br>        data[<span class="hljs-string">&#x27;train_captions&#x27;</span>] = data[<span class="hljs-string">&#x27;train_captions&#x27;</span>][mask]<br>        data[<span class="hljs-string">&#x27;train_image_idxs&#x27;</span>] = data[<span class="hljs-string">&#x27;train_image_idxs&#x27;</span>][mask]<br><br>    <span class="hljs-keyword">return</span> data<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode_captions</span>(<span class="hljs-params">captions, idx_to_word</span>):<br>    singleton = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> captions.ndim == <span class="hljs-number">1</span>:<br>        singleton = <span class="hljs-literal">True</span><br>        captions = captions[<span class="hljs-literal">None</span>]<br>    decoded = []<br>    N, T = captions.shape<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>        words = []<br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T):<br>            word = idx_to_word[captions[i, t]]<br>            <span class="hljs-keyword">if</span> word != <span class="hljs-string">&#x27;&lt;NULL&gt;&#x27;</span>:<br>                words.append(word)<br>            <span class="hljs-keyword">if</span> word == <span class="hljs-string">&#x27;&lt;END&gt;&#x27;</span>:<br>                <span class="hljs-keyword">break</span><br>        decoded.append(<span class="hljs-string">&#x27; &#x27;</span>.join(words))<br>    <span class="hljs-keyword">if</span> singleton:<br>        decoded = decoded[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> decoded<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_coco_minibatch</span>(<span class="hljs-params">data, batch_size=<span class="hljs-number">100</span>, split=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>    split_size = data[<span class="hljs-string">&#x27;%s_captions&#x27;</span> % split].shape[<span class="hljs-number">0</span>]<br>    mask = np.random.choice(split_size, batch_size)<br>    captions = data[<span class="hljs-string">&#x27;%s_captions&#x27;</span> % split][mask]<br>    image_idxs = data[<span class="hljs-string">&#x27;%s_image_idxs&#x27;</span> % split][mask]<br>    image_features = data[<span class="hljs-string">&#x27;%s_features&#x27;</span> % split][image_idxs]<br>    urls = data[<span class="hljs-string">&#x27;%s_urls&#x27;</span> % split][image_idxs]<br>    <span class="hljs-keyword">return</span> captions, image_features, urls<br></code></pre></td></tr></table></figure><p><strong>image_utils.py</strong></p><p>通用的方法用于展示过程中的图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> urllib.request, os, tempfile<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.misc <span class="hljs-keyword">import</span> imread<br><br><span class="hljs-keyword">from</span> cnn_layers <span class="hljs-keyword">import</span> conv_forward_fast<br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Utility functions used for viewing and processing images.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">blur_image</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A very gentle image blurring operation, to be used as a regularizer for image</span><br><span class="hljs-string">    generation.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - X: Image data of shape (N, 3, H, W)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - X_blur: Blurred version of X, of shape (N, 3, H, W)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    w_blur = np.zeros((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>    b_blur = np.zeros(<span class="hljs-number">3</span>)<br>    blur_param = &#123;<span class="hljs-string">&#x27;stride&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;pad&#x27;</span>: <span class="hljs-number">1</span>&#125;<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>        w_blur[i, i] = np.asarray([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">188</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]], dtype=np.float32)<br>    w_blur /= <span class="hljs-number">200.0</span><br>    <span class="hljs-keyword">return</span> conv_forward_fast(X, w_blur, b_blur, blur_param)[<span class="hljs-number">0</span>]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_image</span>(<span class="hljs-params">img, mean_img, mean=<span class="hljs-string">&#x27;image&#x27;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Convert to float, transepose, and subtract mean pixel</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - img: (H, W, 3)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - (1, 3, H, 3)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> mean == <span class="hljs-string">&#x27;image&#x27;</span>:<br>        mean = mean_img<br>    <span class="hljs-keyword">elif</span> mean == <span class="hljs-string">&#x27;pixel&#x27;</span>:<br>        mean = mean_img.mean(axis=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), keepdims=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">elif</span> mean == <span class="hljs-string">&#x27;none&#x27;</span>:<br>        mean = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;mean must be image or pixel or none&#x27;</span>)<br>    <span class="hljs-keyword">return</span> img.astype(np.float32).transpose(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)[<span class="hljs-literal">None</span>] - mean<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">deprocess_image</span>(<span class="hljs-params">img, mean_img, mean=<span class="hljs-string">&#x27;image&#x27;</span>, renorm=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Add mean pixel, transpose, and convert to uint8</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - (1, 3, H, W) or (3, H, W)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - (H, W, 3)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> mean == <span class="hljs-string">&#x27;image&#x27;</span>:<br>        mean = mean_img<br>    <span class="hljs-keyword">elif</span> mean == <span class="hljs-string">&#x27;pixel&#x27;</span>:<br>        mean = mean_img.mean(axis=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), keepdims=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">elif</span> mean == <span class="hljs-string">&#x27;none&#x27;</span>:<br>        mean = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;mean must be image or pixel or none&#x27;</span>)<br>    <span class="hljs-keyword">if</span> img.ndim == <span class="hljs-number">3</span>:<br>        img = img[<span class="hljs-literal">None</span>]<br>    img = (img + mean)[<span class="hljs-number">0</span>].transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">if</span> renorm:<br>        low, high = img.<span class="hljs-built_in">min</span>(), img.<span class="hljs-built_in">max</span>()<br>        img = <span class="hljs-number">255.0</span> * (img - low) / (high - low)<br>    <span class="hljs-keyword">return</span> img.astype(np.uint8)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">image_from_url</span>(<span class="hljs-params">url</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Read an image from a URL. Returns a numpy array with the pixel data.</span><br><span class="hljs-string">    We write the image to a temporary file then read it back. Kinda gross.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">try</span>:<br>        f = urllib.request.urlopen(url)<br>        _, fname = tempfile.mkstemp()<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fname, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> ff:<br>            ff.write(f.read())<br>        img = imread(fname)<br>        os.remove(fname)<br>        <span class="hljs-keyword">return</span> img<br>    <span class="hljs-keyword">except</span> urllib.request.URLError <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;URL Error: &#x27;</span>, e.reason, url)<br>    <span class="hljs-keyword">except</span> urllib.request.HTTPError <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;HTTP Error: &#x27;</span>, e.code, url)<br><br></code></pre></td></tr></table></figure><p><strong><a target="_blank" rel="noopener" href="http://rnn.py">rnn.py</a></strong></p><p>图片说明任务RNN网络类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> rnn_layers <span class="hljs-keyword">import</span> *<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CaptioningRNN</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    处理图片说明任务RNN网络</span><br><span class="hljs-string">    注意：不使用正则化</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, word_to_idx, input_dim=<span class="hljs-number">512</span>, wordvec_dim=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">                             hidden_dim=<span class="hljs-number">128</span>, cell_type=<span class="hljs-string">&#x27;rnn&#x27;</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化CaptioningRNN </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - word_to_idx: 单词字典，用于查询单词索引对应的词向量</span><br><span class="hljs-string">        - input_dim: 输入图片数据维度</span><br><span class="hljs-string">        - wordvec_dim: 词向量维度.</span><br><span class="hljs-string">        - hidden_dim: RNN隐藏层维度.</span><br><span class="hljs-string">        - cell_type: 细胞类型; &#x27;rnn&#x27; 或 &#x27;lstm&#x27;.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 参数检验</span><br>        <span class="hljs-keyword">if</span> cell_type <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> &#123;<span class="hljs-string">&#x27;rnn&#x27;</span>, <span class="hljs-string">&#x27;lstm&#x27;</span>&#125;:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid cell_type &quot;%s&quot;&#x27;</span> % cell_type)<br>        <br>        <span class="hljs-comment"># 初始化数据</span><br>        self.cell_type = cell_type<br>        self.word_to_idx = word_to_idx<br>        self.idx_to_word = &#123;i: w <span class="hljs-keyword">for</span> w, i <span class="hljs-keyword">in</span> word_to_idx.items()&#125;<br>        self.params = &#123;&#125;<br>        <br>        vocab_size = <span class="hljs-built_in">len</span>(word_to_idx)<br><br>        self._null = word_to_idx[<span class="hljs-string">&#x27;&lt;NULL&gt;&#x27;</span>]<br>        self._start = word_to_idx.get(<span class="hljs-string">&#x27;&lt;START&gt;&#x27;</span>, <span class="hljs-literal">None</span>)<br>        self._end = word_to_idx.get(<span class="hljs-string">&#x27;&lt;END&gt;&#x27;</span>, <span class="hljs-literal">None</span>)<br>        <br>        <span class="hljs-comment"># 初始化词向量</span><br>        self.params[<span class="hljs-string">&#x27;W_embed&#x27;</span>] = np.random.randn(vocab_size, wordvec_dim)<br>        self.params[<span class="hljs-string">&#x27;W_embed&#x27;</span>] /= <span class="hljs-number">100</span><br>        <br>        <span class="hljs-comment"># 初始化 CNN -&gt; 隐藏层参数，用于将图片特征提取到RNN中</span><br>        self.params[<span class="hljs-string">&#x27;W_proj&#x27;</span>] = np.random.randn(input_dim, hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;W_proj&#x27;</span>] /= np.sqrt(input_dim)<br>        self.params[<span class="hljs-string">&#x27;b_proj&#x27;</span>] = np.zeros(hidden_dim)<br><br>        <span class="hljs-comment"># 初始化RNN参数</span><br>        dim_mul = &#123;<span class="hljs-string">&#x27;lstm&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;rnn&#x27;</span>: <span class="hljs-number">1</span>&#125;[cell_type]<br>        self.params[<span class="hljs-string">&#x27;Wx&#x27;</span>] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;Wx&#x27;</span>] /= np.sqrt(wordvec_dim)<br>        self.params[<span class="hljs-string">&#x27;Wh&#x27;</span>] = np.random.randn(hidden_dim, dim_mul * hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;Wh&#x27;</span>] /= np.sqrt(hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = np.zeros(dim_mul * hidden_dim)<br>        <br>        <span class="hljs-comment"># 初始化输出层参数 </span><br>        self.params[<span class="hljs-string">&#x27;W_vocab&#x27;</span>] = np.random.randn(hidden_dim, vocab_size)<br>        self.params[<span class="hljs-string">&#x27;W_vocab&#x27;</span>] /= np.sqrt(hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;b_vocab&#x27;</span>] = np.zeros(vocab_size)<br>            <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, features, captions</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        计算RNN或LSTM的损失值。</span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - features: 输入图片特征(N, D)。</span><br><span class="hljs-string">        - captions: 图像文字说明(N, T)。 </span><br><span class="hljs-string">            </span><br><span class="hljs-string">        Returns 元组:</span><br><span class="hljs-string">        - loss: 损失值。</span><br><span class="hljs-string">        - grads:梯度。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">#将文字切分为两段：captions_in除去最后一词用于RNN输入</span><br>        <span class="hljs-comment">#captions_out除去第一个单词，用于RNN输出配对</span><br>        captions_in = captions[:, :-<span class="hljs-number">1</span>]<br>        captions_out = captions[:, <span class="hljs-number">1</span>:]<br>        <br>        <span class="hljs-comment"># 掩码 </span><br>        mask = (captions_out != self._null)<br><br>        <span class="hljs-comment"># 图像仿射转换矩阵</span><br>        W_proj, b_proj = self.params[<span class="hljs-string">&#x27;W_proj&#x27;</span>], self.params[<span class="hljs-string">&#x27;b_proj&#x27;</span>]<br>        <br>        <span class="hljs-comment"># 词嵌入矩阵</span><br>        W_embed = self.params[<span class="hljs-string">&#x27;W_embed&#x27;</span>]<br><br>        <span class="hljs-comment"># RNN参数</span><br>        Wx, Wh, b = self.params[<span class="hljs-string">&#x27;Wx&#x27;</span>], self.params[<span class="hljs-string">&#x27;Wh&#x27;</span>], self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br><br>        <span class="hljs-comment"># 隐藏层输出转化矩阵</span><br>        W_vocab, b_vocab = self.params[<span class="hljs-string">&#x27;W_vocab&#x27;</span>], self.params[<span class="hljs-string">&#x27;b_vocab&#x27;</span>]<br>        <br>        loss, grads = <span class="hljs-number">0.0</span>, &#123;&#125;<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                        任务：实现CaptioningRNN传播                          #</span><br>        <span class="hljs-comment">#         (1)使用仿射变换(features,W_proj,b_proj)，                           #</span><br>        <span class="hljs-comment">#                     将图片特征输入进隐藏层初始状态h0(N,H)                      #</span><br>        <span class="hljs-comment">#         (2)使用词嵌入层将captions_in中的单词索引转换为词向量(N,T,W)              #</span><br>        <span class="hljs-comment">#         (3)使用RNN或LSTM处理词向量(N,T,H)                                    #</span><br>        <span class="hljs-comment">#         (4)使用时序仿射传播temporal_affine_forward计算各单词得分(N,T,V)        #</span><br>        <span class="hljs-comment">#         (5)使用temporal_softmax_loss计算损失值                              #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment"># 1 使用仿射变换(features,W_proj,b_proj)，将图片特征输入进隐藏层初始状态h0(N,H)</span><br>        h0, cache_h0 = affine_forward(features, W_proj, b_proj)<br>        <span class="hljs-comment"># 2 使用词嵌入层将captions_in中的单词索引转换为词向量(N,T,W)</span><br>        x, cache_embedding = word_embedding_forward(captions_in, W_embed)<br>        <span class="hljs-comment"># 3 使用RNN或LSTM处理词向量(N,T,H)</span><br>        <span class="hljs-keyword">if</span> self.cell_type == <span class="hljs-string">&#x27;rnn&#x27;</span>:<br>            out_h, cache_rnn = rnn_forward(x, h0, Wx, Wh, b)<br>        <span class="hljs-keyword">elif</span> self.cell_type == <span class="hljs-string">&#x27;lstm&#x27;</span>:<br>            out_h, cache_rnn = lstm_forward(x, h0, Wx, Wh, b)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid cell_type &quot;%s&quot;&#x27;</span> % self.cell_type)<br>        <span class="hljs-comment"># 4 使用时序仿射传播temporal_affine_forward计算各单词得分(N,T,V)</span><br>        yHat, cache_out = temporal_affine_forward(out_h, W_vocab, b_vocab)<br>        <span class="hljs-comment"># 5 使用temporal_softmax_loss计算损失值</span><br>        loss, dy = temporal_softmax_loss(yHat, captions_out, mask, verbose=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># 计算梯度</span><br>        dout_h, dW_vocab, db_vocab = temporal_affine_backward(dy, cache_out)<br>        <span class="hljs-comment"># 输出层到隐藏层的反向传播</span><br>        <span class="hljs-keyword">if</span> self.cell_type == <span class="hljs-string">&#x27;rnn&#x27;</span>:<br>            dx, dh0, dWx, dWh, db = rnn_backward(dout_h, cache_rnn)<br>        <span class="hljs-keyword">elif</span> self.cell_type == <span class="hljs-string">&#x27;lstm&#x27;</span>:<br>            dx, dh0, dWx, dWh, db = lstm_backward(dout_h, cache_rnn)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid cell_type &quot;%s&quot;&#x27;</span> % self.cell_type)<br>        <span class="hljs-comment"># 隐藏层到隐藏层自身的反向传播 </span><br>        dW_embed = word_embedding_backward(dx, cache_embedding)<br>        <span class="hljs-comment"># 隐藏层到输入层的反向传播</span><br>        dfeatures, dW_proj, db_proj = affine_backward(dh0, cache_h0)<br>        <span class="hljs-comment"># 记录梯度</span><br>        grads[<span class="hljs-string">&#x27;W_proj&#x27;</span>] = dW_proj<br>        grads[<span class="hljs-string">&#x27;b_proj&#x27;</span>] = db_proj<br>        grads[<span class="hljs-string">&#x27;W_embed&#x27;</span>] = dW_embed<br>        grads[<span class="hljs-string">&#x27;Wx&#x27;</span>] = dWx<br>        grads[<span class="hljs-string">&#x27;Wh&#x27;</span>] = dWh<br>        grads[<span class="hljs-string">&#x27;b&#x27;</span>] = db<br>        grads[<span class="hljs-string">&#x27;W_vocab&#x27;</span>] = dW_vocab<br>        grads[<span class="hljs-string">&#x27;b_vocab&#x27;</span>] = db_vocab<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                          结束编码                                          #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">return</span> loss, grads<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, features, max_length=<span class="hljs-number">30</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        测试阶段的前向传播过程，采样一批图片说明作为输入</span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - features: 图片特征(N, D).</span><br><span class="hljs-string">        - max_length:生成说明文字的最大长度</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - captions: 说明文字的字典索引串(N, max_length)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        N = features.shape[<span class="hljs-number">0</span>]<br>        captions = self._null * np.ones((N, max_length), dtype=np.int32)<br><br>        W_proj, b_proj = self.params[<span class="hljs-string">&#x27;W_proj&#x27;</span>], self.params[<span class="hljs-string">&#x27;b_proj&#x27;</span>]<br>        W_embed = self.params[<span class="hljs-string">&#x27;W_embed&#x27;</span>]<br>        Wx, Wh, b = self.params[<span class="hljs-string">&#x27;Wx&#x27;</span>], self.params[<span class="hljs-string">&#x27;Wh&#x27;</span>], self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        W_vocab, b_vocab = self.params[<span class="hljs-string">&#x27;W_vocab&#x27;</span>], self.params[<span class="hljs-string">&#x27;b_vocab&#x27;</span>]<br>        <br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                             任务：测试阶段前向传播                                                                        #</span><br>        <span class="hljs-comment">#    提示:(1)第一个单词应该是&lt;START&gt;标记，captions[:,0]=self._start                 #</span><br>        <span class="hljs-comment">#             (2)当前单词输入为之前RNN的输出                                                                        #</span><br>        <span class="hljs-comment">#        (3)前向传播过程为预测当前单词的下一个单词，                                                    #</span><br>        <span class="hljs-comment">#         你需要计算所有单词得分，然后选取最大得分作为预测单词                                #</span><br>        <span class="hljs-comment">#        (4)你无法使用rnn_forward 或 lstm_forward函数，                                                 #</span><br>        <span class="hljs-comment">#        你需要循环调用rnn_step_forward或lstm_step_forward函数                                #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment"># 获取数据</span><br>        N, D = features.shape<br>        affine_out, affine_cache = affine_forward(features, W_proj, b_proj)<br>        prev_word_idx = [self._start]*N<br>        prev_h = affine_out<br>        prev_c = np.zeros(prev_h.shape)<br>        <span class="hljs-comment"># 1第一个单词应该是&lt;START&gt;标记</span><br>        captions[:, <span class="hljs-number">0</span>] = self._start<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, max_length):<br>            <span class="hljs-comment"># 2当前单词输入为之前RNN的输出 </span><br>            prev_word_embed = W_embed[prev_word_idx]<br>            <span class="hljs-comment"># 4循环调用rnn_step_forward或lstm_step_forward函数</span><br>            <span class="hljs-keyword">if</span> self.cell_type == <span class="hljs-string">&#x27;rnn&#x27;</span>:<br>                next_h, rnn_step_cache = rnn_step_forward(prev_word_embed, prev_h,<br>                                                          Wx, Wh, b)<br>            <span class="hljs-keyword">elif</span> self.cell_type == <span class="hljs-string">&#x27;lstm&#x27;</span>:<br>                next_h, next_c, lstm_step_cache = lstm_step_forward(prev_word_embed, prev_h,<br>                                                          prev_c, Wx, Wh, b)<br>                prev_c = next_c<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid cell_type &quot;%s&quot;&#x27;</span> % self.cell_type)<br>            vocab_affine_out, vocab_affine_out_cache = affine_forward(next_h, <br>                            W_vocab, b_vocab)<br>            <span class="hljs-comment"># 3计算所有单词得分，然后选取最大得分作为预测单词 </span><br>            captions[:, i] = <span class="hljs-built_in">list</span>(np.argmax(vocab_affine_out, axis=<span class="hljs-number">1</span>))<br>            prev_word_idx = captions[:, i]<br>            prev_h = next_h<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">return</span> captions<br><br></code></pre></td></tr></table></figure><p><strong>rnn_layers.py</strong></p><p>RNN隐藏层需要使用到的方法，包括了RNN、LSTM以及词嵌入的前向传播和反向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn_step_forward</span>(<span class="hljs-params">x, prev_h, Wx, Wh, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    RNN单步前向传播，使用tanh激活单元</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 当前时间步数据输入(N, D).</span><br><span class="hljs-string">    - prev_h: 前一时间步隐藏层状态 (N, H)</span><br><span class="hljs-string">    - Wx: 输入层到隐藏层连接权重(D, H)</span><br><span class="hljs-string">    - Wh:隐藏层到隐藏层连接权重(H, H)</span><br><span class="hljs-string">    - b: 隐藏层偏置项(H,)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - next_h: 下一隐藏层状态(N, H)</span><br><span class="hljs-string">    - cache: 缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    next_h, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                        任务：实现RNN单步前向传播                              #</span><br>    <span class="hljs-comment">#                         将输出值储存在next_h中，                              #</span><br>    <span class="hljs-comment">#                 将反向传播时所需的各项缓存存放在cache中                         #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment"># 计算神经元输入</span><br>    a = prev_h.dot(Wh)+x.dot(Wx)+b<br>    <span class="hljs-comment"># 神经元激活</span><br>    next_h = np.tanh(a)<br>    <span class="hljs-comment"># 保留过程中的数据</span><br>    cache = (x, prev_h, Wh, Wx, b, next_h)    <br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                      结束编码                                               #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> next_h, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn_step_backward</span>(<span class="hljs-params">dnext_h, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    RNN单步反向传播。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dnext_h: 后一时间片段的梯度。</span><br><span class="hljs-string">    - cache: 前向传播时的缓存。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 数据梯度(N, D)。</span><br><span class="hljs-string">    - dprev_h: 前一时间片段梯度(N, H)。</span><br><span class="hljs-string">    - dWx: 输入层到隐藏层权重梯度(D,H)。</span><br><span class="hljs-string">    - dWh:    隐藏层到隐藏层权重梯度(H, H)。</span><br><span class="hljs-string">    - db: 偏置项梯度(H,)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dprev_h, dWx, dWh, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                            任务：实现RNN单步反向传播                           #</span><br>    <span class="hljs-comment">#            提示：tanh(x)梯度:    1 - tanh(x)*tanh(x)                         # </span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment"># 获取缓存数据</span><br>    x, prev_h, Wh, Wx, b, next_h = cache<br>    <span class="hljs-comment"># 根据链式求导法则依次计算各个变量的梯度</span><br>    dscores = dnext_h*(<span class="hljs-number">1</span>-next_h*next_h)<br>    dWx = np.dot(x.T, dscores)<br>    db = np.<span class="hljs-built_in">sum</span>(dscores, axis=<span class="hljs-number">0</span>)<br>    dWh = np.dot(prev_h.T, dscores)<br>    dx = np.dot(dscores, Wx.T)<br>    dprev_h = np.dot(dscores, Wh.T)<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                                                             结束编码                                                                         #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dprev_h, dWx, dWh, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn_forward</span>(<span class="hljs-params">x, h0, Wx, Wh, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    RNN前向传播。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 完整的时序数据 (N, T, D)。</span><br><span class="hljs-string">    - h0: 隐藏层初始化状态 (N, H)。</span><br><span class="hljs-string">    - Wx: 输入层到隐藏层权重 (D, H)。</span><br><span class="hljs-string">    - Wh:    隐藏层到隐藏层权重(H, H)。</span><br><span class="hljs-string">    - b: 偏置项(H,)。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - h: 所有时间步隐藏层状态(N, T, H)。</span><br><span class="hljs-string">    - cache: 反向传播所需的缓存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    h, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                              任务：实现RNN前向传播。                           #</span><br>    <span class="hljs-comment">#                提示： 使用前面实现的rnn_step_forward 函数。                     #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment"># 获取数据维度</span><br>    N, T, D = x.shape<br>    (H, ) = b.shape<br>    <span class="hljs-comment"># 初始化h</span><br>    h = np.zeros((N, T, H))<br>    <span class="hljs-comment"># 获取默认隐藏层状态</span><br>    prev_h = h0<br>    <span class="hljs-comment"># 遍历所有时间</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T):<br>        <span class="hljs-comment"># 获取当前时间片段</span><br>        xt = x[:, t, :]<br>        <span class="hljs-comment"># 计算每一个片段</span><br>        next_h, _ = rnn_step_forward(xt, prev_h, Wx, Wh, b)<br>        <span class="hljs-comment"># 更新状态</span><br>        prev_h = next_h<br>        <span class="hljs-comment"># 保留结果</span><br>        h[:, t, :] = prev_h<br>    <span class="hljs-comment"># 数据缓存，</span><br>    cache = (x, h0, Wh, Wx, b, h)<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                              结束编码                                                                                 #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> h, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn_backward</span>(<span class="hljs-params">dh, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    RNN反向传播。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dh: 隐藏层所有时间步梯度(N, T, H)。</span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据时序梯度(N, T, D)。</span><br><span class="hljs-string">    - dh0: 初始隐藏层梯度(N, H)。</span><br><span class="hljs-string">    - dWx: 输入层到隐藏层权重梯度(D, H)。</span><br><span class="hljs-string">    - dWh: 隐藏层到隐藏层权重梯度(H, H)。</span><br><span class="hljs-string">    - db: 偏置项梯度(H,)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dh0, dWx, dWh, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                              任务：实现RNN反向传播。                           #</span><br>    <span class="hljs-comment">#                        提示：使用 rnn_step_backward函数。                     #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment"># 获取缓存数据</span><br>    x, h0, Wh, Wx, b, h = cache <br>    <span class="hljs-comment"># 获取数据维度</span><br>    N, T, H = dh.shape<br>    _, _, D = x.shape<br>    <span class="hljs-comment"># 得到最后的细胞状态</span><br>    next_h = h[:, T-<span class="hljs-number">1</span>, :]<br>    <span class="hljs-comment"># 初始化</span><br>    dprev_h = np.zeros((N, H))<br>    dx = np.zeros((N, T, D))<br>    dh0 = np.zeros((N, H))<br>    dWx = np.zeros((D, H))<br>    dWh = np.zeros((H, H))<br>    db = np.zeros((H,))<br>    <span class="hljs-comment"># 遍历所有时间片段</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T):<br>        <span class="hljs-comment"># 当前处理的时间片段（从后往前）</span><br>        t = T-<span class="hljs-number">1</span>-t<br>        <span class="hljs-comment"># 获取对应的数据</span><br>        xt = x[:, t, :]<br>        <span class="hljs-comment"># 最初时间片段的之前细胞状态默认为h0</span><br>        <span class="hljs-keyword">if</span> t == <span class="hljs-number">0</span>:<br>            prev_h = h0<br>        <span class="hljs-keyword">else</span>:<br>            prev_h = h[:, t-<span class="hljs-number">1</span>, :]<br>        <span class="hljs-comment"># 获取缓存数据 </span><br>        step_cache = (xt, prev_h, Wh, Wx, b, next_h)<br>        <span class="hljs-comment"># 更新状态</span><br>        next_h = prev_h<br>        dnext_h = dh[:, t, :]+dprev_h<br>        <span class="hljs-comment"># 进行反向传播</span><br>        dx[:, t, :], dprev_h, dWxt, dWht, dbt = rnn_step_backward(dnext_h, step_cache)<br>        <span class="hljs-comment"># 状态累加</span><br>        dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbt<br>    <span class="hljs-comment"># 记录h0的梯度</span><br>    dh0 = dprev_h<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                                    结束编码                                  #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dh0, dWx, dWh, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">word_embedding_forward</span>(<span class="hljs-params">x, W</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    词嵌入前向传播，将数据矩阵中的N条长度为T的词索引转化为词向量。</span><br><span class="hljs-string">    如：W[x[i,j]]表示第i条，第j时间步单词索引所对应的词向量。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 整数型数组(N,T),N表示数据条数，T表示单条数据长度，</span><br><span class="hljs-string">        数组的每一元素存放着单词索引，取值范围[0,V)。</span><br><span class="hljs-string">    - W: 词向量矩阵(V,D)存放各单词对应的向量。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out:输出词向量(N, T, D)。 </span><br><span class="hljs-string">    - cache:反向传播时所需的缓存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                           任务：实现词嵌入前向传播。                            #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment"># 获取数据维度</span><br>    N, T = x.shape<br>    V, D = W.shape<br>    <span class="hljs-comment"># 初始化</span><br>    out = np.zeros((N, T, D))<br>    <span class="hljs-comment"># 遍历所有数据</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T):<br>            <span class="hljs-comment"># 将其转化为词向量</span><br>            out[i, j] = W[x[i, j]]<br>    cache = (x, W.shape)<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                                        结束编码                              #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">word_embedding_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    词嵌入反向传播</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度 (N, T, D)</span><br><span class="hljs-string">    - cache:前向传播缓存</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dW: 词嵌入矩阵梯度(V, D).</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dW = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                          任务：实现词嵌入反向传播                               #</span><br>    <span class="hljs-comment">#                     提示：你可以使用np.add.at函数                              #</span><br>    <span class="hljs-comment">#            例如 np.add.at(a,[1,2],1)相当于a[1],a[2]分别加1                    #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    x, W_shape = cache<br>    dW = np.zeros(W_shape)<br>    <span class="hljs-comment"># np.add.at()是将传入的数组中制定下标位置的元素加上指定的值.</span><br>    np.add.at(dW, x, dout)<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                                                             结束编码                                                                         #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> dW<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    数值稳定版本的sigmoid函数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    pos_mask = (x &gt;= <span class="hljs-number">0</span>)<br>    neg_mask = (x &lt; <span class="hljs-number">0</span>)<br>    z = np.zeros_like(x)<br>    z[pos_mask] = np.exp(-x[pos_mask])<br>    z[neg_mask] = np.exp(x[neg_mask])<br>    top = np.ones_like(x)<br>    top[neg_mask] = z[neg_mask]<br>    <span class="hljs-keyword">return</span> top / (<span class="hljs-number">1</span> + z)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lstm_step_forward</span>(<span class="hljs-params">x, prev_h, prev_c, Wx, Wh, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    LSTM单步前向传播</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据 (N, D)</span><br><span class="hljs-string">    - prev_h: 前一隐藏层状态 (N, H)</span><br><span class="hljs-string">    - prev_c: 前一细胞状态(N, H)</span><br><span class="hljs-string">    - Wx: 输入层到隐藏层权重(D, 4H)</span><br><span class="hljs-string">    - Wh: 隐藏层到隐藏层权重 (H, 4H)</span><br><span class="hljs-string">    - b: 偏置项(4H,)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - next_h:    下一隐藏层状态(N, H)</span><br><span class="hljs-string">    - next_c:    下一细胞状态(N, H)</span><br><span class="hljs-string">    - cache: 反向传播所需的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    next_h, next_c, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                            任务：实现LSTM单步前向传播。                        #</span><br>    <span class="hljs-comment">#                 提示：稳定版本的sigmoid函数已经帮你实现，直接调用即可。            #</span><br>    <span class="hljs-comment">#                             tanh函数使用np.tanh。                           #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 获取数据</span><br>    N, D = x.shape<br>    N, H = prev_h.shape<br>    <span class="hljs-comment"># 计算输入门、遗忘门、输出门</span><br>    input_gate = sigmoid(np.dot(x, Wx[:, <span class="hljs-number">0</span>:H])+np.dot(prev_h, Wh[:, <span class="hljs-number">0</span>:H])+b[<span class="hljs-number">0</span>:H])<br>    forget_gate = sigmoid(np.dot(x, Wx[:, H:<span class="hljs-number">2</span>*H])+np.dot(prev_h, Wh[:, H:<span class="hljs-number">2</span>*H])<br>                          +b[H:<span class="hljs-number">2</span>*H])<br>    output_gate = sigmoid(np.dot(x, Wx[:, <span class="hljs-number">2</span>*H:<span class="hljs-number">3</span>*H])+np.dot(prev_h, Wh[:, <span class="hljs-number">2</span>*H:<span class="hljs-number">3</span>*H])<br>                          +b[<span class="hljs-number">2</span>*H:<span class="hljs-number">3</span>*H])<br>    <span class="hljs-comment"># 计算输出单元</span><br>    input_data = np.tanh(np.dot(x, Wx[:, <span class="hljs-number">3</span>*H:<span class="hljs-number">4</span>*H])+np.dot(prev_h, Wh[:, <span class="hljs-number">3</span>*H:<span class="hljs-number">4</span>*H])<br>                         +b[<span class="hljs-number">3</span>*H:<span class="hljs-number">4</span>*H])<br>    <span class="hljs-comment"># 更新细胞记忆</span><br>    next_c = forget_gate*prev_c+input_data*input_gate<br>    <span class="hljs-comment"># 计算细胞输出</span><br>    next_scores_c = np.tanh(next_c)<br>    next_h = output_gate*next_scores_c<br>    cache = (x, Wx, Wh, b, input_data, input_gate, output_gate, forget_gate,<br>             prev_h, prev_c, next_scores_c)<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                             结束编码                                         #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-keyword">return</span> next_h, next_c, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lstm_step_backward</span>(<span class="hljs-params">dnext_h, dnext_c, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     LSTM单步反向传播</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dnext_h: 下一隐藏层梯度 (N, H)</span><br><span class="hljs-string">    - dnext_c: 下一细胞梯度 (N, H)</span><br><span class="hljs-string">    - cache: 前向传播缓存</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据梯度 (N, D)</span><br><span class="hljs-string">    - dprev_h: 前一隐藏层梯度 (N, H)</span><br><span class="hljs-string">    - dprev_c: 前一细胞梯度(N, H)</span><br><span class="hljs-string">    - dWx: 输入层到隐藏层梯度(D, 4H)</span><br><span class="hljs-string">    - dWh:    隐藏层到隐藏层梯度(H, 4H)</span><br><span class="hljs-string">    - db:    偏置梯度(4H,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dprev_h, dc, dWx, dWh, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                      任务：实现LSTM单步反向传播                               #</span><br>    <span class="hljs-comment">#       提示：sigmoid(x)函数梯度：sigmoid(x)*(1-sigmoid(x))                    #</span><br>    <span class="hljs-comment">#             tanh(x)函数梯度：     1-tanh(x)*tanh(x)                         #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 获取数据</span><br>    x, Wx, Wh, b, input_data, input_gate, output_gate, forget_gate, prev_h,\<br>        prev_c, next_scores_c = cache<br>    N, D = x.shape<br>    N, H = prev_h.shape<br>    <span class="hljs-comment"># 初始化变量</span><br>    dWx = np.zeros((D, <span class="hljs-number">4</span>*H))<br>    dxx = np.zeros((D, <span class="hljs-number">4</span>*H))<br>    dWh = np.zeros((H, <span class="hljs-number">4</span>*H))<br>    dhh = np.zeros((H, <span class="hljs-number">4</span>*H))<br>    db = np.zeros(<span class="hljs-number">4</span>*H)<br>    dx = np.zeros((N, D))<br>    dprev_h = np.zeros((N, H))<br>    <span class="hljs-comment"># 计算当前细胞的梯度</span><br>    dc_tem = dnext_c+dnext_h*(<span class="hljs-number">1</span>-next_scores_c**<span class="hljs-number">2</span>)*output_gate<br>    <span class="hljs-comment"># 求解tanh层</span><br>    dprev_c = forget_gate*dc_tem<br>    dforget_gate = prev_c*dc_tem<br>    dinput_gate = input_data*dc_tem<br>    dinput = input_gate*dc_tem<br>    doutput_gate = next_scores_c*dnext_h<br>    <span class="hljs-comment"># 求解sigmoid层</span><br>    dscores_in_gate = input_gate*(<span class="hljs-number">1</span>-input_gate)*dinput_gate<br>    dscores_forget_gate = forget_gate*(<span class="hljs-number">1</span>-forget_gate)*dforget_gate<br>    dscores_out_gate = output_gate*(<span class="hljs-number">1</span>-output_gate)*doutput_gate<br>    dscores_in = (<span class="hljs-number">1</span>-input_data**<span class="hljs-number">2</span>)*dinput<br>    da = np.hstack((dscores_in_gate, dscores_forget_gate, dscores_out_gate, dscores_in))<br>    dWx = np.dot(x.T, da)<br>    dWh = np.dot(prev_h.T, da)<br>    db = np.<span class="hljs-built_in">sum</span>(da, axis=<span class="hljs-number">0</span>)<br>    dx = np.dot(da, Wx.T)<br>    dprev_h = np.dot(da, Wh.T)<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                           结束编码                                           #</span><br>    <span class="hljs-comment">##############################################################################</span><br><br>    <span class="hljs-keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lstm_forward</span>(<span class="hljs-params">x, h0, Wx, Wh, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    LSTM前向传播</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据 (N, T, D)</span><br><span class="hljs-string">    - h0:初始化隐藏层状态(N, H)</span><br><span class="hljs-string">    - Wx: 输入层到隐藏层权重 (D, 4H)</span><br><span class="hljs-string">    - Wh: 隐藏层到隐藏层权重(H, 4H)</span><br><span class="hljs-string">    - b: 偏置项(4H,)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - h: 隐藏层所有状态 (N, T, H)</span><br><span class="hljs-string">    - cache: 用于反向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    h, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                    任务： 实现完整的LSTM前向传播                              #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 获取数据</span><br>    N, T, D = x.shape<br>    H = <span class="hljs-built_in">int</span>(b.shape[<span class="hljs-number">0</span>]/<span class="hljs-number">4</span>)<br>    <span class="hljs-comment"># 初始化信息</span><br>    h = np.zeros((N, T, H))<br>    cache = &#123;&#125;<br>    prev_h = h0<br>    prev_c = np.zeros((N, H))<br>    <span class="hljs-comment"># 遍历所有时序数据</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T):<br>        <span class="hljs-comment"># 当前数据</span><br>        xt = x[:, t, :]<br>        <span class="hljs-comment"># 进行单步LSTM前向传播</span><br>        next_h, next_c, cache[t] = lstm_step_forward(xt, prev_h, prev_c, Wx, Wh, b)<br>        <span class="hljs-comment"># 更新状态</span><br>        prev_h = next_h<br>        prev_c = next_c<br>        h[:, t, :] = prev_h<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                          结束编码                                            #</span><br>    <span class="hljs-comment">##############################################################################</span><br><br>    <span class="hljs-keyword">return</span> h, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lstm_backward</span>(<span class="hljs-params">dh, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    LSTM反向传播</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dh: 各隐藏层梯度(N, T, H)</span><br><span class="hljs-string">    - cache: V前向传播缓存</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据梯度 (N, T, D)</span><br><span class="hljs-string">    - dh0:初始隐藏层梯度(N, H)</span><br><span class="hljs-string">    - dWx: 输入层到隐藏层权重梯度 (D, 4H)</span><br><span class="hljs-string">    - dWh: 隐藏层到隐藏层权重梯度 (H, 4H)</span><br><span class="hljs-string">    - db: 偏置项梯度 (4H,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dh0, dWx, dWh, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#               任务：实现完整的LSTM反向传播                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 获取数据</span><br>    N, T, H = dh.shape<br>    <span class="hljs-comment"># 从最后一条开始更新</span><br>    x, Wx, Wh, b, input_data, input_gate, output_gate, forget_gate, prev_h, prev_c,\<br>        next_scores_c = cache[T-<span class="hljs-number">1</span>]<br>    D = x.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># 初始化</span><br>    dprev_h = np.zeros((N, H))<br>    dprev_c = np.zeros((N, H))<br>    dx = np.zeros((N, T, D))<br>    dh0 = np.zeros((N, H))<br>    dWx = np.zeros((D, <span class="hljs-number">4</span>*H))<br>    dWh = np.zeros((H, <span class="hljs-number">4</span>*H))<br>    db = np.zeros((<span class="hljs-number">4</span>*H,))<br>    <span class="hljs-comment"># 遍历所有数据</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(T):<br>        <span class="hljs-comment"># 选择当前时间（从后向前）</span><br>        t = T-<span class="hljs-number">1</span>-t<br>        <span class="hljs-comment"># 获取数据</span><br>        step_cache = cache[t]<br>        dnext_h = dh[:, t, :]+dprev_h<br>        dnext_c = dprev_c<br>        <span class="hljs-comment"># 进行单步反向传播计算</span><br>        dx[:, t, :], dprev_h, dprev_c, dWxt, dWht, dbt = lstm_step_backward(dnext_h,<br>                        dnext_c, step_cache)<br>        <span class="hljs-comment"># 更新参数</span><br>        dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbt<br>    <span class="hljs-comment"># 更新h0梯度</span><br>    dh0 = dprev_h<br>    <span class="hljs-comment">##############################################################################</span><br>    <span class="hljs-comment">#                            结束编码                                          #</span><br>    <span class="hljs-comment">##############################################################################</span><br>    <br>    <span class="hljs-keyword">return</span> dx, dh0, dWx, dWh, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">temporal_affine_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    时序隐藏层仿射传播：将隐藏层时序数据(N,T,D)重塑为(N*T,D)，</span><br><span class="hljs-string">    完成前向传播后，再重塑回原型输出。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 时序数据(N, T, D)。</span><br><span class="hljs-string">    - w: 权重(D, M)。</span><br><span class="hljs-string">    - b: 偏置(M,)。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 输出(N, T, M)。</span><br><span class="hljs-string">    - cache: 反向传播缓存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    N, T, D = x.shape<br>    M = b.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># Affine层</span><br>    out = x.reshape(N * T, D).dot(w).reshape(N, T, M) + b<br>    cache = x, w, b, out<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">temporal_affine_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    时序隐藏层仿射反向传播。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout:上层梯度 (N, T, M)。</span><br><span class="hljs-string">    - cache: 前向传播缓存。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入梯度(N, T, D)。</span><br><span class="hljs-string">    - dw: 权重梯度 (D, M)。</span><br><span class="hljs-string">    - db: 偏置项梯度 (M,)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b, out = cache<br>    N, T, D = x.shape<br>    M = b.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># Affine层反向传播</span><br>    dx = dout.reshape(N * T, M).dot(w.T).reshape(N, T, D)<br>    dw = dout.reshape(N * T, M).T.dot(x.reshape(N * T, D)).T<br>    db = dout.<span class="hljs-built_in">sum</span>(axis=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">temporal_softmax_loss</span>(<span class="hljs-params">x, y, mask, verbose=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    时序版本的Softmax损失和原版本类似，只需将数据(N, T, V)重塑为(N*T,V)即可。</span><br><span class="hljs-string">    需要注意的是，对于NULL标记不计入损失值，因此，你需要加入掩码进行过滤。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据得分(N, T, V)。</span><br><span class="hljs-string">    - y: 目标索引(N, T)，其中0&lt;= y[i, t] &lt; V。</span><br><span class="hljs-string">    - mask: 过滤NULL标记的掩码。</span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - loss: 损失值。</span><br><span class="hljs-string">    - dx: x梯度。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 获取必备信息</span><br>    N, T, V = x.shape<br>    <br>    x_flat = x.reshape(N * T, V)<br>    y_flat = y.reshape(N * T)<br>    mask_flat = mask.reshape(N * T)<br>    <br>    <span class="hljs-comment"># 和原有softmax类似，不足的部分使用NULL补充，计算的时候过滤</span><br>    probs = np.exp(x_flat - np.<span class="hljs-built_in">max</span>(x_flat, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    probs /= np.<span class="hljs-built_in">sum</span>(probs, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    loss = -np.<span class="hljs-built_in">sum</span>(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N<br>    dx_flat = probs.copy()<br>    dx_flat[np.arange(N * T), y_flat] -= <span class="hljs-number">1</span><br>    dx_flat /= N<br>    dx_flat *= mask_flat[:, <span class="hljs-literal">None</span>]<br>    <br>    <span class="hljs-comment"># 是否打印</span><br>    <span class="hljs-keyword">if</span> verbose: <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;dx_flat: &#x27;</span>, dx_flat.shape)<br>    <br>    dx = dx_flat.reshape(N, T, V)<br>    <br>    <span class="hljs-keyword">return</span> loss, dx<br><br><br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>编码实现RNN以及LSTM</div><div>https://fulequn.github.io/2021/11/Article202111225/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年11月22日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2021/11/Article202111226/" title="《深度学习实战》汇总"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">《深度学习实战》汇总</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/11/Article202111224/" title="《深度学习实战》第7章 循环神经网络"><span class="hidden-mobile">《深度学习实战》第7章 循环神经网络</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>