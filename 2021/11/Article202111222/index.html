<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="参考链接： https:&#x2F;&#x2F;mofanpy.com&#x2F;tutorials&#x2F;machine-learning&#x2F;reinforcement-learning&#x2F;intro-RL&#x2F; https:&#x2F;&#x2F;www.cnblogs.com&#x2F;maybe2030&#x2F;p&#x2F;9862353.html   第1章 简介 强化学习是机器学习大家族中的一大类, 使用强化学习能够让机器学着如何在环境中拿到高分, 表现出优秀的成绩. 而"><meta property="og:type" content="article"><meta property="og:title" content="莫烦强化学习-简介"><meta property="og:url" content="https://fulequn.github.io/2021/11/Article202111222/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="参考链接： https:&#x2F;&#x2F;mofanpy.com&#x2F;tutorials&#x2F;machine-learning&#x2F;reinforcement-learning&#x2F;intro-RL&#x2F; https:&#x2F;&#x2F;www.cnblogs.com&#x2F;maybe2030&#x2F;p&#x2F;9862353.html   第1章 简介 强化学习是机器学习大家族中的一大类, 使用强化学习能够让机器学着如何在环境中拿到高分, 表现出优秀的成绩. 而"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230921860.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230927486.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230933491.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230935893.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111231203925.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230959396.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111231004207.png"><meta property="og:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111231008992.png"><meta property="article:published_time" content="2021-11-22T05:25:49.000Z"><meta property="article:modified_time" content="2024-05-18T14:35:07.673Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="动态规划"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230921860.png"><title>莫烦强化学习-简介 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="莫烦强化学习-简介"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-11-22 13:25" pubdate>2021年11月22日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>25 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">莫烦强化学习-简介</h1><div class="markdown-body"><blockquote><p>参考链接：</p><p><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL/">https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL/</a></p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/maybe2030/p/9862353.html">https://www.cnblogs.com/maybe2030/p/9862353.html</a></p></blockquote><h1 id="第1章-简介"><a class="markdownIt-Anchor" href="#第1章-简介"></a> 第1章 简介</h1><p>强化学习是机器学习大家族中的一大类, 使用强化学习能够让机器学着如何在环境中拿到高分, 表现出优秀的成绩. 而这些成绩背后却是他所付出的辛苦劳动, 不断的试错, 不断地尝试, 累积经验, 学习经验。</p><h2 id="11-什么是强化学习"><a class="markdownIt-Anchor" href="#11-什么是强化学习"></a> 1.1 什么是强化学习</h2><h3 id="111-从无到有"><a class="markdownIt-Anchor" href="#111-从无到有"></a> 1.1.1 从无到有</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230921860.png" srcset="/img/loading.gif" lazyload alt="1"></p><p>强化学习是一类算法, 是让计算机实现<strong>从一开始什么都不懂, 脑袋里没有一点想法, 通过不断地尝试, 从错误中学习, 最后找到规律, 学会了达到目的</strong>的方法。</p><p>强化学习（Reinforcement Learning，简称RL）是机器学习中的一个领域，强调<strong>如何基于环境而行动，以取得最大化的预期利益</strong>。</p><h2 id="112-虚拟老师"><a class="markdownIt-Anchor" href="#112-虚拟老师"></a> 1.1.2 虚拟老师</h2><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230927486.png" srcset="/img/loading.gif" lazyload alt="2"></p><p>原来计算机也需要一位虚拟的老师, 这个老师比较吝啬, <strong>他不会告诉你如何移动, 如何做决定, 他为你做的事只有给你的行为打分</strong>, 那我们应该以什么形式学习这些现有的资源, 或者说怎么样只从分数中学习到我应该怎样做决定呢? 很简单, <strong>我只需要记住那些高分, 低分对应的行为, 下次用同样的行为拿高分, 并避免低分的行为</strong>。</p><p>比如老师会根据我的开心程度来打分, 我开心时, 可以得到高分, 我不开心时得到低分。有了这些被打分的经验, 我就能判断为了拿到高分, 我应该选择一张开心的脸, 避免选到伤心的脸。这也是强化学习的核心思想。可以看出在强化学习中, 一种行为的分数是十分重要的。所以强化学习具有<strong>分数导向性</strong>。我们换一个角度来思考。这种分数导向性好比我们在监督学习中的正确标签。</p><h3 id="113-对比监督学习"><a class="markdownIt-Anchor" href="#113-对比监督学习"></a> 1.1.3 对比监督学习</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230933491.png" srcset="/img/loading.gif" lazyload alt="3"></p><p>我们知道监督学习, 是已经有了数据和数据对应的正确标签, 比如这样。 监督学习就能学习出那些脸对应哪种标签。 <strong>不过强化学习还要更进一步, 一开始它并没有数据和标签。</strong></p><p><strong>他要通过一次次在环境中的尝试, 获取这些数据和标签, 然后再学习通过哪些数据能够对应哪些标签, 通过学习到的这些规律, 竟可能地选择带来高分的行为 (比如这里的开心脸)。 这也就证明了在强化学习中, 分数标签就是他的老师, 他和监督学习中的老师也差不多。</strong></p><h3 id="114-rl算法们"><a class="markdownIt-Anchor" href="#114-rl算法们"></a> 1.1.4 RL算法们</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230935893.png" srcset="/img/loading.gif" lazyload alt="4"></p><p>强化学习是一个大家族, 他包含了很多种算法, 我们也会一一提到之中一些比较有名的算法, 比如有通过行为的价值来选取特定行为的方法, 包括使用表格学习的 q learning, sarsa, 使用神经网络学习的 deep q network, 还有直接输出行为的 policy gradients, 又或者了解所处的环境, 想象出一个虚拟的环境并从虚拟的环境中学习 等等。</p><h2 id="12-强化学习汇总"><a class="markdownIt-Anchor" href="#12-强化学习汇总"></a> 1.2 强化学习汇总</h2><h3 id="121-modelfree-和-modelbased"><a class="markdownIt-Anchor" href="#121-modelfree-和-modelbased"></a> 1.2.1 Modelfree 和 Modelbased</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111231203925.png" srcset="/img/loading.gif" lazyload alt="5"></p><p>我们可以将所有强化学习的方法分为<strong>理不理解所处环境,如果我们不尝试去理解环境, 环境给了我们什么就是什么. 我们就把这种方法叫做 model-free, 这里的 model 就是用模型来表示环境, 那理解了环境也就是学会了用一个模型来代表环境, 所以这种就是 model-based 方法。</strong> 我们想象，现在环境就是我们的世界, 我们的机器人正在这个世界里玩耍, 他不理解这个世界是怎样构成的, 也不理解世界对于他的行为会怎么样反馈。举个例子, 他决定丢颗原子弹去真实的世界, 结果把自己给炸死了, 所有结果都是那么现实（model-free，因为现实世界遭遇原子弹爆炸是会把机器人炸死的，是现实环境做出的反馈）。不过如果采取的是 model-based RL, 机器人会通过过往的经验, <strong>先理解真实世界是怎样的, 并建立一个模型来模拟现实世界的反馈</strong>, 最后他不仅可以在现实世界中玩耍, 也能在模拟的世界中玩耍, 这样就没必要去炸真实世界, 连自己也炸死了, 他可以像玩游戏一样炸炸游戏里的世界（在模拟环境中进行训练，现实中的机器人就不会被炸死）, 也保住了自己的小命。</p><p>Model-free 的方法有很多, 像 Q learning, Sarsa, Policy Gradients 都是从环境中得到反馈然后从中学习。而 model-based RL 只是多了一道程序, <strong>为真实世界建模</strong>（建立一个模拟世界）, 也可以说他们都是 model-free 的强化学习, 只是 model-based 多出了一个虚拟环境, 我们不仅可以像 model-free 那样在现实中玩耍,还能在游戏中玩耍, 而玩耍的方式也都是 model-free 中那些玩耍方式, 最终 model-based 还有一个杀手锏是 model-free 超级羡慕的。那就是<strong>想象力</strong>。</p><p>**Model-free 中, 机器人只能按部就班, 一步一步等待真实世界的反馈, 再根据反馈采取下一步行动。而 model-based, 他能通过想象来预判断接下来将要发生的所有情况。然后选择这些想象情况中最好的那种。**并依据这种情况来采取下一步的策略, 这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来, 我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p><h3 id="122-基于概率和基于价值"><a class="markdownIt-Anchor" href="#122-基于概率和基于价值"></a> 1.2.2 基于概率和基于价值</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111230959396.png" srcset="/img/loading.gif" lazyload alt="6"></p><p><strong>基于概率是强化学习中最直接的一种, 他能通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动, 所以每种动作都有可能被选中, 只是可能性不同。而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选动作, 相比基于概率的方法, 基于价值的决策部分更为铁定, 毫不留情, 就选价值最高的, 而基于概率的, 即使某个动作的概率最高, 但是还是不一定会选到他。</strong></p><p>我们现在说的动作都是一个一个不连续的动作, 而<strong>对于选取连续的动作, 基于价值的方法是无能为力的</strong>。我们却能用一个概率分布在连续动作中选取特定动作, 这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢?</p><p>比如在基于概率这边, 有 Policy Gradients, 在基于价值这边有 Q learning, Sarsa 等. 而且我们还能结合这两类方法的优势之处, 创造更牛逼的一种方法, 叫做 <strong>Actor-Critic, actor 会基于概率做出动作, 而 critic 会对做出的动作给出动作的价值, 这样就在原有的 policy gradients 上加速了学习过程.</strong></p><h3 id="123-回合更新和单步更新"><a class="markdownIt-Anchor" href="#123-回合更新和单步更新"></a> 1.2.3 回合更新和单步更新</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111231004207.png" srcset="/img/loading.gif" lazyload alt="7"></p><p><strong>强化学习还能用另外一种方式分类, 回合更新和单步更新</strong>, 想象强化学习就是在玩游戏, 游戏回合有开始和结束。<strong>回合更新指的是游戏开始后, 我们要等待游戏结束, 然后再总结这一回合中的所有转折点, 再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新, 不用等待游戏的结束, 这样我们就能边玩边学习了。</strong></p><p>再来说说方法, Monte-carlo learning 和基础版的 policy gradients 等 都是回合更新制, Qlearning, Sarsa, 升级版的 policy gradients 等都是单步更新制。<strong>因为单步更新更有效率, 所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</strong></p><h3 id="124-在线学习和离线学习"><a class="markdownIt-Anchor" href="#124-在线学习和离线学习"></a> 1.2.4 在线学习和离线学习</h3><p><img src="https://raw.githubusercontent.com/fulequn/oss_img/master/img211/202111231008992.png" srcset="/img/loading.gif" lazyload alt="8"></p><p>这个视频的最后一种分类方式是 <strong>在线学习和离线学习, 所谓在线学习, 就是指我必须本人在场, 并且一定是本人边玩边学习, 而离线学习是你可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则, 离线学习 同样是从过往的经验中学习, 但是这些过往的经历没必要是自己的经历, 任何人的经历都能被学习</strong>。或者我也不必要边玩边学习, 我可以白天先存储下来玩耍时的记忆, 然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢?</p><p>最典型的在线学习就是 Sarsa 了, 还有一种优化 Sarsa 的算法, 叫做 Sarsa lambda, 最典型的离线学习就是 Q learning, 后来人也根据离线学习的属性, 开发了更强大的算法, 比如让计算机学会玩电动的 Deep-Q-Network。</p><h3 id="125-总结"><a class="markdownIt-Anchor" href="#125-总结"></a> 1.2.5 总结</h3><table><thead><tr><th>分类标准</th><th>model-free</th><th>model-based</th></tr></thead><tbody><tr><td>理不理解环境</td><td>Q learning, Sarsa, Policy Gradients</td><td>model-based RL</td></tr><tr><td></td><td>基于概率</td><td>基于价值</td></tr><tr><td>价值与概率</td><td>Policy Gradients</td><td>Q learning, Sarsa</td></tr><tr><td></td><td>回合更新</td><td>单步更新</td></tr><tr><td>更新制</td><td>Monte-carlo learning, 基础版的 policy gradients</td><td>Q learning, Sarsa, 升级版的 policy gradients</td></tr><tr><td></td><td>在线学习</td><td>离线学习</td></tr><tr><td>是否在线</td><td>Sarsa, Sarsa lambda</td><td>Q learning, Deep-Q-Network</td></tr></tbody></table><h2 id="13-为什么要用强化学习"><a class="markdownIt-Anchor" href="#13-为什么要用强化学习"></a> 1.3 为什么要用强化学习</h2><h3 id="131-强化学习介绍"><a class="markdownIt-Anchor" href="#131-强化学习介绍"></a> 1.3.1 强化学习介绍</h3><p>强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。</p><p>这些教程的教学, 不依赖于任何强化学习的 python 模块。因为强化学习的复杂性, 多样性, 到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p><h3 id="132-模拟程序提前看"><a class="markdownIt-Anchor" href="#132-模拟程序提前看"></a> 1.3.2 模拟程序提前看</h3><p>优酷的模拟视频在这里:</p><p><a target="_blank" rel="noopener" href="http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1">http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1</a></p><h2 id="14-课程要求"><a class="markdownIt-Anchor" href="#14-课程要求"></a> 1.4 课程要求</h2><h3 id="141-教程必备模块"><a class="markdownIt-Anchor" href="#141-教程必备模块"></a> 1.4.1 教程必备模块</h3><p>强化学习有一些现成的模块可以使用, 但是那些模块并不全面, 而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学习, 可能 RL 的代码就不同. 所以我们要抱着以不变应万变的心态, 用基础的模块, 从基础学起。 懂了原理, 再复杂的环境也不在话下。</p><p>所以用到的模块和对应的教程:</p><ul><li>Numpy, Pandas (必学), 用于学习的数据处理，深度学习中也经常用到</li><li>Matplotlib(可学), 偶尔会用来呈现误差曲线什么的</li><li>Tkinter (可学), 你可以自己用它来编写模拟环境</li><li>Tensorflow(可学), 后面实现神经网络与强化学习结合的时候用到</li><li>OpenAI gym(可学), 提供了很多现成的模拟环境</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" class="print-no-link">#动态规划</a></div></div><div class="license-box my-3"><div class="license-title"><div>莫烦强化学习-简介</div><div>https://fulequn.github.io/2021/11/Article202111222/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年11月22日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2021/11/Article202111223/" title="录制动态帧视频在PR中音画不同步问题"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">录制动态帧视频在PR中音画不同步问题</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/11/Article202111221/" title="深度学习实战 第7章循环神经网络笔记"><span class="hidden-mobile">深度学习实战 第7章循环神经网络笔记</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>