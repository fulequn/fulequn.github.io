<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="cnn_layers.py 实现卷积神经网络的前向后传播的函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878"><meta property="og:type" content="article"><meta property="og:title" content="编码实现卷积神经网络"><meta property="og:url" content="https://fulequn.github.io/2021/01/Article202101103/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="cnn_layers.py 实现卷积神经网络的前向后传播的函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2021-01-10T11:32:08.000Z"><meta property="article:modified_time" content="2024-05-30T00:16:36.000Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="Python"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><title>编码实现卷积神经网络 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:60,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="编码实现卷积神经网络"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-01-10 19:32" pubdate>2021年1月10日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>6.5k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>55 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">编码实现卷积神经网络</h1><div class="markdown-body"><h3 id="cnn_layerspy"><a class="markdownIt-Anchor" href="#cnn_layerspy"></a> cnn_layers.py</h3><p>实现卷积神经网络的前向后传播的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> bn_layers <span class="hljs-keyword">import</span> *<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_forward_naive</span>(<span class="hljs-params">x, w, b, conv_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    卷积前向传播。</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 四维图片数据(N, C, H, W)分别表示(数量，色道，高，宽)</span><br><span class="hljs-string">    - w: 四维卷积核(F, C, HH, WW)分别表示(下层色道，上层色道，高，宽)</span><br><span class="hljs-string">    - b: 偏置项(F,)</span><br><span class="hljs-string">    - conv_param: 字典型参数表，其键值为:</span><br><span class="hljs-string">        - &#x27;stride&#x27;:跳跃数据卷积的跨幅数量</span><br><span class="hljs-string">        - &#x27;pad&#x27;:输入数据的零填充数量</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组型:</span><br><span class="hljs-string">    - out: 输出数据(N, F, H&#x27;, W&#x27;) ，其中 H&#x27; 和 W&#x27; 分别为：</span><br><span class="hljs-string">        H&#x27; = 1 + (H + 2 * pad - HH) / stride</span><br><span class="hljs-string">        W&#x27; = 1 + (W + 2 * pad - WW) / stride</span><br><span class="hljs-string">    - cache: (x, w, b, conv_param)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                         任务: 实现卷积层的前向传播                                                                    #</span><br>    <span class="hljs-comment">#                    提示: 你可以使用np.pad函数进行零填充                                                         #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 获取数据的各种数据量</span><br>    <span class="hljs-comment"># 数量，色道，高，宽</span><br>    N, C, H, W = x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>], x.shape[<span class="hljs-number">2</span>], x.shape[<span class="hljs-number">3</span>]<br>    <span class="hljs-comment"># 下层色道，高，宽</span><br>    F,HH,WW = w.shape[<span class="hljs-number">0</span>],w.shape[<span class="hljs-number">2</span>],w.shape[<span class="hljs-number">3</span>]<br>    <span class="hljs-comment"># 输入数据的零填充数量</span><br>    pad = conv_param[<span class="hljs-string">&#x27;pad&#x27;</span>]<br>    <span class="hljs-comment"># 跳跃数据进行卷积的跨幅数量</span><br>    stride = conv_param[<span class="hljs-string">&#x27;stride&#x27;</span>]<br>    <span class="hljs-comment"># 进行填充</span><br>    x_pad = np.pad(x, ((<span class="hljs-number">0</span>,), (<span class="hljs-number">0</span>,), (pad,), (pad,)), <span class="hljs-string">&#x27;constant&#x27;</span>)<br>    <br>    <span class="hljs-comment"># 计算循环次数</span><br>    Hhat = <span class="hljs-built_in">int</span>(<span class="hljs-number">1</span> + (H + <span class="hljs-number">2</span> * pad - HH) / stride)<br>    What= <span class="hljs-built_in">int</span>(<span class="hljs-number">1</span> + (W + <span class="hljs-number">2</span> * pad - WW) / stride)<br>    <span class="hljs-comment"># 输出值</span><br>    out = np.zeros([N,F,Hhat,What])<br>    <span class="hljs-comment"># 遍历所有数据的下层色道的高和宽</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>        <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F):<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Hhat):<br>                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(What):<br>                    xx =x_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW]<br>                    out[n,f,i,j] =np.<span class="hljs-built_in">sum</span>(xx*w[f])+b[f]<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = (x, w, b, conv_param)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_forward_fast</span>(<span class="hljs-params">x, w, b, conv_param</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    卷积前向传播的快速版本</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    x : 四维图片数据(N, C, H, W)分别表示(数量，色道，高，宽)</span><br><span class="hljs-string">    w : 四维卷积核(F, C, HH, WW)分别表示(下层色道，上层色道，高，宽)</span><br><span class="hljs-string">    b : 偏置项(F,)</span><br><span class="hljs-string">    conv_param : 字典型参数表，其键值为:</span><br><span class="hljs-string">        - &#x27;stride&#x27;:跳跃数据卷积的跨幅数量</span><br><span class="hljs-string">        - &#x27;pad&#x27;:输入数据的零填充数量</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    out : 输出数据(N, F, H&#x27;, W&#x27;) ，其中 H&#x27; 和 W&#x27; 分别为：</span><br><span class="hljs-string">        H&#x27; = 1 + (H + 2 * pad - HH) / stride</span><br><span class="hljs-string">        W&#x27; = 1 + (W + 2 * pad - WW) / stride</span><br><span class="hljs-string">    cache : (x, w, b, conv_param)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    N, C, H, W = x.shape<br>    F, _, HH, WW = w.shape<br>    stride, pad = conv_param[<span class="hljs-string">&#x27;stride&#x27;</span>], conv_param[<span class="hljs-string">&#x27;pad&#x27;</span>]<br>    <span class="hljs-keyword">assert</span> (W + <span class="hljs-number">2</span> * pad - WW) % stride == <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;宽度异常&#x27;</span><br>    <span class="hljs-keyword">assert</span> (H + <span class="hljs-number">2</span> * pad - HH) % stride == <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;高度异常&#x27;</span><br>    <span class="hljs-comment"># 零填充</span><br>    p = pad<br>    x_padded = np.pad(x, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (p, p), (p, p)), <br>                                        mode=<span class="hljs-string">&#x27;constant&#x27;</span>) <br>    <span class="hljs-comment"># 计算输出维度</span><br>    H += <span class="hljs-number">2</span> * pad<br>    W += <span class="hljs-number">2</span> * pad<br>    out_h = <span class="hljs-built_in">int</span>((H - HH) / stride + <span class="hljs-number">1</span>)<br>    out_w = <span class="hljs-built_in">int</span>((W - WW) / stride + <span class="hljs-number">1</span>)<br>    shape = (C, HH, WW, N, out_h, out_w)<br>    strides = (H * W, W, <span class="hljs-number">1</span>, C * H * W, stride * W, stride)<br>    strides = x.itemsize * np.array(strides)<br>    x_stride = np.lib.stride_tricks.as_strided(x_padded,<br>                                shape=shape, strides=strides)<br>    x_cols = np.ascontiguousarray(x_stride)<br>    x_cols.shape = (C * HH * WW, N * out_h * out_w)<br>    <span class="hljs-comment"># 将所有卷积核重塑成一行</span><br>    res = w.reshape(F, -<span class="hljs-number">1</span>).dot(x_cols) + b.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 重塑输出</span><br>    res.shape = (F, N, out_h, out_w)<br>    out = res.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>    out = np.ascontiguousarray(out)<br>    cache = (x, w, b, conv_param)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_backward_naive1</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    卷积层反向传播显式循环版本</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout:上层梯度.</span><br><span class="hljs-string">    - cache: 前向传播时的缓存元组 (x, w, b, conv_param) </span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx:    x梯度</span><br><span class="hljs-string">    - dw:    w梯度</span><br><span class="hljs-string">    - db:    b梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dw, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                        任务 ：实现卷积层反向传播                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    x, w, b, conv_param = cache<br>    P = conv_param[<span class="hljs-string">&#x27;pad&#x27;</span>]<br>    x_pad = np.pad(x,((<span class="hljs-number">0</span>,),(<span class="hljs-number">0</span>,),(P,),(P,)),<span class="hljs-string">&#x27;constant&#x27;</span>)<br>    N, C, H, W = x.shape<br>    F, C, HH, WW = w.shape<br>    N, F, Hh, Hw = dout.shape<br>    S = conv_param[<span class="hljs-string">&#x27;stride&#x27;</span>]<br>    dw = np.zeros((F, C, HH, WW))<br>    <span class="hljs-keyword">for</span> fprime <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F):<br>        <span class="hljs-keyword">for</span> cprime <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(C):<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(HH):<br>                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(WW):<br>                    sub_xpad =x_pad[:,cprime,i:i+Hh*S:S,j:j+Hw*S:S]<br>                    dw[fprime,cprime,i,j] = np.<span class="hljs-built_in">sum</span>(<br>                            dout[:,fprime,:,:]*sub_xpad)<br><br>    <br>    db = np.zeros((F))<br>    <span class="hljs-keyword">for</span> fprime <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F):<br>                db[fprime] = np.<span class="hljs-built_in">sum</span>(dout[:,fprime,:,:])<br>    dx = np.zeros((N, C, H, W))<br>    <br>    <span class="hljs-keyword">for</span> nprime <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(W):<br>                <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F):<br>                    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Hh):<br>                        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Hw):<br>                            mask1 = np.zeros_like(w[f,:,:,:])<br>                            mask2 = np.zeros_like(w[f,:,:,:])<br>                            <span class="hljs-keyword">if</span> (i+P-k*S)&lt;HH <span class="hljs-keyword">and</span> (i+P-k*S)&gt;= <span class="hljs-number">0</span>:<br>                                    mask1[:,i+P-k*S,:] = <span class="hljs-number">1.0</span><br>                            <span class="hljs-keyword">if</span> (j+P-l* S) &lt; WW <span class="hljs-keyword">and</span> (j+P-l*S)&gt;= <span class="hljs-number">0</span>:<br>                                    mask2[:,:,j+P-l*S] = <span class="hljs-number">1.0</span><br>                            w_masked=np.<span class="hljs-built_in">sum</span>(w[f,:,:,:]*mask1*mask2,axis=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br>                            dx[nprime,:,i,j] +=dout[nprime,f,k,l]*w_masked<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                            结束编码                                                                         #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_backward_naive</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    卷积层反向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout:上层梯度.</span><br><span class="hljs-string">    - cache: 前向传播时的缓存元组 (x, w, b, conv_param) </span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx:    x梯度</span><br><span class="hljs-string">    - dw:    w梯度</span><br><span class="hljs-string">    - db:    b梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dw, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                        任务 ：实现卷积层反向传播                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    x, w, b, conv_param = cache<br>    <span class="hljs-comment"># 初始化参数</span><br>    N, C, H, W = x.shape<br>    F, _, HH, WW = w.shape<br>    stride, pad = conv_param[<span class="hljs-string">&#x27;stride&#x27;</span>], conv_param[<span class="hljs-string">&#x27;pad&#x27;</span>]<br>    <span class="hljs-comment"># 计算循环次数</span><br>    H_out = <span class="hljs-built_in">int</span>(<span class="hljs-number">1</span>+(H+<span class="hljs-number">2</span>*pad-HH)/stride)<br>    W_out = <span class="hljs-built_in">int</span>(<span class="hljs-number">1</span>+(W+<span class="hljs-number">2</span>*pad-WW)/stride)<br>    <span class="hljs-comment"># 进行0填充</span><br>    x_pad = np.pad(x,((<span class="hljs-number">0</span>,), (<span class="hljs-number">0</span>,), (pad,), (pad,)), <br>                   mode=<span class="hljs-string">&#x27;constant&#x27;</span>, constant_values=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># 计算梯度</span><br>    dx = np.zeros_like(x)<br>    dx_pad = np.zeros_like(x_pad)<br>    dw = np.zeros_like(w)<br>    db = np.zeros_like(b)<br>    <span class="hljs-comment"># 进行求解</span><br>    db = np.<span class="hljs-built_in">sum</span>(dout, axis=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>    x_pad = np.pad(x,((<span class="hljs-number">0</span>,), (<span class="hljs-number">0</span>,), (pad,), (pad,)), <br>                   mode=<span class="hljs-string">&#x27;constant&#x27;</span>, constant_values=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H_out):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(W_out):<br>            x_pad_masked = x_pad[:, :, i*stride:i*stride+HH, <br>                                 j*stride:j*stride+WW]<br>            <span class="hljs-comment"># 计算dw</span><br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(F):<br>                dw[k, :, :, :] += np.<span class="hljs-built_in">sum</span>(x_pad_masked*(dout[:, k, i, j])[:,<br>                            <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>], axis=<span class="hljs-number">0</span>)<br>            <br>            <span class="hljs-comment"># 计算dx_pad</span><br>            <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):<br>                dx_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += \<br>                    np.<span class="hljs-built_in">sum</span>((w[:, :, :, :]*(dout[n, :, i, j])[:, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>]),<br>                           axis=<span class="hljs-number">0</span>)<br>                    <br>    dx = dx_pad[:, :, pad:-pad, pad:-pad]<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                              结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">max_pool_forward_naive</span>(<span class="hljs-params">x, pool_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    最大池化前向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 数据 (N, C, H, W)</span><br><span class="hljs-string">    - pool_param: 键值:</span><br><span class="hljs-string">        - &#x27;pool_height&#x27;: 池化高</span><br><span class="hljs-string">        - &#x27;pool_width&#x27;: 池化宽</span><br><span class="hljs-string">        - &#x27;stride&#x27;: 步幅</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组型:</span><br><span class="hljs-string">    - out: 输出数据</span><br><span class="hljs-string">    - cache: (x, pool_param)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                        任务: 实现最大池化操作的前向传播                                                #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 初始化参数</span><br>    N, C, H, W = x.shape<br>    HH = pool_param[<span class="hljs-string">&#x27;pool_height&#x27;</span>]<br>    WW = pool_param[<span class="hljs-string">&#x27;pool_width&#x27;</span>]<br>    stride = pool_param[<span class="hljs-string">&#x27;stride&#x27;</span>]<br>    <br>    <span class="hljs-comment"># 计算循环次数</span><br>    H_out = <span class="hljs-built_in">int</span>((H-HH)/stride+<span class="hljs-number">1</span>) <br>    W_out = <span class="hljs-built_in">int</span>((W-WW)/stride+<span class="hljs-number">1</span>)<br>    out = np.zeros((N, C, H_out, W_out))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H_out):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(W_out):<br>            <span class="hljs-comment"># 先找到对应区域</span><br>            x_masked = x[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW]<br>            <span class="hljs-comment"># 选择其中最大的值</span><br>            out[:, :, i, j] = np.<span class="hljs-built_in">max</span>(x_masked, axis=(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                            结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = (x, pool_param)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">max_pool_forward_fast</span>(<span class="hljs-params">x, pool_param</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    最大池化前向传播的快速版本</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    x : 四维图片数据(N, C, H, W)分别表示(数量，色道，高，宽)</span><br><span class="hljs-string">    pool_param : 字典型参数表，其键值为:</span><br><span class="hljs-string">        - &#x27;pool_height&#x27;: 池化高</span><br><span class="hljs-string">        - &#x27;pool_width&#x27;: 池化宽</span><br><span class="hljs-string">        - &#x27;stride&#x27;: 步幅</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    out : 输出数据</span><br><span class="hljs-string">    cache : (x, x_reshaped, out)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 初始化参数</span><br>    N, C, H, W = x.shape<br>    pool_height = pool_param[<span class="hljs-string">&#x27;pool_height&#x27;</span>]<br>    pool_width = pool_param[<span class="hljs-string">&#x27;pool_width&#x27;</span>]<br>    stride = pool_param[<span class="hljs-string">&#x27;stride&#x27;</span>]<br>    <br>    <span class="hljs-keyword">assert</span> pool_height == pool_width == stride, <span class="hljs-string">&#x27;Invalid pool params&#x27;</span><br>    <span class="hljs-keyword">assert</span> H % pool_height == <span class="hljs-number">0</span><br>    <span class="hljs-keyword">assert</span> W % pool_height == <span class="hljs-number">0</span><br>    <br>    x_reshaped = x.reshape(N, C, <span class="hljs-built_in">int</span>(H / pool_height), pool_height,<br>                                                <span class="hljs-built_in">int</span>(W / pool_width), pool_width)<br>    out = x_reshaped.<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">3</span>).<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">4</span>)<br><br>    cache = (x, x_reshaped, out)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">max_pool_backward_naive</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    最大池化反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度</span><br><span class="hljs-string">    - cache: 缓存 (x, pool_param)</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx:    x梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                        任务：实现最大池化反向传播                                                                         #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    x, pool_param = cache<br>    N, C, H, W = x.shape<br>    HH = pool_param[<span class="hljs-string">&#x27;pool_height&#x27;</span>]<br>    WW = pool_param[<span class="hljs-string">&#x27;pool_width&#x27;</span>]<br>    stride = pool_param[<span class="hljs-string">&#x27;stride&#x27;</span>]<br>    H_out = <span class="hljs-built_in">int</span>((H-HH)/stride+<span class="hljs-number">1</span>)<br>    W_out = <span class="hljs-built_in">int</span>((W-WW)/stride+<span class="hljs-number">1</span>)<br>    dx = np.zeros_like(x)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H_out):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(W_out):<br>            x_masked = x[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW]<br>            max_x_masked = np.<span class="hljs-built_in">max</span>(x_masked, axis=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>            temp_binary_mask = (x_masked == (max_x_masked)[:, :, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>])<br>            dx[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += \<br>                temp_binary_mask*(dout[:, :, i, j])[:, :, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>]<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                          结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">max_pool_backward_fast</span>(<span class="hljs-params">dout, cache</span>):<br>    x, x_reshaped, out = cache<br>    dx_reshaped = np.zeros_like(x_reshaped)<br>    out_newaxis = out[:, :, :, np.newaxis, :, np.newaxis]<br>    mask = (x_reshaped == out_newaxis)<br>    dout_newaxis = dout[:, :, :, np.newaxis, :, np.newaxis]<br>    dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)<br>    dx_reshaped[mask] = dout_broadcast[mask]<br>    dx_reshaped /= np.<span class="hljs-built_in">sum</span>(mask, axis=(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>), keepdims=<span class="hljs-literal">True</span>)<br>    dx = dx_reshaped.reshape(x.shape)<br>    <span class="hljs-keyword">return</span> dx<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">spatial_batchnorm_forward</span>(<span class="hljs-params">x, gamma, beta, bn_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    空间批量归一化前向传播</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 数据 (N, C, H, W)</span><br><span class="hljs-string">    - gamma: 缩放因子 (C,)</span><br><span class="hljs-string">    - beta: 偏移因子 (C,)</span><br><span class="hljs-string">    - bn_param: 参数字典:</span><br><span class="hljs-string">        - mode: &#x27;train&#x27; or &#x27;test&#x27;;</span><br><span class="hljs-string">        - eps: 数值稳定常数</span><br><span class="hljs-string">        - momentum: 运行平均值衰减因子</span><br><span class="hljs-string">        - running_mean: 形状为(D,) 的运行均值</span><br><span class="hljs-string">        - running_var ：形状为 (D,) 的运行方差</span><br><span class="hljs-string">        </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out:输出 (N, C, H, W)</span><br><span class="hljs-string">    - cache: 用于反向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务：实现空间BN算法前向传播                                                                #</span><br>    <span class="hljs-comment">#            提示：你只需要重塑数据，调用 batchnorm_forward函数即可                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    N, C, H, W = x.shape<br>    temp_output, cache = batchnorm_forward(<br>        x.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>).reshape(N*H*W, C), gamma, beta, bn_param)<br>    out = temp_output.reshape(N, W, H, C).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                          结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">spatial_batchnorm_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        空间批量归一化反向传播</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度 (N, C, H, W)</span><br><span class="hljs-string">    - cache: 前向传播缓存</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx:输入梯度 (N, C, H, W)</span><br><span class="hljs-string">    - dgamma: gamma梯度 (C,)</span><br><span class="hljs-string">    - dbeta: beta梯度 (C,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dgamma, dbeta = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                        任务：实现空间BN算法反向传播                                                                     #</span><br>    <span class="hljs-comment">#                        提示：你只需要重塑数据调用batchnorm_backward_alt函数即可             #                            </span><br>    <span class="hljs-comment">#############################################################################    </span><br>    N, C, H, W = dout.shape<br>    dx_temp, dgamma, dbeta = batchnorm_backward_alt(<br>        dout.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">3</span> , <span class="hljs-number">2</span>, <span class="hljs-number">1</span>).reshape((N*H*W, C)), cache)<br>    dx = dx_temp.reshape(N, W, H, C).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                       结束编码                                                                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dgamma, dbeta<br>    <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_relu_forward</span>(<span class="hljs-params">x, w, b, conv_param</span>):<br>    a, conv_cache = conv_forward_fast(x, w, b, conv_param)<br>    out, relu_cache = relu_forward(a)<br>    cache = (conv_cache, relu_cache)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    conv_cache, relu_cache = cache<br>    da = relu_backward(dout, relu_cache)<br>    dx, dw, db = conv_backward_naive(da, conv_cache)<br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_relu_pool_forward</span>(<span class="hljs-params">x, w, b, conv_param, pool_param</span>):<br>    a, conv_cache = conv_forward_fast(x, w, b, conv_param)<br>    s, relu_cache = relu_forward(a)<br>    out, pool_cache = max_pool_forward_fast(s, pool_param)<br>    cache = (conv_cache, relu_cache, pool_cache)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv_relu_pool_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    完整卷积层的反向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    dout : 上层梯度 (N, C, H, W)</span><br><span class="hljs-string">    cache : (conv_cache, relu_cache, pool_cache)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    dx : x的梯度</span><br><span class="hljs-string">    dw : w的梯度</span><br><span class="hljs-string">    db : b的梯度</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    conv_cache, relu_cache, pool_cache = cache<br>    ds = max_pool_backward_fast(dout, pool_cache)<br>    da = relu_backward(ds, relu_cache)<br>    dx, dw, db = conv_backward_naive(da, conv_cache)<br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br></code></pre></td></tr></table></figure><h3 id="layerspy"><a class="markdownIt-Anchor" href="#layerspy"></a> <a target="_blank" rel="noopener" href="http://layers.py">layers.py</a></h3><p>之前已经写好的前向传播与后向传播代码以及softmax的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算神经网络当前层的前馈传播。该方法计算在全连接情况下的得分函数</span><br><span class="hljs-string">    注：如果不理解affine仿射变换，简单的理解为在全连接情况下的得分函数即可</span><br><span class="hljs-string"></span><br><span class="hljs-string">    输入数据x的形状为(N, d_1, ..., d_k)，其中N表示数据量，(d_1, ..., d_k)表示</span><br><span class="hljs-string">    每一通道的数据维度。如果是图片数据就为(长，宽，色道)，数据的总维度就为</span><br><span class="hljs-string">    D = d_1 * ... * d_k，因此我们需要数据整合成完整的（N,D)形式再进行仿射变换。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据，其形状为(N, d_1, ..., d_k)的numpy array</span><br><span class="hljs-string">    - w: 权重矩阵，其形状为(D,M)的numpy array，D表示输入数据维度，M表示输出数据维度</span><br><span class="hljs-string">             可以将D看成输入的神经元个数，M看成输出神经元个数</span><br><span class="hljs-string">    - b: 偏置向量，其形状为(M,)的numpy array</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 形状为(N, M)的输出结果</span><br><span class="hljs-string">    - cache: 将输入进行缓存(x, w, b)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 任务: 实现全连接前向传播</span><br>    <span class="hljs-comment"># 注：首先你需要将输入数据重塑成行。  </span><br>    N=x.shape[<span class="hljs-number">0</span>]<br>    x_new=x.reshape(N,-<span class="hljs-number">1</span>)<span class="hljs-comment">#将x重塑成2维向量</span><br>    out=np.dot(x_new,w)+b<br>    cache = (x, w, b)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"> 计算仿射层的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 形状为(N, M)的上层梯度</span><br><span class="hljs-string">    - cache: 元组:</span><br><span class="hljs-string">        - x: (N, d_1, ... d_k)的输入数据</span><br><span class="hljs-string">        - w: 形状为(D, M)的权重矩阵</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度，其形状为(N, d1, ..., d_k)</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度，其形状为(D,M)</span><br><span class="hljs-string">    - db: 偏置项b的梯度，其形状为(M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b = cache<br>    dx, dw, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 注意：你需要将x重塑成(N,D)后才能计算各梯度，                                            #</span><br>    <span class="hljs-comment"># 完梯度后你需要将dx的形状与x重塑成一样     </span><br>    db = np.<span class="hljs-built_in">sum</span>(dout,axis=<span class="hljs-number">0</span>)<br>    xx= x.reshape(x.shape[<span class="hljs-number">0</span>],-<span class="hljs-number">1</span>)<br>    dw = np.dot(xx.T,dout)<br>    dx = np.dot(dout,w.T)<br>    dx=np.reshape(dx,x.shape)<br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算tified linear units (ReLUs)激活函数的前向传播，并保存相应缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 和输入数据x形状相同</span><br><span class="hljs-string">    - cache: x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 实现ReLU 的前向传播.                                                                        #</span><br>    out =np.maximum(<span class="hljs-number">0</span>,x)<br>    cache = x<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 rectified linear units (ReLUs)激活函数的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: 输入 x,其形状应该和dout相同</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: x的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, x = <span class="hljs-literal">None</span>, cache<br>    <span class="hljs-comment"># 实现 ReLU 反向传播.    </span><br>    dx=dout<br>    dx[x&lt;=<span class="hljs-number">0</span>]=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元前向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入到 affine层的数据</span><br><span class="hljs-string">    - w, b:    affine层的权重矩阵和偏置向量</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: Output from the ReLU的输出结果</span><br><span class="hljs-string">    - cache: 前向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 你需要调用affine_forward以及relu_forward函数，并将各自的缓存保存在cache中                                                                    #</span><br>    a, fc_cache = affine_forward(x, w, b)<br>    out, relu_cache = relu_forward(a)<br>    cache = (fc_cache, relu_cache)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元的反向传播</span><br><span class="hljs-string">     </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: affine缓存，以及relu缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度</span><br><span class="hljs-string">    - db: 偏置向量b的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    fc_cache, relu_cache = cache<br>    da = relu_backward(dout, relu_cache)<br>    dx, dw, db = affine_backward(da, fc_cache)<br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss</span>(<span class="hljs-params">x, y</span>):<br><br>    probs = np.exp(x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    probs /= np.<span class="hljs-built_in">sum</span>(probs, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    N = x.shape[<span class="hljs-number">0</span>]<br>    loss = -np.<span class="hljs-built_in">sum</span>(np.log(probs[np.arange(N), y])) / N<br>    dx = probs.copy()<br>    dx[np.arange(N), y] -= <span class="hljs-number">1</span><br>    dx /= N<br><br>    <span class="hljs-keyword">return</span> loss, dx<br><br></code></pre></td></tr></table></figure><h3 id="trainerpy"><a class="markdownIt-Anchor" href="#trainerpy"></a> <a target="_blank" rel="noopener" href="http://trainer.py">trainer.py</a></h3><p>解耦训练器的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">import</span> updater<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用形式:</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    data = &#123;</span><br><span class="hljs-string">        &#x27;X_train&#x27;: # 训练数据</span><br><span class="hljs-string">        &#x27;y_train&#x27;: # 训练类标</span><br><span class="hljs-string">        &#x27;X_val&#x27;: # 验证数据</span><br><span class="hljs-string">        &#x27;X_train&#x27;: # 验证类标</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    model = MyAwesomeModel(hidden_size=100, reg=10)</span><br><span class="hljs-string">    Trainer = Trainer(model, data,</span><br><span class="hljs-string">                                    update_rule=&#x27;sgd&#x27;,</span><br><span class="hljs-string">                                    updater_config=&#123;</span><br><span class="hljs-string">                                        &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="hljs-string">                                    &#125;,</span><br><span class="hljs-string">                                    lr_decay=0.95,</span><br><span class="hljs-string">                                    num_epochs=10, batch_size=100,</span><br><span class="hljs-string">                                    print_every=100)</span><br><span class="hljs-string">    Trainer.train()</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, data, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        构造一个新的Trainer实例</span><br><span class="hljs-string">        必须参数:</span><br><span class="hljs-string">        - model: 网络模型</span><br><span class="hljs-string">        - data: 数据字典，其中:</span><br><span class="hljs-string">            &#x27;X_train&#x27;:    形状为(N_train, d_1, ..., d_k)训练数据</span><br><span class="hljs-string">            &#x27;X_val&#x27;:    形状为(N_val, d_1, ..., d_k) 验证数据</span><br><span class="hljs-string">            &#x27;y_train&#x27;:    形状为(N_train,) 训练数据类标</span><br><span class="hljs-string">            &#x27;y_val&#x27;:    形状为(N_val,) 验证数据类标</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        可选参数:</span><br><span class="hljs-string">        - update_rule: 更新规则，其存放在updater.py文件中，默认选项为&#x27;sgd&#x27;。</span><br><span class="hljs-string">        - updater_config: 字典类型的，更新规则所对应的超参数配置，同见updater.py文件。</span><br><span class="hljs-string">        - lr_decay: 学习率衰减系数。</span><br><span class="hljs-string">        - batch_size: 批量数据大小</span><br><span class="hljs-string">        - num_epochs: 训练周期</span><br><span class="hljs-string">        - print_every: 整数型; 每迭代多少次进行打印一次中间结果</span><br><span class="hljs-string">        - verbose: 布尔型; 是否在训练期间打印中间结果</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-variable language_">self</span>.model = model<br>        <span class="hljs-variable language_">self</span>.X_train = data[<span class="hljs-string">&#x27;X_train&#x27;</span>]<br>        <span class="hljs-variable language_">self</span>.y_train = data[<span class="hljs-string">&#x27;y_train&#x27;</span>]<br>        <span class="hljs-variable language_">self</span>.X_val = data[<span class="hljs-string">&#x27;X_val&#x27;</span>]<br>        <span class="hljs-variable language_">self</span>.y_val = data[<span class="hljs-string">&#x27;y_val&#x27;</span>]<br>        <br>        <span class="hljs-comment"># 弹出可选参数，进行相关配置</span><br>        <span class="hljs-variable language_">self</span>.update_rule = kwargs.pop(<span class="hljs-string">&#x27;update_rule&#x27;</span>, <span class="hljs-string">&#x27;sgd&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.updater_config = kwargs.pop(<span class="hljs-string">&#x27;updater_config&#x27;</span>, &#123;&#125;)<br>        <span class="hljs-variable language_">self</span>.lr_decay = kwargs.pop(<span class="hljs-string">&#x27;lr_decay&#x27;</span>, <span class="hljs-number">1.0</span>)<br>        <span class="hljs-variable language_">self</span>.batch_size = kwargs.pop(<span class="hljs-string">&#x27;batch_size&#x27;</span>, <span class="hljs-number">100</span>)<br>        <span class="hljs-variable language_">self</span>.num_epochs = kwargs.pop(<span class="hljs-string">&#x27;num_epochs&#x27;</span>, <span class="hljs-number">10</span>)<br><br>        <span class="hljs-variable language_">self</span>.print_every = kwargs.pop(<span class="hljs-string">&#x27;print_every&#x27;</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-variable language_">self</span>.verbose = kwargs.pop(<span class="hljs-string">&#x27;verbose&#x27;</span>, <span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 若可选参数错误，抛出异常</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(kwargs) &gt; <span class="hljs-number">0</span>:<br>            extra = <span class="hljs-string">&#x27;, &#x27;</span>.join(<span class="hljs-string">&#x27;&quot;%s&quot;&#x27;</span> % k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> kwargs.keys())<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Unrecognized arguments %s&#x27;</span> % extra)<br><br><br>        <span class="hljs-comment">#确认updater中含有更新规则</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(updater, <span class="hljs-variable language_">self</span>.update_rule):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % <span class="hljs-variable language_">self</span>.update_rule)<br>        <span class="hljs-variable language_">self</span>.update_rule = <span class="hljs-built_in">getattr</span>(updater, <span class="hljs-variable language_">self</span>.update_rule)<br><br>        <span class="hljs-comment"># 初始化相关变量</span><br>        <span class="hljs-variable language_">self</span>.epoch = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.best_val_acc = <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.best_params = &#123;&#125;<br>        <span class="hljs-variable language_">self</span>.loss_history = []<br>        <span class="hljs-variable language_">self</span>.train_acc_history = []<br>        <span class="hljs-variable language_">self</span>.val_acc_history = []<br><br>        <span class="hljs-comment"># 对updater_config中的参数进行深拷贝</span><br>        <span class="hljs-variable language_">self</span>.updater_configs = &#123;&#125;<br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.params:<br>            d = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.updater_config.items()&#125;<br>            <span class="hljs-variable language_">self</span>.updater_configs[p] = d<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        执行单步梯度更新</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 采样批量数据</span><br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        batch_mask = np.random.choice(num_train, <span class="hljs-variable language_">self</span>.batch_size)<br>        X_batch = <span class="hljs-variable language_">self</span>.X_train[batch_mask]<br>        y_batch = <span class="hljs-variable language_">self</span>.y_train[batch_mask]<br><br>        <span class="hljs-comment"># 计算损失及梯度</span><br>        loss, grads = <span class="hljs-variable language_">self</span>.model.loss(X_batch, y_batch)<br>        <span class="hljs-variable language_">self</span>.loss_history.append(loss)<br><br>        <span class="hljs-comment"># 更新参数</span><br>        <span class="hljs-keyword">for</span> p, w <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.params.items():<br>            dw = grads[p]<br>            config = <span class="hljs-variable language_">self</span>.updater_configs[p]<br>            next_w, next_config = <span class="hljs-variable language_">self</span>.update_rule(w, dw, config)<br>            <span class="hljs-variable language_">self</span>.model.params[p] = next_w<br>            <span class="hljs-variable language_">self</span>.updater_configs[p] = next_config<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_accuracy</span>(<span class="hljs-params">self, X, y, num_samples=<span class="hljs-literal">None</span>, batch_size=<span class="hljs-number">100</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     根据提供的数据检验精度，若数据集过大，可进行采样测试。</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 形状为(N, d_1, ..., d_k)的数据</span><br><span class="hljs-string">        - y: 形状为 (N,)的数据类标</span><br><span class="hljs-string">        - num_samples: 采样次数</span><br><span class="hljs-string">        - batch_size:批量数据大小</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - acc: 测试数据正确率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        <span class="hljs-comment"># 对数据进行采样</span><br>        N = X.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> num_samples <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> N &gt; num_samples:<br>            mask = np.random.choice(N, num_samples)<br>            N = num_samples<br>            X = X[mask]<br>            y = y[mask]<br><br>        <span class="hljs-comment"># 计算精度</span><br>        num_batches = <span class="hljs-built_in">int</span>(N / batch_size)<br>        <span class="hljs-keyword">if</span> N % batch_size != <span class="hljs-number">0</span>:<br>          num_batches += <span class="hljs-number">1</span><br>        y_pred = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):<br>            start = i * batch_size<br>            end = (i + <span class="hljs-number">1</span>) * batch_size<br>            scores = <span class="hljs-variable language_">self</span>.model.loss(X[start:end])<br>            y_pred.append(np.argmax(scores, axis=<span class="hljs-number">1</span>))<br>        y_pred = np.hstack(y_pred)<br>        acc = np.mean(y_pred == y)<br><br>        <span class="hljs-keyword">return</span> acc<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        根据配置训练模型</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train = <span class="hljs-variable language_">self</span>.X_train.shape[<span class="hljs-number">0</span>]<br>        iterations_per_epoch = <span class="hljs-built_in">max</span>(num_train / <span class="hljs-variable language_">self</span>.batch_size, <span class="hljs-number">1</span>)<br>        num_iterations = <span class="hljs-built_in">int</span>(<span class="hljs-variable language_">self</span>.num_epochs * iterations_per_epoch)<br>    <br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>            <span class="hljs-variable language_">self</span>._step()<br>    <br>            <span class="hljs-comment"># 打印损失值</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.verbose <span class="hljs-keyword">and</span> t % <span class="hljs-variable language_">self</span>.print_every == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(迭代 %d / %d) 损失值: %f&#x27;</span> % (<br>                             t + <span class="hljs-number">1</span>, num_iterations, <span class="hljs-variable language_">self</span>.loss_history[-<span class="hljs-number">1</span>]))<br><br>            <span class="hljs-comment"># 更新学习率</span><br>            epoch_end = (t + <span class="hljs-number">1</span>) % iterations_per_epoch == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> epoch_end:<br>                <span class="hljs-variable language_">self</span>.epoch += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.updater_configs:<br>                    <span class="hljs-variable language_">self</span>.updater_configs[k][<span class="hljs-string">&#x27;learning_rate&#x27;</span>] *= <span class="hljs-variable language_">self</span>.lr_decay<br><br><br>            <span class="hljs-comment">#在训练的开始，末尾，每一轮训练周期检验精度</span><br>            first_it = (t == <span class="hljs-number">0</span>)<br>            last_it = (t == num_iterations + <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> first_it <span class="hljs-keyword">or</span> last_it <span class="hljs-keyword">or</span> epoch_end:<br>                train_acc = <span class="hljs-variable language_">self</span>.check_accuracy(<span class="hljs-variable language_">self</span>.X_train, <span class="hljs-variable language_">self</span>.y_train,<br>                                                                                num_samples=<span class="hljs-number">1000</span>)<br>                val_acc = <span class="hljs-variable language_">self</span>.check_accuracy(<span class="hljs-variable language_">self</span>.X_val, <span class="hljs-variable language_">self</span>.y_val)<br>                <span class="hljs-variable language_">self</span>.train_acc_history.append(train_acc)<br>                <span class="hljs-variable language_">self</span>.val_acc_history.append(val_acc)<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.verbose:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(周期 %d / %d) 训练精度: %f; 验证精度: %f&#x27;</span> % (<br>                                 <span class="hljs-variable language_">self</span>.epoch, <span class="hljs-variable language_">self</span>.num_epochs, train_acc, val_acc))<br><br>                <span class="hljs-comment"># 记录最佳模型</span><br>                <span class="hljs-keyword">if</span> val_acc &gt; <span class="hljs-variable language_">self</span>.best_val_acc:<br>                    <span class="hljs-variable language_">self</span>.best_val_acc = val_acc<br>                    <span class="hljs-variable language_">self</span>.best_params = &#123;&#125;<br>                    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.params.items():<br>                        <span class="hljs-variable language_">self</span>.best_params[k] = v.copy()<br>    <br>            <span class="hljs-comment"># 训练结束后返回最佳模型</span><br>            <span class="hljs-variable language_">self</span>.model.params = <span class="hljs-variable language_">self</span>.best_params<br><br><br></code></pre></td></tr></table></figure><h3 id="updaterpy"><a class="markdownIt-Anchor" href="#updaterpy"></a> <a target="_blank" rel="noopener" href="http://updater.py">updater.py</a></h3><p>解耦更新器，主要负责更新神经网络的权重，其传入参数有神经网络的权重w ww、当前权重的梯度d w dwdw及相应的更新配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">频繁使用在训练神经网络中的一阶梯度更新规则。每次更新接受当前的权重，</span><br><span class="hljs-string">对应的梯度，以及相关配置进行权重更新。</span><br><span class="hljs-string">def update(w, dw, config=None):</span><br><span class="hljs-string">Inputs:</span><br><span class="hljs-string">    - w:当前权重.</span><br><span class="hljs-string">    - dw: 和权重形状相同的梯度.</span><br><span class="hljs-string">    - config: 字典型超参数配置，比如学习率，动量值等。如果更新规则需要用到缓存，</span><br><span class="hljs-string">        在配置中需要保存相应的缓存。</span><br><span class="hljs-string"></span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    - next_w: 更新后的权重.</span><br><span class="hljs-string">    - config: 更新规则相应的配置.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    随机梯度下降更新规则.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    config 格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br><br>    w -= config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] * dw<br>    <span class="hljs-keyword">return</span> w, config<br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd_momentum</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    动量随机梯度下降更新规则</span><br><span class="hljs-string"></span><br><span class="hljs-string">    config 格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    - momentum: [0,1]的动量，0表示不使用动量，退化为SGD</span><br><span class="hljs-string">    - velocity: 和w，dw同形的速度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;momentum&#x27;</span>, <span class="hljs-number">0.9</span>)<br>    v = config.setdefault(<span class="hljs-string">&#x27;velocity&#x27;</span>, np.zeros_like(w))<br>    <br>    next_w = <span class="hljs-literal">None</span><br>    v =config[<span class="hljs-string">&#x27;momentum&#x27;</span>]*config[<span class="hljs-string">&#x27;velocity&#x27;</span>] - config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] * dw<br>    next_w = w + v<br>    config[<span class="hljs-string">&#x27;velocity&#x27;</span>] = v<br><br>    <span class="hljs-keyword">return</span> next_w, config<br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rmsprop</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    RMSProp 更新规则</span><br><span class="hljs-string"></span><br><span class="hljs-string">    config 格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    - decay_rate:用于衰减历史梯度值的衰减率,取值为[0,1]</span><br><span class="hljs-string">    - epsilon: 避免除零异常的小数.</span><br><span class="hljs-string">    - cache:历史梯度缓存.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;decay_rate&#x27;</span>, <span class="hljs-number">0.99</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;epsilon&#x27;</span>, <span class="hljs-number">1e-8</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;cache&#x27;</span>, np.zeros_like(w))<br><br>    next_w = <span class="hljs-literal">None</span><br>    config[<span class="hljs-string">&#x27;cache&#x27;</span>] = config[<span class="hljs-string">&#x27;decay_rate&#x27;</span>] * config[<span class="hljs-string">&#x27;cache&#x27;</span>] + (<span class="hljs-number">1</span> - config[<span class="hljs-string">&#x27;decay_rate&#x27;</span>]) * dw**<span class="hljs-number">2</span><br>    next_w = w - config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] * dw /(np.sqrt(config[<span class="hljs-string">&#x27;cache&#x27;</span>] + config[<span class="hljs-string">&#x27;epsilon&#x27;</span>]))<br><br>    <span class="hljs-keyword">return</span> next_w, config<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">adam</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用 Adam更新规则 ,融合了“热身”更新</span><br><span class="hljs-string"></span><br><span class="hljs-string">    config 格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    - beta1: 动量衰减率.</span><br><span class="hljs-string">    - beta2: 学习步长衰减率.</span><br><span class="hljs-string">    - epsilon: 防除0小数.</span><br><span class="hljs-string">    - m: 梯度.</span><br><span class="hljs-string">    - v: 梯度平方.</span><br><span class="hljs-string">    - t: 迭代次数.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-3</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;beta1&#x27;</span>, <span class="hljs-number">0.9</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;beta2&#x27;</span>, <span class="hljs-number">0.999</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;epsilon&#x27;</span>, <span class="hljs-number">1e-8</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;m&#x27;</span>, np.zeros_like(w))<br>    config.setdefault(<span class="hljs-string">&#x27;v&#x27;</span>, np.zeros_like(w))<br>    config.setdefault(<span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-number">0</span>)<br>    <br>    next_w = <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 将更新后的权重存放在next_w中，记得将m,v,t存放在相应的config中 </span><br>    config[<span class="hljs-string">&#x27;t&#x27;</span>] += <span class="hljs-number">1</span><br>    beta1 = config[<span class="hljs-string">&#x27;beta1&#x27;</span>]<br>    beta2 = config[<span class="hljs-string">&#x27;beta2&#x27;</span>]<br>    epsilon = config[<span class="hljs-string">&#x27;epsilon&#x27;</span>]<br>    learning_rate = config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>]<br>    config[<span class="hljs-string">&#x27;m&#x27;</span>] = beta1 * config[<span class="hljs-string">&#x27;m&#x27;</span>] + (<span class="hljs-number">1</span>-beta1) * dw<br>    config[<span class="hljs-string">&#x27;v&#x27;</span>] = beta2 * config[<span class="hljs-string">&#x27;v&#x27;</span>] + (<span class="hljs-number">1</span>-beta2) * dw**<span class="hljs-number">2</span><br>    mb = config[<span class="hljs-string">&#x27;m&#x27;</span>]/(<span class="hljs-number">1</span> - beta1**config[<span class="hljs-string">&#x27;t&#x27;</span>])<br>    vb = config[<span class="hljs-string">&#x27;v&#x27;</span>]/(<span class="hljs-number">1</span> - beta2**config[<span class="hljs-string">&#x27;t&#x27;</span>])<br>    next_w = w - learning_rate * mb / (np.sqrt(vb)+epsilon)<br>    <span class="hljs-keyword">return</span> next_w, config<br><br><br></code></pre></td></tr></table></figure><h3 id="bn_layerspy"><a class="markdownIt-Anchor" href="#bn_layerspy"></a> bn_layers.py</h3><p>实现BN算法的前向传播、反向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> dropout_layers <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchnorm_forward</span>(<span class="hljs-params">x, gamma, beta, bn_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    使用使用类似动量衰减的运行时平均，计算总体均值与方差 例如:</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span><br><span class="hljs-string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 数据(N, D)</span><br><span class="hljs-string">    - gamma: 缩放参数 (D,)</span><br><span class="hljs-string">    - beta: 平移参数 (D,)</span><br><span class="hljs-string">    - bn_param: 字典型，使用下列键值:</span><br><span class="hljs-string">        - mode: &#x27;train&#x27; 或&#x27;test&#x27;; </span><br><span class="hljs-string">        - eps: 保证数值稳定</span><br><span class="hljs-string">        - momentum: 运行时平均衰减因子 </span><br><span class="hljs-string">        - running_mean: 形状为(D,)的运行时均值</span><br><span class="hljs-string">        - running_var : 形状为 (D,)的运行时方差</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 输出(N, D)</span><br><span class="hljs-string">    - cache: 用于反向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    mode = bn_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>    eps = bn_param.get(<span class="hljs-string">&#x27;eps&#x27;</span>, <span class="hljs-number">1e-5</span>)<br>    momentum = bn_param.get(<span class="hljs-string">&#x27;momentum&#x27;</span>, <span class="hljs-number">0.9</span>)<br><br>    N, D = x.shape<br>    running_mean = bn_param.get(<span class="hljs-string">&#x27;running_mean&#x27;</span>, np.zeros(D, dtype=x.dtype))<br>    running_var = bn_param.get(<span class="hljs-string">&#x27;running_var&#x27;</span>, np.zeros(D, dtype=x.dtype))<br><br>    out, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>        <span class="hljs-comment"># Forward pass</span><br>        <span class="hljs-comment"># Step 1 - shape of mu (D,)</span><br>        mu = <span class="hljs-number">1</span> / <span class="hljs-built_in">float</span>(N) * np.<span class="hljs-built_in">sum</span>(x, axis=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># Step 2 - shape of var (N,D)</span><br>        xmu = x - mu<br>        <span class="hljs-comment"># Step 3 - shape of carre (N,D)</span><br>        carre = xmu**<span class="hljs-number">2</span><br>        <span class="hljs-comment"># Step 4 - shape of var (D,)</span><br>        var = <span class="hljs-number">1</span> / <span class="hljs-built_in">float</span>(N) * np.<span class="hljs-built_in">sum</span>(carre, axis=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># Step 5 - Shape sqrtvar (D,)</span><br>        sqrtvar = np.sqrt(var + eps)<br>        <span class="hljs-comment"># Step 6 - Shape invvar (D,)</span><br>        invvar = <span class="hljs-number">1.</span> / sqrtvar<br>        <span class="hljs-comment"># Step 7 - Shape va2 (N,D)</span><br>        va2 = xmu * invvar<br>        <span class="hljs-comment"># Step 8 - Shape va3 (N,D)</span><br>        va3 = gamma * va2<br>        <span class="hljs-comment"># Step 9 - Shape out (N,D)</span><br>        out = va3 + beta<br>        running_mean = momentum * running_mean + (<span class="hljs-number">1.0</span> - momentum) * mu<br>        running_var = momentum * running_var + (<span class="hljs-number">1.0</span> - momentum) * var<br>        cache = (mu, xmu, carre, var, sqrtvar, invvar,va2, va3, gamma, beta, x, bn_param)<br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        <span class="hljs-comment"># 使用运行时均值与方差归一化数据</span><br>        mu = running_mean<br>        var = running_var<br>        xhat = (x - mu) / np.sqrt(var + eps)<br>        <span class="hljs-comment"># 使用gamma和beta参数缩放，平移数据。</span><br>        out = gamma * xhat + beta<br>        cache = (mu, var, gamma, beta, bn_param)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;无法识别的BN模式： &quot;%s&quot;&#x27;</span> % mode)<br><br>    <span class="hljs-comment"># 更新运行时均值，方差</span><br>    bn_param[<span class="hljs-string">&#x27;running_mean&#x27;</span>] = running_mean<br>    bn_param[<span class="hljs-string">&#x27;running_var&#x27;</span>] = running_var<br><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchnorm_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    BN反向传播 </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度 (N, D)</span><br><span class="hljs-string">    - cache: 前向传播时的缓存.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 数据梯度 (N, D)</span><br><span class="hljs-string">    - dgamma: gamma梯度 (D,)</span><br><span class="hljs-string">    - dbeta: beta梯度 (D,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dgamma, dbeta = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <br>    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param = cache<br>    eps = bn_param.get(<span class="hljs-string">&#x27;eps&#x27;</span>, <span class="hljs-number">1e-5</span>)<br>    N, D = dout.shape<br>    <span class="hljs-comment"># Backprop Step 9</span><br>    dva3 = dout<br>    dbeta = np.<span class="hljs-built_in">sum</span>(dout, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Backprop step 8</span><br>    dva2 = gamma * dva3<br>    dgamma = np.<span class="hljs-built_in">sum</span>(va2 * dva3, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Backprop step 7</span><br>    dxmu = invvar * dva2<br>    dinvvar = np.<span class="hljs-built_in">sum</span>(xmu * dva2, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Backprop step 6</span><br>    dsqrtvar = -<span class="hljs-number">1.</span> / (sqrtvar**<span class="hljs-number">2</span>) * dinvvar<br>    <span class="hljs-comment"># Backprop step 5</span><br>    dvar = <span class="hljs-number">0.5</span> * (var + eps)**(-<span class="hljs-number">0.5</span>) * dsqrtvar<br>    <span class="hljs-comment"># Backprop step 4</span><br>    dcarre = <span class="hljs-number">1</span> / <span class="hljs-built_in">float</span>(N) * np.ones((carre.shape)) * dvar<br>    <span class="hljs-comment"># Backprop step 3</span><br>    dxmu += <span class="hljs-number">2</span> * xmu * dcarre<br>    <span class="hljs-comment"># Backprop step 2</span><br>    dx = dxmu<br>    dmu = - np.<span class="hljs-built_in">sum</span>(dxmu, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># Basckprop step 1</span><br>    dx += <span class="hljs-number">1</span> / <span class="hljs-built_in">float</span>(N) * np.ones((dxmu.shape)) * dmu<br>    <br>    <span class="hljs-keyword">return</span> dx, dgamma, dbeta<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchnorm_backward_alt</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    可选的BN反向传播</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dgamma, dbeta = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param = cache<br>    eps = bn_param.get(<span class="hljs-string">&#x27;eps&#x27;</span>, <span class="hljs-number">1e-5</span>)<br>    N, D = dout.shape<br>    dbeta = np.<span class="hljs-built_in">sum</span>(dout, axis=<span class="hljs-number">0</span>)<br>    dgamma = np.<span class="hljs-built_in">sum</span>((x - mu) * (var + eps)**(-<span class="hljs-number">1.</span> / <span class="hljs-number">2.</span>) * dout, axis=<span class="hljs-number">0</span>)<br>    dx = (<span class="hljs-number">1.</span>/N) * gamma * (var + eps)**(-<span class="hljs-number">1.</span>/<span class="hljs-number">2.</span>)*(N*dout-np.<span class="hljs-built_in">sum</span>(<br>                        dout, axis=<span class="hljs-number">0</span>)-(x-mu)*(var+eps)**(-<span class="hljs-number">1.0</span>)*np.<span class="hljs-built_in">sum</span>(dout*(x-mu),axis=<span class="hljs-number">0</span>))<br> <br>    <span class="hljs-keyword">return</span> dx, dgamma, dbeta<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_bn_relu_forward</span>(<span class="hljs-params">x,w,b,gamma, beta,bn_param</span>):<br>    x_affine,cache_affine= affine_forward(x,w,b)<br>    x_bn,cache_bn = batchnorm_forward(x_affine,gamma, beta,bn_param)<br>    out,cache_relu = relu_forward(x_bn)<br>    cache = (cache_affine,cache_bn,cache_relu)<br>    <span class="hljs-keyword">return</span> out,cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_bn_relu_backward</span>(<span class="hljs-params">dout,cache</span>):<br>    cache_affine,cache_bn,cache_relu = cache<br>    drelu = relu_backward(dout,cache_relu)<br>    dbn,dgamma, dbeta= batchnorm_backward_alt(drelu,cache_bn)<br>    dx,dw,db = affine_backward(dbn,cache_affine)<br>    <span class="hljs-keyword">return</span> dx,dw,db,dgamma,dbeta<br><br><br></code></pre></td></tr></table></figure><h3 id="dropout_layerspy"><a class="markdownIt-Anchor" href="#dropout_layerspy"></a> dropout_layers.py</h3><p>包含了Dropout前向传播以及反向传播，组合Dropout传播层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_forward</span>(<span class="hljs-params">x, dropout_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    执行dropout前向传播</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string">    - dropout_param: 字典类型，使用下列键值:</span><br><span class="hljs-string">        - p: dropout参数。每个神经元的激活概率p</span><br><span class="hljs-string">        - mode: &#x27;test&#x27;或&#x27;train&#x27;. 训练模式使用dropout;测试模式仅仅返回输入值。</span><br><span class="hljs-string">        - seed: 随机数生成种子. </span><br><span class="hljs-string"></span><br><span class="hljs-string">    Outputs:</span><br><span class="hljs-string">    - out: 和输入数据相同形状</span><br><span class="hljs-string">    - cache:元组(dropout_param, mask). </span><br><span class="hljs-string">                  训练模式时，掩码mask用于激活该层神经元，测试模式时不使用</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    p, mode = dropout_param[<span class="hljs-string">&#x27;p&#x27;</span>], dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;seed&#x27;</span> <span class="hljs-keyword">in</span> dropout_param:<br>        np.random.seed(dropout_param[<span class="hljs-string">&#x27;seed&#x27;</span>])<br><br>    mask = <span class="hljs-literal">None</span><br>    out = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>        mask = (np.random.rand(*x.shape) &lt; p)/p<br>        out =x*mask<br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        out = x<br><br>    cache = (dropout_param, mask)<br>    out = out.astype(x.dtype, copy=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    dropout反向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度</span><br><span class="hljs-string">    - cache: dropout_forward中的缓存(dropout_param, mask)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dropout_param, mask = cache<br>    mode = dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>    <br>    dx = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>        dx =dout*mask<br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        dx = dout<br>    <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_dropout_forward</span>(<span class="hljs-params">x,w,b,dropout_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    组合affine_relu_dropout前向传播</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string">    - w: 权重参数</span><br><span class="hljs-string">    - b: 偏置项</span><br><span class="hljs-string">    - dropout_param: 字典类型，使用下列键值:</span><br><span class="hljs-string">        - p: dropout参数。每个神经元的激活概率p</span><br><span class="hljs-string">        - mode: &#x27;test&#x27;或&#x27;train&#x27;. 训练模式使用dropout;测试模式仅仅返回输入值。</span><br><span class="hljs-string">        - seed: 随机数生成种子. </span><br><span class="hljs-string"></span><br><span class="hljs-string">    Outputs:</span><br><span class="hljs-string">    - out: 和输入数据相同形状</span><br><span class="hljs-string">    - cache:缓存包含(cache_affine,cache_relu,cache_dropout)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span> <br>    out_dropout = <span class="hljs-literal">None</span><br>    cache =<span class="hljs-literal">None</span><br>    out_affine, cache_affine = affine_forward(x,w,b)<br>    out_relu,cache_relu =relu_forward(out_affine)<br>    out_dropout,cache_dropout =dropout_forward(out_relu,dropout_param)<br>    cache = (cache_affine,cache_relu,cache_dropout)<br>    <span class="hljs-keyword">return</span> out_dropout,cache<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_dropout_backward</span>(<span class="hljs-params">dout,cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     affine_relu_dropout神经元的反向传播</span><br><span class="hljs-string">     </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: 缓存(cache_affine,cache_relu,cache_dropout)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度</span><br><span class="hljs-string">    - db: 偏置向量b的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span>    <br>    cache_affine,cache_relu,cache_dropout = cache<br>    dx,dw,db=<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span><br>    ddropout = dropout_backward(dout,cache_dropout)<br>    drelu = relu_backward(ddropout,cache_relu)<br>    dx,dw,db = affine_backward(drelu,cache_affine)<br>    <span class="hljs-keyword">return</span> dx,dw,db<br><br></code></pre></td></tr></table></figure><h3 id="cnnpy"><a class="markdownIt-Anchor" href="#cnnpy"></a> <a target="_blank" rel="noopener" href="http://cnn.py">cnn.py</a></h3><p>接下来我们实现简单的浅层卷积网络，该网络由一层卷积层，两层全连接层组成：输入 - conv - relu - 2x2 max pool - affine - relu - affine - softmax。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> cnn_layers <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ThreeLayerConvNet</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; </span><br><span class="hljs-string">    conv - relu - 2x2 max pool - affine - relu - affine - softmax</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim=(<span class="hljs-params"><span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span></span>), num_filters=<span class="hljs-number">32</span>, filter_size=<span class="hljs-number">7</span>,</span><br><span class="hljs-params">                             hidden_dim=<span class="hljs-number">100</span>, num_classes=<span class="hljs-number">10</span>, weight_scale=<span class="hljs-number">1e-3</span>, reg=<span class="hljs-number">0.0</span>,</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化网络.</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - input_dim: 输入数据形状 (C, H, W)</span><br><span class="hljs-string">        - num_filters: 卷积核个数</span><br><span class="hljs-string">        - filter_size: 卷积核尺寸</span><br><span class="hljs-string">        - hidden_dim: 全连接层隐藏层个数</span><br><span class="hljs-string">        - num_classes: 分类个数</span><br><span class="hljs-string">        - weight_scale: 权重规模（标准差）</span><br><span class="hljs-string">        - reg:权重衰减因子</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-variable language_">self</span>.params = &#123;&#125;<br>        <span class="hljs-variable language_">self</span>.reg = reg<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                            任务：初始化权重参数                             #</span><br>        <span class="hljs-comment"># &#x27;W1&#x27;为卷积层参数，形状为(num_filters,C,filter_size,filter_size)             #</span><br>        <span class="hljs-comment"># &#x27;W2&#x27;为卷积层到全连接层参数，形状为((H/2)*(W/2)*num_filters, hidden_dim)       #</span><br>        <span class="hljs-comment">#         &#x27;W3&#x27;隐藏层到全连接层参数                                            #</span><br>        <span class="hljs-comment">############################################################################</span><br>        C, H, W = input_dim<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>] = weight_scale*np.random.randn(num_filters, C, <br>                    filter_size, filter_size)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.zeros(num_filters)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>] = weight_scale*np.random.randn(<span class="hljs-built_in">int</span>((H/<span class="hljs-number">2</span>)*(W/<span class="hljs-number">2</span>)*num_filters), <br>                    hidden_dim)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.zeros(hidden_dim)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W3&#x27;</span>] = weight_scale*np.random.randn(hidden_dim, num_classes)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b3&#x27;</span>] = np.zeros(num_classes)<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                             结束编码                                      #</span><br>        <span class="hljs-comment">############################################################################</span><br>         <br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, X, y=<span class="hljs-literal">None</span></span>):<br>        <br>        <span class="hljs-comment"># 初始化参数</span><br>        W1, b1 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>]<br>        W2, b2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>]<br>        W3, b3 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W3&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b3&#x27;</span>]<br>        <span class="hljs-comment"># 使用卷积层</span><br>        filter_size = W1.shape[<span class="hljs-number">2</span>]<br>        <span class="hljs-comment"># 设置卷积层和池化层所需要的参数</span><br>        conv_param = &#123;<span class="hljs-string">&#x27;stride&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;pad&#x27;</span>: <span class="hljs-built_in">int</span>((filter_size - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>)&#125;<br>        pool_param = &#123;<span class="hljs-string">&#x27;pool_height&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;pool_width&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;stride&#x27;</span>: <span class="hljs-number">2</span>&#125;<br><br>        scores = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                          任务： 实现前向传播                                #</span><br>        <span class="hljs-comment">#                    计算每类得分，将其存放在scores中                          #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment"># 组合卷积层：卷积，ReLU，池化</span><br>        conv_forward_out_1, cache_forward_1 = conv_relu_pool_forward(X, <br>                <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>], conv_param, pool_param)<br>        <span class="hljs-comment"># affine层</span><br>        affine_forward_out_2, cache_forward_2 = affine_forward(conv_forward_out_1,<br>                <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>])<br>        <span class="hljs-comment"># relu层</span><br>        affine_relu_2, cache_relu_2 = relu_forward(affine_forward_out_2)<br>        <span class="hljs-comment"># affine层</span><br>        scores, cache_forward_3 = affine_forward(affine_relu_2, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W3&#x27;</span>],<br>                <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b3&#x27;</span>])<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                           结束编码                                        #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> scores<br>            <br>        loss, grads = <span class="hljs-number">0</span>, &#123;&#125;<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                        任务：实现反向转播                                   #</span><br>        <span class="hljs-comment">#                      注意：别忘了权重衰减项                                 #</span><br>        <span class="hljs-comment">############################################################################</span><br>        loss, dout = softmax_loss(scores, y)<br>        loss += <span class="hljs-variable language_">self</span>.reg*<span class="hljs-number">0.5</span>*(np.<span class="hljs-built_in">sum</span>(<span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>]**<span class="hljs-number">2</span>)<br>                              +np.<span class="hljs-built_in">sum</span>(<span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>]**<span class="hljs-number">2</span>)<br>                              +np.<span class="hljs-built_in">sum</span>(<span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W3&#x27;</span>]**<span class="hljs-number">2</span>))<br>        dX3, grads[<span class="hljs-string">&#x27;W3&#x27;</span>], grads[<span class="hljs-string">&#x27;b3&#x27;</span>] = affine_backward(dout, cache_forward_3)<br>        dX2 = relu_backward(dX3, cache_relu_2)<br>        dX2, grads[<span class="hljs-string">&#x27;W2&#x27;</span>], grads[<span class="hljs-string">&#x27;b2&#x27;</span>] = affine_backward(dX2, cache_forward_2)<br>        dX1, grads[<span class="hljs-string">&#x27;W1&#x27;</span>], grads[<span class="hljs-string">&#x27;b1&#x27;</span>] = conv_relu_pool_backward(dX2, cache_forward_1)<br>        grads[<span class="hljs-string">&#x27;W3&#x27;</span>] = grads[<span class="hljs-string">&#x27;W3&#x27;</span>]+<span class="hljs-variable language_">self</span>.reg*<span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>        grads[<span class="hljs-string">&#x27;W2&#x27;</span>] = grads[<span class="hljs-string">&#x27;W2&#x27;</span>]+<span class="hljs-variable language_">self</span>.reg*<span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        grads[<span class="hljs-string">&#x27;W1&#x27;</span>] = grads[<span class="hljs-string">&#x27;W1&#x27;</span>]+<span class="hljs-variable language_">self</span>.reg*<span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>]<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                          结束编码                                        #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">return</span> loss, grads<br>    <br><br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Python/" class="print-no-link">#Python</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>编码实现卷积神经网络</div><div>https://fulequn.github.io/2021/01/Article202101103/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2021年1月10日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2021/01/Article202101201/" title="Centos安装docker以及ServiceAccount"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Centos安装docker以及ServiceAccount</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/01/Article202101102/" title="Pytorch的view方法"><span class="hidden-mobile">Pytorch的view方法</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>