<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="dropout_layers.py 包含了Dropout前向传播以及反向传播，组合Dropout传播层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778"><meta property="og:type" content="article"><meta property="og:title" content="编码实现深度学习优化"><meta property="og:url" content="https://fulequn.github.io/2020/12/Article202012032/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="dropout_layers.py 包含了Dropout前向传播以及反向传播，组合Dropout传播层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-12-03T12:24:06.000Z"><meta property="article:modified_time" content="2024-05-18T14:35:07.642Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><title>编码实现深度学习优化 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="编码实现深度学习优化"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-12-03 20:24" pubdate>2020年12月3日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>5.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>44 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">编码实现深度学习优化</h1><div class="markdown-body"><h2 id="dropout_layerspy"><a class="markdownIt-Anchor" href="#dropout_layerspy"></a> dropout_layers.py</h2><p>包含了Dropout前向传播以及反向传播，组合Dropout传播层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_forward</span>(<span class="hljs-params">x, dropout_param</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  执行dropout前向传播</span><br><span class="hljs-string">  Inputs:</span><br><span class="hljs-string">  - x: 输入数据</span><br><span class="hljs-string">  - dropout_param: 字典类型，使用下列键值:</span><br><span class="hljs-string">    - p: dropout参数。每个神经元的激活概率p</span><br><span class="hljs-string">    - mode: &#x27;test&#x27;或&#x27;train&#x27;. 训练模式使用dropout;测试模式仅仅返回输入值。</span><br><span class="hljs-string">    - seed: 随机数生成种子. </span><br><span class="hljs-string"></span><br><span class="hljs-string">  Outputs:</span><br><span class="hljs-string">  - out: 和输入数据相同形状</span><br><span class="hljs-string">  - cache:元组(dropout_param, mask). </span><br><span class="hljs-string">          训练模式时，掩码mask用于激活该层神经元，测试模式时不使用</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  p, mode = dropout_param[<span class="hljs-string">&#x27;p&#x27;</span>], dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>  <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;seed&#x27;</span> <span class="hljs-keyword">in</span> dropout_param:<br>    np.random.seed(dropout_param[<span class="hljs-string">&#x27;seed&#x27;</span>])<br><br>  mask = <span class="hljs-literal">None</span><br>  out = <span class="hljs-literal">None</span><br><br>  <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                任务：执行dropout训练模式前向传播。                      #</span><br>    <span class="hljs-comment">###########################################################################</span><br>    mask = (np.random.rand(*x.shape) &lt; p)/p<br>    out =x*mask<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                           结束编码                                      #</span><br>    <span class="hljs-comment">###########################################################################</span><br>  <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#               任务： 执行测试阶段dropout前向传播。                      #</span><br>    <span class="hljs-comment">###########################################################################</span><br>    out = x<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                           结束编码                                      #</span><br>    <span class="hljs-comment">###########################################################################</span><br><br>  cache = (dropout_param, mask)<br>  out = out.astype(x.dtype, copy=<span class="hljs-literal">False</span>)<br><br>  <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_backward</span>(<span class="hljs-params">dout, cache</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  dropout反向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Inputs:</span><br><span class="hljs-string">  - dout: 上层梯度</span><br><span class="hljs-string">  - cache: dropout_forward中的缓存(dropout_param, mask)。</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  dropout_param, mask = cache<br>  mode = dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>  <br>  dx = <span class="hljs-literal">None</span><br>  <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                      任务：实现dropout反向传播                          #</span><br>    <span class="hljs-comment">###########################################################################</span><br>    dx =dout*mask<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                            结束编码                                     #</span><br>    <span class="hljs-comment">###########################################################################</span><br>  <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>    dx = dout<br>  <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_dropout_forward</span>(<span class="hljs-params">x,w,b,dropout_param</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  组合affine_relu_dropout前向传播</span><br><span class="hljs-string">  Inputs:</span><br><span class="hljs-string">  - x: 输入数据</span><br><span class="hljs-string">  - w: 权重参数</span><br><span class="hljs-string">  - b: 偏置项</span><br><span class="hljs-string">  - dropout_param: 字典类型，使用下列键值:</span><br><span class="hljs-string">    - p: dropout参数。每个神经元的激活概率p</span><br><span class="hljs-string">    - mode: &#x27;test&#x27;或&#x27;train&#x27;. 训练模式使用dropout;测试模式仅仅返回输入值。</span><br><span class="hljs-string">    - seed: 随机数生成种子. </span><br><span class="hljs-string"></span><br><span class="hljs-string">  Outputs:</span><br><span class="hljs-string">  - out: 和输入数据相同形状</span><br><span class="hljs-string">  - cache:缓存包含(cache_affine,cache_relu,cache_dropout)</span><br><span class="hljs-string">  &quot;&quot;&quot;</span> <br>  out_dropout = <span class="hljs-literal">None</span><br>  cache =<span class="hljs-literal">None</span><br>  <span class="hljs-comment">#############################################################################</span><br>  <span class="hljs-comment">#               任务: 实现 affine_relu_dropout 神经元前向传播.              #</span><br>  <span class="hljs-comment">#         注意：你需要调用affine_forward以及relu_forward函数，              #</span><br>  <span class="hljs-comment">#              并将各自的缓存保存在cache中                                  #</span><br>  <span class="hljs-comment">#############################################################################  </span><br>  out_affine, cache_affine = affine_forward(x,w,b)<br>  out_relu,cache_relu =relu_forward(out_affine)<br>  out_dropout,cache_dropout =dropout_forward(out_relu,dropout_param)<br>  cache = (cache_affine,cache_relu,cache_dropout)<br>  <span class="hljs-comment">###########################################################################</span><br>  <span class="hljs-comment">#                            结束编码                                     #</span><br>  <span class="hljs-comment">###########################################################################    </span><br>  <span class="hljs-keyword">return</span> out_dropout,cache<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_dropout_backward</span>(<span class="hljs-params">dout,cache</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">   affine_relu_dropout神经元的反向传播</span><br><span class="hljs-string">   </span><br><span class="hljs-string">  Input:</span><br><span class="hljs-string">  - dout: 上层误差梯度</span><br><span class="hljs-string">  - cache: 缓存(cache_affine,cache_relu,cache_dropout)</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Returns:</span><br><span class="hljs-string">  - dx: 输入数据x的梯度</span><br><span class="hljs-string">  - dw: 权重矩阵w的梯度</span><br><span class="hljs-string">  - db: 偏置向量b的梯度</span><br><span class="hljs-string">  &quot;&quot;&quot;</span>  <br>  cache_affine,cache_relu,cache_dropout = cache<br>  dx,dw,db=<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span><br>  <span class="hljs-comment">###########################################################################</span><br>  <span class="hljs-comment">#               任务：实现affine_relu_dropout反向传播                     #</span><br>  <span class="hljs-comment">###########################################################################  </span><br>  ddropout = dropout_backward(dout,cache_dropout)<br>  drelu = relu_backward(ddropout,cache_relu)<br>  dx,dw,db = affine_backward(drelu,cache_affine)<br>  <span class="hljs-comment">###########################################################################</span><br>  <span class="hljs-comment">#                          结束编码                                      #</span><br>  <span class="hljs-comment">###########################################################################</span><br>  <span class="hljs-keyword">return</span> dx,dw,db<br><br><br></code></pre></td></tr></table></figure><h2 id="layerspy"><a class="markdownIt-Anchor" href="#layerspy"></a> <a target="_blank" rel="noopener" href="http://layers.py">layers.py</a></h2><p>之前已经写好的前向传播与后向传播代码以及softmax的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算神经网络当前层的前馈传播。该方法计算在全连接情况下的得分函数</span><br><span class="hljs-string">    注：如果不理解affine仿射变换，简单的理解为在全连接情况下的得分函数即可</span><br><span class="hljs-string"></span><br><span class="hljs-string">    输入数据x的形状为(N, d_1, ..., d_k)，其中N表示数据量，(d_1, ..., d_k)表示</span><br><span class="hljs-string">    每一通道的数据维度。如果是图片数据就为(长，宽，色道)，数据的总维度就为</span><br><span class="hljs-string">    D = d_1 * ... * d_k，因此我们需要数据整合成完整的（N,D)形式再进行仿射变换。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据，其形状为(N, d_1, ..., d_k)的numpy array</span><br><span class="hljs-string">    - w: 权重矩阵，其形状为(D,M)的numpy array，D表示输入数据维度，M表示输出数据维度</span><br><span class="hljs-string">             可以将D看成输入的神经元个数，M看成输出神经元个数</span><br><span class="hljs-string">    - b: 偏置向量，其形状为(M,)的numpy array</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 形状为(N, M)的输出结果</span><br><span class="hljs-string">    - cache: 将输入进行缓存(x, w, b)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                            任务: 实现全连接前向传播                                                         #</span><br>    <span class="hljs-comment">#                                     注：首先你需要将输入数据重塑成行。                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    N=x.shape[<span class="hljs-number">0</span>]<br>    x_new=x.reshape(N,-<span class="hljs-number">1</span>)<span class="hljs-comment">#将x重塑成2维向量</span><br>    out=np.dot(x_new,w)+b<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = (x, w, b)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"> 计算仿射层的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 形状为(N, M)的上层梯度</span><br><span class="hljs-string">    - cache: 元组:</span><br><span class="hljs-string">        - x: (N, d_1, ... d_k)的输入数据</span><br><span class="hljs-string">        - w: 形状为(D, M)的权重矩阵</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度，其形状为(N, d1, ..., d_k)</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度，其形状为(D,M)</span><br><span class="hljs-string">    - db: 偏置项b的梯度，其形状为(M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b = cache<br>    dx, dw, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                    任务: 实现仿射层反向传播                                                                 #</span><br>    <span class="hljs-comment">#                 注意：你需要将x重塑成(N,D)后才能计算各梯度，                                            #</span><br>    <span class="hljs-comment">#                            求完梯度后你需要将dx的形状与x重塑成一样                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    db = np.<span class="hljs-built_in">sum</span>(dout,axis=<span class="hljs-number">0</span>)<br>    xx= x.reshape(x.shape[<span class="hljs-number">0</span>],-<span class="hljs-number">1</span>)<br>    dw = np.dot(xx.T,dout)<br>    dx = np.dot(dout,w.T)<br>    dx=np.reshape(dx,x.shape)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算tified linear units (ReLUs)激活函数的前向传播，并保存相应缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 和输入数据x形状相同</span><br><span class="hljs-string">    - cache: x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                         任务: 实现ReLU 的前向传播.                                                                        #</span><br>    <span class="hljs-comment">#                        注意：你住需要1行代码即可完成                                                                    #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    out =np.maximum(<span class="hljs-number">0</span>,x)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = x<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 rectified linear units (ReLUs)激活函数的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: 输入 x,其形状应该和dout相同</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: x的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, x = <span class="hljs-literal">None</span>, cache<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU 反向传播.                                                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dx=dout<br>    dx[x&lt;=<span class="hljs-number">0</span>]=<span class="hljs-number">0</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                        结束编码                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元前向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入到 affine层的数据</span><br><span class="hljs-string">    - w, b:    affine层的权重矩阵和偏置向量</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: Output from the ReLU的输出结果</span><br><span class="hljs-string">    - cache: 前向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU神经元前向传播.                                                            #</span><br>    <span class="hljs-comment">#                             注意：你需要调用affine_forward以及relu_forward函数，                #</span><br>    <span class="hljs-comment">#                            并将各自的缓存保存在cache中                                                                    #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    a, fc_cache = affine_forward(x, w, b)<br>    out, relu_cache = relu_forward(a)<br>    cache = (fc_cache, relu_cache)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                         结束编码                                                                                         #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元的反向传播</span><br><span class="hljs-string">     </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: affine缓存，以及relu缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度</span><br><span class="hljs-string">    - db: 偏置向量b的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU神经元反向传播.                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    fc_cache, relu_cache = cache<br>    da = relu_backward(dout, relu_cache)<br>    dx, dw, db = affine_backward(da, fc_cache)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                     结束编码                                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss</span>(<span class="hljs-params">x, y</span>):<br>    probs = np.exp(x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    probs /= np.<span class="hljs-built_in">sum</span>(probs, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    N = x.shape[<span class="hljs-number">0</span>]<br>    loss = -np.<span class="hljs-built_in">sum</span>(np.log(probs[np.arange(N), y])) / N<br>    dx = probs.copy()<br>    dx[np.arange(N), y] -= <span class="hljs-number">1</span><br>    dx /= N<br><br>    <span class="hljs-keyword">return</span> loss, dx<br><br></code></pre></td></tr></table></figure><h2 id="fc_netpy"><a class="markdownIt-Anchor" href="#fc_netpy"></a> fc_net.py</h2><p>实现了深层全连接神经网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> dropout_layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> bn_layers <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyConnectedNet</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    &#123;affine - [batch norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim=<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>,hidden_dims=[<span class="hljs-number">100</span>],    num_classes=<span class="hljs-number">10</span>,</span><br><span class="hljs-params">                             dropout=<span class="hljs-number">0</span>, use_batchnorm=<span class="hljs-literal">False</span>, reg=<span class="hljs-number">0.0</span>,</span><br><span class="hljs-params">                             weight_scale=<span class="hljs-number">1e-2</span>, seed=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化全连接网络.    </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - input_dim: 输入维度</span><br><span class="hljs-string">        - hidden_dims: 隐藏层各层维度向量，如[100,100]</span><br><span class="hljs-string">        - num_classes: 分类个数.</span><br><span class="hljs-string">        - dropout: 如果dropout=0，表示不使用dropout.</span><br><span class="hljs-string">        - use_batchnorm：布尔型，是否使用BN</span><br><span class="hljs-string">        - reg:正则化衰减因子.</span><br><span class="hljs-string">        - weight_scale:权重初始化范围，标准差.</span><br><span class="hljs-string">        - seed: 使用seed产生相同的随机数。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.use_batchnorm = use_batchnorm<br>        self.use_dropout = dropout &gt; <span class="hljs-number">0</span><br>        self.reg = reg<br>        self.num_layers = <span class="hljs-number">1</span> + <span class="hljs-built_in">len</span>(hidden_dims)<br>        self.params = &#123;&#125;<br><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                             任务：初始化网络参数                                                                             #</span><br>        <span class="hljs-comment">#                    权重参数初始化和前面章节类似                                                                        #</span><br>        <span class="hljs-comment">#                    针对每一层神经元都要初始化对应的gamma和beta                                         #</span><br>        <span class="hljs-comment">#                    如:第一层使用gamma1，beta1，第二层gamma2,beta2,                                 #</span><br>        <span class="hljs-comment">#                     gamma初始化为1，beta初始化为0                                                                    # </span><br>        <span class="hljs-comment">############################################################################</span><br>        layers_dims = [input_dim]+hidden_dims+[num_classes]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>                self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)] = weight_scale*np.random.randn(layers_dims[i],<br>                                                                                                layers_dims[i+<span class="hljs-number">1</span>])<br>                self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)] = np.zeros((<span class="hljs-number">1</span>, layers_dims[i+<span class="hljs-number">1</span>]))<br>                <span class="hljs-comment"># 批量正则化</span><br>                <span class="hljs-keyword">if</span> self.use_batchnorm <span class="hljs-keyword">and</span> i &lt; <span class="hljs-built_in">len</span>(hidden_dims):<br>                        self.params[<span class="hljs-string">&#x27;gamma&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)] = np.ones((<span class="hljs-number">1</span>, layers_dims[i+<span class="hljs-number">1</span>]))<br>                        self.params[<span class="hljs-string">&#x27;beta&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)] = np.zeros((<span class="hljs-number">1</span>, layers_dims[i+<span class="hljs-number">1</span>]))<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                                        结束编码                                                                            #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment"># dropout相关配置</span><br>        self.dropout_param = &#123;&#125;<br>        <span class="hljs-keyword">if</span> self.use_dropout:<br>            self.dropout_param = &#123;<span class="hljs-string">&#x27;mode&#x27;</span>: <span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>: dropout&#125;<br>            <span class="hljs-keyword">if</span> seed <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                self.dropout_param[<span class="hljs-string">&#x27;seed&#x27;</span>] = seed     <br>        <span class="hljs-comment"># 批量正则化</span><br>        self.bn_params = []<br>        <span class="hljs-keyword">if</span> self.use_batchnorm:<br>            self.bn_params = [&#123;<span class="hljs-string">&#x27;mode&#x27;</span>: <span class="hljs-string">&#x27;train&#x27;</span>&#125; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers - <span class="hljs-number">1</span>)]<br>        <br><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, X, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        计算损失值</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Parameters</span><br><span class="hljs-string">        ----------</span><br><span class="hljs-string">        X : 训练数据</span><br><span class="hljs-string">        y : 标签</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns</span><br><span class="hljs-string">        -------</span><br><span class="hljs-string">        TYPE</span><br><span class="hljs-string">            DESCRIPTION.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-comment"># 设置执行模式</span><br>        mode = <span class="hljs-string">&#x27;test&#x27;</span> <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;train&#x27;</span><br><br>        <span class="hljs-keyword">if</span> self.dropout_param <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>] = mode     <br>        <span class="hljs-keyword">if</span> self.use_batchnorm:<br>            <span class="hljs-keyword">for</span> bn_param <span class="hljs-keyword">in</span> self.bn_params:<br>                bn_param[mode] = mode<br><br>        scores = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                    任务：执行全连接网络的前馈过程。                                                #</span><br>        <span class="hljs-comment">#                             计算数据的分类得分，将结果保存在scores中。                                 #</span><br>        <span class="hljs-comment">#            当使用dropout时，你需要使用self.dropout_param进行dropout前馈。            #</span><br>        <span class="hljs-comment">#            当使用BN时，self.bn_params[0]传到第一层，self.bn_params[1]第二层        #</span><br>        <span class="hljs-comment">############################################################################</span><br>        outs, cache = &#123;&#125;, &#123;&#125;<br>        outs[<span class="hljs-number">0</span>] = X<br>        num_h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_h):<br>            <span class="hljs-keyword">if</span> self.use_dropout:<br>                outs[i+<span class="hljs-number">1</span>], cache[i+<span class="hljs-number">1</span>] = affine_relu_dropout_forward(<br>                    outs[i], self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)],<br>                    self.dropout_param)<br>            <span class="hljs-keyword">elif</span> self.use_batchnorm:<br>                gamma = self.params[<span class="hljs-string">&#x27;gamma&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]<br>                beta = self.params[<span class="hljs-string">&#x27;beta&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]<br>                outs[i+<span class="hljs-number">1</span>], cache[i+<span class="hljs-number">1</span>] = affine_bn_relu_forward(outs[i],<br>                    self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], gamma,<br>                    beta, self.bn_params[i])<br>            <span class="hljs-keyword">else</span>:<br>                outs[i+<span class="hljs-number">1</span>], cache[i+<span class="hljs-number">1</span>] = affine_relu_forward(outs[i],<br>                    self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)])<br>        scores, cache[num_h+<span class="hljs-number">1</span>] = affine_forward(outs[num_h], <br>                        self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)])<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment"># 测试模式</span><br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>            <span class="hljs-keyword">return</span> scores<br>        <span class="hljs-comment"># 损失值与梯度</span><br>        loss, grads = <span class="hljs-number">0.0</span>, &#123;&#125;<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                任务：实现全连接网络的反向传播。                                                                    #</span><br>        <span class="hljs-comment">#                将损失值储存在loss中，梯度值储存在grads字典中                                         #</span><br>        <span class="hljs-comment">#                当使用dropout时，需要求解dropout梯度                                                            #</span><br>        <span class="hljs-comment">#                当使用BN时，需要求解BN梯度                                                                                #</span><br>        <span class="hljs-comment">############################################################################</span><br>        dout = &#123;&#125;<br>        loss, dy = softmax_loss(scores, y)<br>        h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>            loss += <span class="hljs-number">0.5</span>*self.reg*(np.<span class="hljs-built_in">sum</span>(self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]))<br>        dout[h], grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)], grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)] = affine_backward(dy, cache[h+<span class="hljs-number">1</span>])<br>        grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)] += self.reg*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(h):<br>            <span class="hljs-keyword">if</span> self.use_dropout:<br>                dx, dw, db = affine_relu_dropout_backward(dout[h-i], cache[h-i])<br>                dout[h-<span class="hljs-number">1</span>-i] = dx<br>                grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = dw<br>                grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = db<br>            <span class="hljs-keyword">elif</span> self.use_batchnorm:<br>                dx, dw, db, dgamma, dbeta = affine_bn_relu_backward(dout[h-i], cache[h-i])<br>                dout[h-<span class="hljs-number">1</span>-i] = dx<br>                grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = dw<br>                grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = db<br>                grads[<span class="hljs-string">&#x27;gamma&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = dgamma<br>                grads[<span class="hljs-string">&#x27;beta&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = dbeta<br>            <span class="hljs-keyword">else</span>:<br>                dx, dw, db = affine_relu_backward(dout[h-i], cache[h-i])<br>                dout[h-<span class="hljs-number">1</span>-i] = dx<br>                grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = dw<br>                grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = db<br>            grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] += self.reg*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)]<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">return</span> loss, grads<br><br></code></pre></td></tr></table></figure><h2 id="trainerpy"><a class="markdownIt-Anchor" href="#trainerpy"></a> <a target="_blank" rel="noopener" href="http://trainer.py">trainer.py</a></h2><p>解耦训练器的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> updater<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用形式:</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    data = &#123;</span><br><span class="hljs-string">        &#x27;X_train&#x27;: # 训练数据</span><br><span class="hljs-string">        &#x27;y_train&#x27;: # 训练类标</span><br><span class="hljs-string">        &#x27;X_val&#x27;: # 验证数据</span><br><span class="hljs-string">        &#x27;X_train&#x27;: # 验证类标</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    model = MyAwesomeModel(hidden_size=100, reg=10)</span><br><span class="hljs-string">    Trainer = Trainer(model, data,</span><br><span class="hljs-string">                                    update_rule=&#x27;sgd&#x27;,</span><br><span class="hljs-string">                                    updater_config=&#123;</span><br><span class="hljs-string">                                        &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="hljs-string">                                    &#125;,</span><br><span class="hljs-string">                                    lr_decay=0.95,</span><br><span class="hljs-string">                  num_epochs=10, batch_size=100,</span><br><span class="hljs-string">                  print_every=100)</span><br><span class="hljs-string">  Trainer.train()</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, data, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        构造一个新的Trainer实例</span><br><span class="hljs-string">        必须参数:</span><br><span class="hljs-string">        - model: 网络模型</span><br><span class="hljs-string">        - data: 数据字典，其中:</span><br><span class="hljs-string">            &#x27;X_train&#x27;:    形状为(N_train, d_1, ..., d_k)训练数据</span><br><span class="hljs-string">            &#x27;X_val&#x27;:    形状为(N_val, d_1, ..., d_k) 验证数据</span><br><span class="hljs-string">            &#x27;y_train&#x27;:    形状为(N_train,) 训练数据类标</span><br><span class="hljs-string">            &#x27;y_val&#x27;:    形状为(N_val,) 验证数据类标</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        可选参数:</span><br><span class="hljs-string">        - update_rule: 更新规则，其存放在updater.py文件中，默认选项为&#x27;sgd&#x27;。</span><br><span class="hljs-string">        - updater_config: 字典类型的，更新规则所对应的超参数配置，同见updater.py文件。</span><br><span class="hljs-string">        - lr_decay: 学习率衰减系数。</span><br><span class="hljs-string">        - batch_size: 批量数据大小</span><br><span class="hljs-string">        - num_epochs: 训练周期</span><br><span class="hljs-string">        - print_every: 整数型; 每迭代多少次进行打印一次中间结果</span><br><span class="hljs-string">        - verbose: 布尔型; 是否在训练期间打印中间结果</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.model = model<br>        self.X_train = data[<span class="hljs-string">&#x27;X_train&#x27;</span>]<br>        self.y_train = data[<span class="hljs-string">&#x27;y_train&#x27;</span>]<br>        self.X_val = data[<span class="hljs-string">&#x27;X_val&#x27;</span>]<br>        self.y_val = data[<span class="hljs-string">&#x27;y_val&#x27;</span>]<br>        <br>        <span class="hljs-comment"># 弹出可选参数，进行相关配置</span><br>        self.update_rule = kwargs.pop(<span class="hljs-string">&#x27;update_rule&#x27;</span>, <span class="hljs-string">&#x27;sgd&#x27;</span>)<br>        self.updater_config = kwargs.pop(<span class="hljs-string">&#x27;updater_config&#x27;</span>, &#123;&#125;)<br>        self.lr_decay = kwargs.pop(<span class="hljs-string">&#x27;lr_decay&#x27;</span>, <span class="hljs-number">1.0</span>)<br>        self.batch_size = kwargs.pop(<span class="hljs-string">&#x27;batch_size&#x27;</span>, <span class="hljs-number">100</span>)<br>        self.num_epochs = kwargs.pop(<span class="hljs-string">&#x27;num_epochs&#x27;</span>, <span class="hljs-number">10</span>)<br><br>        self.print_every = kwargs.pop(<span class="hljs-string">&#x27;print_every&#x27;</span>, <span class="hljs-number">10</span>)<br>        self.verbose = kwargs.pop(<span class="hljs-string">&#x27;verbose&#x27;</span>, <span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 若可选参数错误，抛出异常</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(kwargs) &gt; <span class="hljs-number">0</span>:<br>            extra = <span class="hljs-string">&#x27;, &#x27;</span>.join(<span class="hljs-string">&#x27;&quot;%s&quot;&#x27;</span> % k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> kwargs.keys())<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Unrecognized arguments %s&#x27;</span> % extra)<br><br><br>        <span class="hljs-comment">#确认updater中含有更新规则</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(updater, self.update_rule):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % self.update_rule)<br>        self.update_rule = <span class="hljs-built_in">getattr</span>(updater, self.update_rule)<br><br>        <span class="hljs-comment"># 初始化相关变量</span><br>        self.epoch = <span class="hljs-number">0</span><br>        self.best_val_acc = <span class="hljs-number">0</span><br>        self.best_params = &#123;&#125;<br>        self.loss_history = []<br>        self.train_acc_history = []<br>        self.val_acc_history = []<br><br>        <span class="hljs-comment"># 对updater_config中的参数进行深拷贝</span><br>        self.updater_configs = &#123;&#125;<br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.model.params:<br>            d = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.updater_config.items()&#125;<br>            self.updater_configs[p] = d<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        执行单步梯度更新</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 采样批量数据</span><br>        num_train = self.X_train.shape[<span class="hljs-number">0</span>]<br>        batch_mask = np.random.choice(num_train, self.batch_size)<br>        X_batch = self.X_train[batch_mask]<br>        y_batch = self.y_train[batch_mask]<br><br>        <span class="hljs-comment"># 计算损失及梯度</span><br>        loss, grads = self.model.loss(X_batch, y_batch)<br>        self.loss_history.append(loss)<br><br>        <span class="hljs-comment"># 更新参数</span><br>        <span class="hljs-keyword">for</span> p, w <span class="hljs-keyword">in</span> self.model.params.items():<br>            dw = grads[p]<br>            config = self.updater_configs[p]<br>            next_w, next_config = self.update_rule(w, dw, config)<br>            self.model.params[p] = next_w<br>            self.updater_configs[p] = next_config<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_accuracy</span>(<span class="hljs-params">self, X, y, num_samples=<span class="hljs-literal">None</span>, batch_size=<span class="hljs-number">100</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     根据提供的数据检验精度，若数据集过大，可进行采样测试。</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 形状为(N, d_1, ..., d_k)的数据</span><br><span class="hljs-string">        - y: 形状为 (N,)的数据类标</span><br><span class="hljs-string">        - num_samples: 采样次数</span><br><span class="hljs-string">        - batch_size:批量数据大小</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - acc: 测试数据正确率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        <span class="hljs-comment"># 对数据进行采样</span><br>        N = X.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> num_samples <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> N &gt; num_samples:<br>            mask = np.random.choice(N, num_samples)<br>            N = num_samples<br>            X = X[mask]<br>            y = y[mask]<br><br>        <span class="hljs-comment"># 计算精度</span><br>        num_batches = <span class="hljs-built_in">int</span>(N / batch_size)<br>        <span class="hljs-keyword">if</span> N % batch_size != <span class="hljs-number">0</span>:<br>            num_batches += <span class="hljs-number">1</span><br>        y_pred = []<br>        <span class="hljs-comment"># 遍历所有批次</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):<br>            start = i * batch_size<br>            end = (i + <span class="hljs-number">1</span>) * batch_size<br>            <br>            scores = self.model.loss(X[start:end])<br>            <br>            y_pred.append(np.argmax(scores, axis=<span class="hljs-number">1</span>))<br>        y_pred = np.hstack(y_pred)<br>        acc = np.mean(y_pred == y)<br><br>        <span class="hljs-keyword">return</span> acc<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        根据配置训练模型</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train = self.X_train.shape[<span class="hljs-number">0</span>]<br>        iterations_per_epoch = <span class="hljs-built_in">max</span>(num_train / self.batch_size, <span class="hljs-number">1</span>)<br>        num_iterations = <span class="hljs-built_in">int</span>(self.num_epochs * iterations_per_epoch)<br><br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>            self._step()<br><br>            <span class="hljs-comment"># 打印损失值</span><br>            <span class="hljs-keyword">if</span> self.verbose <span class="hljs-keyword">and</span> t % self.print_every == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(迭代 %d / %d) 损失值: %f&#x27;</span> % (t + <span class="hljs-number">1</span>, num_iterations, self.loss_history[-<span class="hljs-number">1</span>]))<br><br>            <span class="hljs-comment"># 更新学习率</span><br>            epoch_end = (t + <span class="hljs-number">1</span>) % iterations_per_epoch == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> epoch_end:<br>                self.epoch += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> self.updater_configs:<br>                    self.updater_configs[k][<span class="hljs-string">&#x27;learning_rate&#x27;</span>] *= self.lr_decay<br><br><br>            <span class="hljs-comment">#在训练的开始，末尾，每一轮训练周期检验精度</span><br>            first_it = (t == <span class="hljs-number">0</span>)<br>            last_it = (t == num_iterations + <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> first_it <span class="hljs-keyword">or</span> last_it <span class="hljs-keyword">or</span> epoch_end:<br>                train_acc = self.check_accuracy(self.X_train, self.y_train,<br>                                                                                num_samples=<span class="hljs-number">1000</span>)<br>                val_acc = self.check_accuracy(self.X_val, self.y_val)<br>                self.train_acc_history.append(train_acc)<br>                self.val_acc_history.append(val_acc)<br><br>                <span class="hljs-keyword">if</span> self.verbose:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(周期 %d / %d) 训练精度: %f; 验证精度: %f&#x27;</span> % (<br>                                 self.epoch, self.num_epochs, train_acc, val_acc))<br><br>                <span class="hljs-comment"># 记录最佳模型</span><br>                <span class="hljs-keyword">if</span> val_acc &gt; self.best_val_acc:<br>                    self.best_val_acc = val_acc<br>                    self.best_params = &#123;&#125;<br>                    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.model.params.items():<br>                        self.best_params[k] = v.copy()<br><br>        <span class="hljs-comment"># 训练结束后返回最佳模型</span><br>        self.model.params = self.best_params<br><br><br></code></pre></td></tr></table></figure><h2 id="updaterpy"><a class="markdownIt-Anchor" href="#updaterpy"></a> <a target="_blank" rel="noopener" href="http://updater.py">updater.py</a></h2><p>解耦更新器，主要负责更新神经网络的权重，其传入参数有神经网络的权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span>、当前权重的梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">dw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span>及相应的更新配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">频繁使用在训练神经网络中的一阶梯度更新规则。每次更新接受当前的权重，</span><br><span class="hljs-string">对应的梯度，以及相关配置进行权重更新。</span><br><span class="hljs-string">def update(w, dw, config=None):</span><br><span class="hljs-string">Inputs:</span><br><span class="hljs-string">    - w:当前权重.</span><br><span class="hljs-string">    - dw: 和权重形状相同的梯度.</span><br><span class="hljs-string">    - config: 字典型超参数配置，比如学习率，动量值等。如果更新规则需要用到缓存，</span><br><span class="hljs-string">        在配置中需要保存相应的缓存。</span><br><span class="hljs-string"></span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">    - next_w: 更新后的权重.</span><br><span class="hljs-string">    - config: 更新规则相应的配置.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    随机梯度下降更新规则.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    config 格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br><br>    w -= config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] * dw<br>    <span class="hljs-keyword">return</span> w, config<br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd_momentum</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    动量随机梯度下降更新规则。</span><br><span class="hljs-string">    config 使用格式:</span><br><span class="hljs-string">    - learning_rate: 学习率。</span><br><span class="hljs-string">    - momentum: [0,1]的动量衰减因子，0表示不使用动量，即退化为SGD。</span><br><span class="hljs-string">    - velocity: 和w，dw形状相同的速度。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: <br>      config = &#123; &#125;<br>    <span class="hljs-comment"># 设置默认值</span><br>    <span class="hljs-comment"># 学习率</span><br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br>    <span class="hljs-comment"># 动量衰减因子</span><br>    config.setdefault(<span class="hljs-string">&#x27;momentum&#x27;</span>, <span class="hljs-number">0.9</span>)<br>    <span class="hljs-comment"># 速度</span><br>    v = config.setdefault(<span class="hljs-string">&#x27;velocity&#x27;</span>, np.zeros_like(w))<br>    <br>    next_w = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                  任务：实现动量更新                                                                    #</span><br>    <span class="hljs-comment">#                 更新后的速度存放在v中，更新后的权重存放在next_w中                                 #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    v = config[<span class="hljs-string">&#x27;momentum&#x27;</span>]*config[<span class="hljs-string">&#x27;velocity&#x27;</span>]-config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>]*dw<br>    next_w = w+v<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 更新速度</span><br>    config[<span class="hljs-string">&#x27;velocity&#x27;</span>] = v<br><br>    <span class="hljs-keyword">return</span> next_w, config<br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rmsprop</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    RMSProp更新规则</span><br><span class="hljs-string"></span><br><span class="hljs-string">    config 使用格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    - decay_rate:历史累积梯度衰减率因子,取值为[0,1]</span><br><span class="hljs-string">    - epsilon: 避免除零异常的小数.</span><br><span class="hljs-string">    - cache:历史梯度缓存.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;decay_rate&#x27;</span>, <span class="hljs-number">0.99</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;epsilon&#x27;</span>, <span class="hljs-number">1e-8</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;cache&#x27;</span>, np.zeros_like(w))<br><br>    next_w = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                 任务：实现 RMSprop 更新                                                     #</span><br>    <span class="hljs-comment">#    将更新后的权重保存在next_w中，将历史梯度累积存放在config[&#x27;cache&#x27;]中。        #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    config[<span class="hljs-string">&#x27;cache&#x27;</span>] = config[<span class="hljs-string">&#x27;decay_rate&#x27;</span>]*config[<span class="hljs-string">&#x27;cache&#x27;</span>]+(<span class="hljs-number">1</span>-config[<span class="hljs-string">&#x27;decay_rate&#x27;</span>])*dw**<span class="hljs-number">2</span><br>    next_w = w-config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>]*dw/(np.sqrt(config[<span class="hljs-string">&#x27;cache&#x27;</span>]+config[<span class="hljs-string">&#x27;epsilon&#x27;</span>]))<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> next_w, config<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">adam</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用 Adam更新规则 ,融合了“热身”更新操作。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    config 使用格式:</span><br><span class="hljs-string">    - learning_rate: 学习率.</span><br><span class="hljs-string">    - beta1: 动量衰减因子.</span><br><span class="hljs-string">    - beta2: 学习率衰减因子.</span><br><span class="hljs-string">    - epsilon: 防除0小数.</span><br><span class="hljs-string">    - m: 梯度.</span><br><span class="hljs-string">    - v: 梯度平方.</span><br><span class="hljs-string">    - t: 迭代次数.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: config = &#123;&#125;<br>    config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-3</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;beta1&#x27;</span>, <span class="hljs-number">0.9</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;beta2&#x27;</span>, <span class="hljs-number">0.999</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;epsilon&#x27;</span>, <span class="hljs-number">1e-8</span>)<br>    config.setdefault(<span class="hljs-string">&#x27;m&#x27;</span>, np.zeros_like(w))<br>    config.setdefault(<span class="hljs-string">&#x27;v&#x27;</span>, np.zeros_like(w))<br>    config.setdefault(<span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-number">0</span>)<br>    <br>    next_w = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                    任务：实现Adam更新                                                             #                             </span><br>    <span class="hljs-comment">#         将更新后的权重存放在next_w中，记得将m,v,t存放在相应的config中                 #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    config[<span class="hljs-string">&#x27;t&#x27;</span>] += <span class="hljs-number">1</span><br>    beta1 = config[<span class="hljs-string">&#x27;beta1&#x27;</span>]<br>    beta2 = config[<span class="hljs-string">&#x27;beta2&#x27;</span>]<br>    epsilon = config[<span class="hljs-string">&#x27;epsilon&#x27;</span>]<br>    learning_rate = config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>]<br>    config[<span class="hljs-string">&#x27;m&#x27;</span>] = beta1*config[<span class="hljs-string">&#x27;m&#x27;</span>]+(<span class="hljs-number">1</span>-beta1)*dw<br>    config[<span class="hljs-string">&#x27;v&#x27;</span>] = beta2*config[<span class="hljs-string">&#x27;v&#x27;</span>]+(<span class="hljs-number">1</span>-beta2)*dw**<span class="hljs-number">2</span><br>    mb = config[<span class="hljs-string">&#x27;m&#x27;</span>]/(<span class="hljs-number">1</span>-beta1**config[<span class="hljs-string">&#x27;t&#x27;</span>])<br>    vb = config[<span class="hljs-string">&#x27;v&#x27;</span>]/(<span class="hljs-number">1</span>-beta2**config[<span class="hljs-string">&#x27;t&#x27;</span>])<br>    next_w = w-learning_rate*mb/(np.sqrt(vb)+epsilon)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                        结束编码                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> next_w, config<br><br><br></code></pre></td></tr></table></figure><h2 id="bn_layerspy"><a class="markdownIt-Anchor" href="#bn_layerspy"></a> bn_layers.py</h2><p>实现BN算法的前向传播、反向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> dropout_layers <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchnorm_forward</span>(<span class="hljs-params">x, gamma, beta, bn_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用类似动量衰减的运行时平均，计算总体均值与方差 例如:</span><br><span class="hljs-string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span><br><span class="hljs-string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 数据(N, D)</span><br><span class="hljs-string">    - gamma: 缩放参数 (D,)</span><br><span class="hljs-string">    - beta: 平移参数 (D,)</span><br><span class="hljs-string">    - bn_param: 字典型，使用下列键值:</span><br><span class="hljs-string">        - mode: &#x27;train&#x27; 或&#x27;test&#x27;; </span><br><span class="hljs-string">        - eps: 保证数值稳定</span><br><span class="hljs-string">        - momentum: 运行时平均衰减因子 </span><br><span class="hljs-string">        - running_mean: 形状为(D,)的运行时均值</span><br><span class="hljs-string">        - running_var : 形状为 (D,)的运行时方差</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 输出(N, D)</span><br><span class="hljs-string">    - cache: 用于反向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 不同阶段</span><br>    mode = bn_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>    <span class="hljs-comment"># 防止为0</span><br>    eps = bn_param.get(<span class="hljs-string">&#x27;eps&#x27;</span>, <span class="hljs-number">1e-5</span>)<br>    <span class="hljs-comment"># 动量衰减</span><br>    momentum = bn_param.get(<span class="hljs-string">&#x27;momentum&#x27;</span>, <span class="hljs-number">0.9</span>)<br><br>    N, D = x.shape    <span class="hljs-comment"># N:数据数目 D:数据维度</span><br>    <span class="hljs-comment"># 获取运行时均差以及运行时方差，默认为0</span><br>    running_mean = bn_param.get(<span class="hljs-string">&#x27;running_mean&#x27;</span>, np.zeros(D, dtype=x.dtype))<br>    running_var = bn_param.get(<span class="hljs-string">&#x27;running_var&#x27;</span>, np.zeros(D, dtype=x.dtype))<br><br>    out, cache = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>        <span class="hljs-comment">#############################################################################</span><br>        <span class="hljs-comment">#                            任务：实现训练阶段BN的前向传播                                                             #</span><br>        <span class="hljs-comment">#            首先，你需要计算输入数据的均值和方差 ；                                                            #</span><br>        <span class="hljs-comment">#            然后，使用均值和方差将数据进行归一化处理；                                                     #</span><br>        <span class="hljs-comment">#            之后，使用gamma和beta参数将数据进行缩放和平移；                                            #</span><br>        <span class="hljs-comment">#            最后，将该批数据均值和方差添加到累积均值和方差中；                                     #</span><br>        <span class="hljs-comment">#            注意：将反向传播时所需的所有中间值保存在cache中。                                        #</span><br>        <span class="hljs-comment">#############################################################################</span><br>        mu = <span class="hljs-number">1</span>/<span class="hljs-built_in">float</span>(N)*np.<span class="hljs-built_in">sum</span>(x, axis=<span class="hljs-number">0</span>)<br>        xmu = x-mu<br>        carre = xmu**<span class="hljs-number">2</span><br>        var = <span class="hljs-number">1</span>/<span class="hljs-built_in">float</span>(N)*np.<span class="hljs-built_in">sum</span>(carre, axis=<span class="hljs-number">0</span>)<br>        sqrtvar = np.sqrt(var+eps)<br>        invvar = <span class="hljs-number">1.</span>/sqrtvar<br>        va2 = xmu*invvar<br>        va3 = gamma*va2<br>        out = va3+beta<br>        running_mean = momentum*running_mean+(<span class="hljs-number">1.0</span>-momentum)*mu<br>        running_var = momentum * running_var+(<span class="hljs-number">1.0</span>-momentum)*var<br>        cache=(mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param)<br>        <span class="hljs-comment">#############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        <span class="hljs-comment">#############################################################################</span><br>        <span class="hljs-comment">#                    任务：实现测试阶段BN的前向传播                                                                     #</span><br>        <span class="hljs-comment">#            首先，使用运行时均值与方差归一化数据，                                                             #</span><br>        <span class="hljs-comment">#            然后，使用gamma和beta参数缩放，平移数据。                                                        #</span><br>        <span class="hljs-comment">#############################################################################</span><br>        mu = running_mean<br>        var = running_var<br>        xhat = (x-mu)/np.sqrt(var+eps)<br>        out = gamma*xhat+beta<br>        cache = (mu, var, gamma, beta, bn_param)<br>        <span class="hljs-comment">#############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;无法识别的BN模式： &quot;%s&quot;&#x27;</span> % mode)<br> <br>     <span class="hljs-comment"># 更新运行时均值，方差</span><br>    bn_param[<span class="hljs-string">&#x27;running_mean&#x27;</span>] = running_mean<br>    bn_param[<span class="hljs-string">&#x27;running_var&#x27;</span>] = running_var<br><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchnorm_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    BN反向传播 </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度 (N, D)</span><br><span class="hljs-string">    - cache: 前向传播时的缓存.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 数据梯度 (N, D)</span><br><span class="hljs-string">    - dgamma: gamma梯度 (D,)</span><br><span class="hljs-string">    - dbeta: beta梯度 (D,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dgamma, dbeta = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                任务：实现BN反向传播                                                                             #</span><br>    <span class="hljs-comment">#                     将结果分别保存在dx,dgamma,dbeta中                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param=cache<br>    eps = bn_param.get(<span class="hljs-string">&#x27;eps&#x27;</span>, <span class="hljs-number">1e-5</span>)<br>    N, D = dout.shape<br>    <span class="hljs-comment"># 第9步反向传播</span><br>    dva3 = dout<br>    dbeta = np.<span class="hljs-built_in">sum</span>(dout, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># 第8步反向传播</span><br>    dva2 = gamma*dva3<br>    dgamma = np.<span class="hljs-built_in">sum</span>(va2*dva3, axis=<span class="hljs-number">0</span>)    <br>    <span class="hljs-comment"># 第7步反向传播</span><br>    dxmu = invvar*dva2<br>    dinvvar = np.<span class="hljs-built_in">sum</span>(xmu*dva2, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># 第6步反向传播</span><br>    dsqrtvar = -<span class="hljs-number">1.</span>/(sqrtvar**<span class="hljs-number">2</span>)*dinvvar<br>    <span class="hljs-comment"># 第5步反向传播</span><br>    dvar = <span class="hljs-number">0.5</span>*(var+eps)**(-<span class="hljs-number">0.5</span>)*dsqrtvar<br>    <span class="hljs-comment"># 第4步反向传播</span><br>    dcarre = <span class="hljs-number">1</span>/<span class="hljs-built_in">float</span>(N)*np.ones((carre.shape))*dvar<br>    <span class="hljs-comment"># 第3步反向传播</span><br>    dxmu += <span class="hljs-number">2</span>*xmu*dcarre<br>    <span class="hljs-comment"># 第2步反向传播</span><br>    dx = dxmu<br>    dmu = -np.<span class="hljs-built_in">sum</span>(dxmu, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># 第1步反向传播</span><br>    dx += <span class="hljs-number">1</span>/<span class="hljs-built_in">float</span>(N)*np.ones((dxmu.shape))*dmu<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dgamma, dbeta<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batchnorm_backward_alt</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    可选的BN反向传播</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, dgamma, dbeta = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    mu, xmu, carre, var, sqrtvar, invvar, va2, va3, gamma, beta, x, bn_param = cache<br>    eps = bn_param.get(<span class="hljs-string">&#x27;eps&#x27;</span>, <span class="hljs-number">1e-5</span>)<br>    N, D = dout.shape<br>    dbeta = np.<span class="hljs-built_in">sum</span>(dout, axis=<span class="hljs-number">0</span>)<br>    dgamma = np.<span class="hljs-built_in">sum</span>((x - mu) * (var + eps)**(-<span class="hljs-number">1.</span> / <span class="hljs-number">2.</span>) * dout, axis=<span class="hljs-number">0</span>)<br>    dx = (<span class="hljs-number">1.</span>/N) * gamma * (var + eps)**(-<span class="hljs-number">1.</span>/<span class="hljs-number">2.</span>)*(N*dout-np.<span class="hljs-built_in">sum</span>(<br>                        dout, axis=<span class="hljs-number">0</span>)-(x-mu)*(var+eps)**(-<span class="hljs-number">1.0</span>)*np.<span class="hljs-built_in">sum</span>(dout*(x-mu),axis=<span class="hljs-number">0</span>))<br> <br>    <span class="hljs-keyword">return</span> dx, dgamma, dbeta<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_bn_relu_forward</span>(<span class="hljs-params">x,w,b,gamma, beta,bn_param</span>):<br>    x_affine,cache_affine= affine_forward(x,w,b)<br>    x_bn,cache_bn = batchnorm_forward(x_affine,gamma, beta,bn_param)<br>    out,cache_relu = relu_forward(x_bn)<br>    cache = (cache_affine,cache_bn,cache_relu)<br>    <span class="hljs-keyword">return</span> out,cache<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_bn_relu_backward</span>(<span class="hljs-params">dout,cache</span>):<br>    cache_affine,cache_bn,cache_relu = cache<br>    drelu = relu_backward(dout,cache_relu)<br>    dbn,dgamma, dbeta= batchnorm_backward_alt(drelu,cache_bn)<br>    dx,dw,db = affine_backward(dbn,cache_affine)<br>    <span class="hljs-keyword">return</span> dx,dw,db,dgamma,dbeta<br><br><br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>编码实现深度学习优化</div><div>https://fulequn.github.io/2020/12/Article202012032/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2020年12月3日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2020/12/Article202012033/" title="Typora添加数学公式"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Typora添加数学公式</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/12/Article202012031/" title="深度学习实战 第5章深度学习优化笔记"><span class="hidden-mobile">深度学习实战 第5章深度学习优化笔记</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>