<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="dropout_layers.py 包含了Dropout前向传播以及反向传播，组合Dropout传播层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778"><meta property="og:type" content="article"><meta property="og:title" content="编码实现Dropout正则化编码"><meta property="og:url" content="https://fulequn.github.io/2020/11/Article202011305/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="dropout_layers.py 包含了Dropout前向传播以及反向传播，组合Dropout传播层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-11-30T04:49:06.000Z"><meta property="article:modified_time" content="2024-05-30T00:16:34.000Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><title>编码实现Dropout正则化编码 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:60,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="编码实现Dropout正则化编码"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-11-30 12:49" pubdate>2020年11月30日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>3.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>33 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">编码实现Dropout正则化编码</h1><div class="markdown-body"><h2 id="dropout_layerspy"><a class="markdownIt-Anchor" href="#dropout_layerspy"></a> dropout_layers.py</h2><p>包含了Dropout前向传播以及反向传播，组合Dropout传播层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> .layers <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_forward</span>(<span class="hljs-params">x, dropout_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    执行dropout前向传播过程。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string">    - dropout_param: 字典类型的dropout参数，使用下列键值:</span><br><span class="hljs-string">        - p: dropout激活参数，每个神经元的激活概率p。</span><br><span class="hljs-string">        - mode: &#x27;test&#x27;或&#x27;train&#x27;，train：使用激活概率p与神经元进行&quot;and&quot;运算;</span><br><span class="hljs-string">                                                            test：去除激活概率p仅仅返回输入值。</span><br><span class="hljs-string">        - seed: 随机数生成种子. </span><br><span class="hljs-string">    Outputs:</span><br><span class="hljs-string">    - out: 和输入数据形状相同。</span><br><span class="hljs-string">    - cache:元组(dropout_param, mask). </span><br><span class="hljs-string">                    训练模式：掩码mask用于激活该层神经元为“1”激活，为“0”抑制。</span><br><span class="hljs-string">                    测试模式：去除掩码操作。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    p, mode = dropout_param[<span class="hljs-string">&#x27;p&#x27;</span>], dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>    <span class="hljs-comment"># 设置随机种子，控制输出稳定</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;seed&#x27;</span> <span class="hljs-keyword">in</span> dropout_param:<br>        np.random.seed(dropout_param[<span class="hljs-string">&#x27;seed&#x27;</span>])<br><br>    mask = <span class="hljs-literal">None</span>  <span class="hljs-comment"># 掩码层,用于筛选神经元</span><br>    out = <span class="hljs-literal">None</span>  <span class="hljs-comment"># 输出层结果</span><br><br>    <span class="hljs-comment"># 训练阶段</span><br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                                任务：执行训练阶段dropout前向传播。                                            #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment"># 使用mask=mask/p，这样就可以保证输出均值与输入均值相同</span><br>        mask = (np.random.rand(*x.shape)&lt;p)/p<br>        out = x*mask<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                                                     结束编码                                                                            #</span><br>        <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment"># 测试阶段</span><br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                             任务： 执行测试阶段dropout前向传播。                                            #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        out = x<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                                                     结束编码                                                                            #</span><br>        <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment"># 输入参数与掩码层</span><br>    cache = (dropout_param, mask)<br>    out = out.astype(x.dtype, copy=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    dropout反向传播过程。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 上层梯度，形状和其输入相同。</span><br><span class="hljs-string">    - cache: 前向传播中的缓存(dropout_param, mask)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dropout_param, mask = cache<br>    mode = dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>]<br>    <br>    dx = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                                            任务：实现dropout反向传播                                                    #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        dx = dout*mask<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                                                        结束编码                                                                         #</span><br>        <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        dx = dout<br>    <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_dropout_forward</span>(<span class="hljs-params">x,w,b,dropout_param</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    组合affine_relu_dropout前向传播过程。</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据，其形状为(N, d_1, ..., d_k)的numpy数组。</span><br><span class="hljs-string">    - w: 权重矩阵，其形状为(D,M)的numpy数组，</span><br><span class="hljs-string">             D表示输入数据维度，M表示输出数据维度。</span><br><span class="hljs-string">             可以将D看成输入的神经元个数，M看成输出神经元个数。</span><br><span class="hljs-string">    - b: 偏置向量，其形状为(M,)的numpy数组。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    - dropout_param: 字典类型的dropout参数，使用下列键值:</span><br><span class="hljs-string">        - p: dropout激活参数，每个神经元的激活概率p。</span><br><span class="hljs-string">        - mode: &#x27;test&#x27;或&#x27;train&#x27;，train：使用激活概率p与神经元进行&quot;and&quot;运算;</span><br><span class="hljs-string">                                                            test：去除激活概率p仅仅返回输入值。</span><br><span class="hljs-string">        - seed: 随机数生成种子.    </span><br><span class="hljs-string"></span><br><span class="hljs-string">    Outputs:</span><br><span class="hljs-string">    - out: 和输入数据形状相同。</span><br><span class="hljs-string">    - cache:缓存包含(cache_affine,cache_relu,cache_dropout)</span><br><span class="hljs-string">                    cache_affine：仿射前向传播的各项缓存；</span><br><span class="hljs-string">                    cache_relu：ReLU前向传播的各项缓存；</span><br><span class="hljs-string">                    cache_dropout：dropout前向传播的各项缓存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span> <br>    out_dropout = <span class="hljs-literal">None</span><br>    cache = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 affine_relu_dropout 神经元前向传播.                            #</span><br>    <span class="hljs-comment">#                 注意：你需要调用affine_forward以及relu_forward函数，                            #</span><br>    <span class="hljs-comment">#                            并将各自的缓存保存在cache中                                                                    #</span><br>    <span class="hljs-comment">#############################################################################    </span><br>    out_affine, cache_affine = affine_forward(x, w, b)<br>    out_relu, cache_relu = relu_forward(out_affine)<br>    out_dropout, cache_dropout = dropout_forward(out_relu, dropout_param)<br>    cache = (cache_affine, cache_relu, cache_dropout)<br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                                                        结束编码                                                                         #</span><br>    <span class="hljs-comment">###########################################################################        </span><br>    <span class="hljs-keyword">return</span> out_dropout,cache<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_dropout_backward</span>(<span class="hljs-params">dout,cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     affine_relu_dropout神经元的反向传播过程。</span><br><span class="hljs-string">     </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 形状为(N, M)的上层梯度。</span><br><span class="hljs-string">    - cache: 缓存(cache_affine,cache_relu,cache_dropout)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度，其形状为(N, d1, ..., d_k)</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度，其形状为(D,M)</span><br><span class="hljs-string">    - db: 偏置项b的梯度，其形状为(M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span>    <br>    cache_affine,cache_relu,cache_dropout = cache<br>    dx,dw,db=<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span><br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                             任务：实现affine_relu_dropout反向传播                                         #</span><br>    <span class="hljs-comment">###########################################################################    </span><br>    ddropout = dropout_backward(dout, cache_dropout)<br>    drelu = relu_backward(ddropout, cache_relu)<br>    dx, dw, db = affine_backward(drelu, cache_affine)     <br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-comment">#                                                    结束编码                                                                            #</span><br>    <span class="hljs-comment">###########################################################################</span><br>    <span class="hljs-keyword">return</span> dx,dw,db<br>    <br></code></pre></td></tr></table></figure><h2 id="layerspy"><a class="markdownIt-Anchor" href="#layerspy"></a> <a target="_blank" rel="noopener" href="http://layers.py">layers.py</a></h2><p>之前已经写好的前向传播与后向传播代码以及softmax的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算神经网络当前层的前馈传播，该方法计算在全连接情况下的得分函数。</span><br><span class="hljs-string">    注：如果不理解affine仿射变换，简单的理解为在全连接情况下的得分函数即可。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    输入数据x的形状为(N, d_1, ..., d_k)，其中N表示数据量，(d_1, ..., d_k)表示</span><br><span class="hljs-string">    每一通道的数据维度，如果是图片数据就为(长，宽，色道)。数据的总维度为</span><br><span class="hljs-string">    D = d_1 * ... * d_k，因此我们需要数据整合成完整的（N,D)形式再进行仿射变换。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据，其形状为(N, d_1, ..., d_k)的numpy数组。</span><br><span class="hljs-string">    - w: 权重矩阵，其形状为(D,M)的numpy数组，D表示输入数据维度，M表示输出数据维度</span><br><span class="hljs-string">             可以将D看成输入的神经元个数，M看成输出神经元个数。</span><br><span class="hljs-string">    - b: 偏置向量，其形状为(M,)的numpy数组。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 形状为(N, M)的输出结果。</span><br><span class="hljs-string">    - cache: 将输入进行缓存(x, w, b)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                            任务: 实现全连接前向传播                                                         #</span><br>    <span class="hljs-comment">#                                     注：首先你需要将输入数据重塑成行。                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    N=x.shape[<span class="hljs-number">0</span>]<br>    x_new=x.reshape(N,-<span class="hljs-number">1</span>)<span class="hljs-comment">#将x重塑成2维向量</span><br>    out=np.dot(x_new,w)+b<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = (x, w, b)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"> 计算仿射层的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 形状为(N, M)的上层梯度</span><br><span class="hljs-string">    - cache: 元组:</span><br><span class="hljs-string">        - x: (N, d_1, ... d_k)的输入数据</span><br><span class="hljs-string">        - w: 形状为(D, M)的权重矩阵</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度，其形状为(N, d1, ..., d_k)</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度，其形状为(D,M)</span><br><span class="hljs-string">    - db: 偏置项b的梯度，其形状为(M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b = cache<br>    dx, dw, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                    任务: 实现仿射层反向传播                                                                 #</span><br>    <span class="hljs-comment">#                 注意：你需要将x重塑成(N,D)后才能计算各梯度，                                            #</span><br>    <span class="hljs-comment">#                            求完梯度后你需要将dx的形状与x重塑成一样                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    db = np.<span class="hljs-built_in">sum</span>(dout,axis=<span class="hljs-number">0</span>)<br>    xx= x.reshape(x.shape[<span class="hljs-number">0</span>],-<span class="hljs-number">1</span>)<br>    dw = np.dot(xx.T,dout)<br>    dx = np.dot(dout,w.T)<br>    dx=np.reshape(dx,x.shape)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算rectified linear units (ReLUs)激活函数的前向传播，并保存相应缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 和输入数据x形状相同</span><br><span class="hljs-string">    - cache: x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                         任务: 实现ReLU 的前向传播.                                                                        #</span><br>    <span class="hljs-comment">#                        注意：你只需要1行代码即可完成                                                                    #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    out =np.maximum(<span class="hljs-number">0</span>,x)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = x<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 rectified linear units (ReLUs)激活函数的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: 输入 x,其形状应该和dout相同</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: x的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, x = <span class="hljs-literal">None</span>, cache<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU 反向传播.                                                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dx=dout<br>    dx[x&lt;=<span class="hljs-number">0</span>]=<span class="hljs-number">0</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                        结束编码                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元前向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入到 affine层的数据</span><br><span class="hljs-string">    - w, b:    affine层的权重矩阵和偏置向量</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out:    ReLU的输出结果</span><br><span class="hljs-string">    - cache: 前向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">######################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU神经元前向传播.                                             #</span><br>    <span class="hljs-comment">#                注意：你需要调用affine_forward以及relu_forward函数，                #</span><br>    <span class="hljs-comment">#                            并将各自的缓存保存在cache中                                                     #</span><br>    <span class="hljs-comment">######################################################################</span><br>    a, fc_cache = affine_forward(x, w, b)<br>    out, relu_cache = relu_forward(a)<br>    cache = (fc_cache, relu_cache)<br>    <span class="hljs-comment">######################################################################</span><br>    <span class="hljs-comment">#                                         结束编码                                                                             #</span><br>    <span class="hljs-comment">######################################################################</span><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元的反向传播</span><br><span class="hljs-string">     </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: affine缓存，以及relu缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度</span><br><span class="hljs-string">    - db: 偏置向量b的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU神经元反向传播.                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    fc_cache, relu_cache = cache<br>    da = relu_backward(dout, relu_cache)<br>    dx, dw, db = affine_backward(da, fc_cache)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                     结束编码                                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss</span>(<span class="hljs-params">x, y</span>):<br>    probs = np.exp(x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    probs /= np.<span class="hljs-built_in">sum</span>(probs, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    N = x.shape[<span class="hljs-number">0</span>]<br>    loss = -np.<span class="hljs-built_in">sum</span>(np.log(probs[np.arange(N), y])) / N<br>    dx = probs.copy()<br>    dx[np.arange(N), y] -= <span class="hljs-number">1</span><br>    dx /= N<br><br>    <span class="hljs-keyword">return</span> loss, dx<br><br></code></pre></td></tr></table></figure><h2 id="fc_netpy"><a class="markdownIt-Anchor" href="#fc_netpy"></a> fc_net.py</h2><p>实现了深层全连接神经网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> layers <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> dropout_layers <span class="hljs-keyword">import</span> *<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyConnectedNet</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    深层全连接神经网络，其中隐藏层使用ReLU作为激活函数，输出层使用softmax作为分类器.</span><br><span class="hljs-string">    该网络结构应该为:&#123;affine - relu- [dropout]&#125;x(L - 1) - affine - softmax</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim=<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>,hidden_dims=[<span class="hljs-number">100</span>,<span class="hljs-number">100</span>],    num_classes=<span class="hljs-number">10</span>,</span><br><span class="hljs-params">                             dropout=<span class="hljs-number">0</span>, reg=<span class="hljs-number">0.0</span>, weight_scale=<span class="hljs-number">1e-2</span>, seed=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化全连接网络.</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - input_dim: 输入维度</span><br><span class="hljs-string">        - hidden_dims:    隐藏层各层维度，如[100,100]</span><br><span class="hljs-string">        - num_classes: 分类数量.</span><br><span class="hljs-string">        - dropout: 如果dropout = 0 表示不使用dropout.</span><br><span class="hljs-string">        - reg:正则化衰减因子.</span><br><span class="hljs-string">        - weight_scale:权重范围，给予初始化权重的标准差.</span><br><span class="hljs-string">        - seed: 使用seed产生相同的随机数.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.use_dropout = dropout &gt; <span class="hljs-number">0</span><br>        self.reg = reg<br>        self.num_layers = <span class="hljs-number">1</span> + <span class="hljs-built_in">len</span>(hidden_dims)<br>        self.params = &#123;&#125;<br><br>        layers_dims = [input_dim] + hidden_dims + [num_classes]<br>        <span class="hljs-comment"># 初始化每一层的参数</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>                self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]=weight_scale*np.random.randn(<br>                        layers_dims[i],layers_dims[i+<span class="hljs-number">1</span>])<br>                self.params[<span class="hljs-string">&#x27;b&#x27;</span> + <span class="hljs-built_in">str</span>(i + <span class="hljs-number">1</span>)] = np.zeros(<br>                        (<span class="hljs-number">1</span>, layers_dims[i + <span class="hljs-number">1</span>]))<br>        <br>        <span class="hljs-comment"># 初始化与dropout有关的参数</span><br>        self.dropout_param = &#123;&#125;<br>        <span class="hljs-keyword">if</span> self.use_dropout:<br>            self.dropout_param = &#123;<span class="hljs-string">&#x27;mode&#x27;</span>: <span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>: dropout&#125;<br>            <span class="hljs-keyword">if</span> seed <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                self.dropout_param[<span class="hljs-string">&#x27;seed&#x27;</span>] = seed<br>        <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, X, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        计算损失函数，有两种模式Dropout传播与无Dropout传播</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-comment"># 首先判断执行模式</span><br>        mode = <span class="hljs-string">&#x27;test&#x27;</span> <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;train&#x27;</span><br>        <span class="hljs-comment"># 设置执行模式</span><br>        <span class="hljs-keyword">if</span> self.dropout_param <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.dropout_param[<span class="hljs-string">&#x27;mode&#x27;</span>] = mode     <br>        <br>        scores = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                    任务：执行全连接网络的前馈过程。                                                #</span><br>        <span class="hljs-comment">#                             计算数据的分类得分，将结果保存在scores中。                                 #</span><br>        <span class="hljs-comment">#            当使用dropout时，你需要使用self.dropout_param进行dropout前馈。            #</span><br>        <span class="hljs-comment">#                     例如 if self.use_dropout: dropout传播     else：正常传播                 #</span><br>        <span class="hljs-comment">############################################################################</span><br>        outs, cache = &#123;&#125;, &#123;&#125;<br>        outs[<span class="hljs-number">0</span>] = X<br>        num_h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-comment"># 隐藏层</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_h):<br>            <span class="hljs-keyword">if</span> self.use_dropout:<br>                outs[i+<span class="hljs-number">1</span>], cache[i+<span class="hljs-number">1</span>] = affine_relu_dropout_forward(<br>                    outs[i], self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)],<br>                    self.dropout_param)<br>            <span class="hljs-keyword">else</span>:<br>                outs[i+<span class="hljs-number">1</span>], cache[i+<span class="hljs-number">1</span>] = affine_relu_forward(<br>                    outs[i], self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)])<br>        <span class="hljs-comment"># 输出层</span><br>        scores, cache[num_h+<span class="hljs-number">1</span>] = affine_forward(outs[num_h], <br>                    self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)])<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>            <span class="hljs-keyword">return</span> scores<br><br>        loss, grads = <span class="hljs-number">0.0</span>, &#123;&#125;<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#            任务：实现全连接网络的反向传播。                                                                        #</span><br>        <span class="hljs-comment">#                将损失值储存在loss中，梯度值储存在grads字典中                                         #</span><br>        <span class="hljs-comment">#         注意网络需要设置两种模式：有dropout，无dropout                                             #</span><br>        <span class="hljs-comment">#             例如if self.use_dropout: dropout传播，else：正常传播                             #</span><br>        <span class="hljs-comment">############################################################################</span><br>        dout = &#123;&#125;<br>        loss, dy = softmax_loss(scores, y)<br>        h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>            loss += <span class="hljs-number">0.5</span>*self.reg*(np.<span class="hljs-built_in">sum</span>(self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]<br>                    * self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]))<br>        dout[h], grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)], grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)] = affine_backward(dy, <br>                    cache[h+<span class="hljs-number">1</span>])<br>        grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)]	+= self.reg * self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(h):<br>            <span class="hljs-keyword">if</span> self.use_dropout:<br>                dout[h-<span class="hljs-number">1</span>-i], grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)], grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] =  affine_relu_dropout_backward(dout[h-i], cache[h-i])<br>            <span class="hljs-keyword">else</span>:<br>                dout[h-<span class="hljs-number">1</span>-i], grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)], grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = affine_relu_backward(dout[h-i], cache[h-i])<br>            grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)]+=self.reg*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)]<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                                                         结束编码                                                                         #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">return</span> loss, grads<br><br></code></pre></td></tr></table></figure><h2 id="trainerpy"><a class="markdownIt-Anchor" href="#trainerpy"></a> <a target="_blank" rel="noopener" href="http://trainer.py">trainer.py</a></h2><p>解耦训练器的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.path.realpath(os.path.dirname(os.path.realpath(__file__))))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> updater<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用形式:</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    data = &#123;</span><br><span class="hljs-string">        &#x27;X_train&#x27;: # 训练数据</span><br><span class="hljs-string">        &#x27;y_train&#x27;: # 训练类标</span><br><span class="hljs-string">        &#x27;X_val&#x27;: # 验证数据</span><br><span class="hljs-string">        &#x27;X_train&#x27;: # 验证类标</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string">    model = MyAwesomeModel(hidden_size=100, reg=10)</span><br><span class="hljs-string">    Trainer = Trainer(model, data,</span><br><span class="hljs-string">                                    update_rule=&#x27;sgd&#x27;,</span><br><span class="hljs-string">                                    updater_config=&#123;</span><br><span class="hljs-string">                                        &#x27;learning_rate&#x27;: 1e-3,</span><br><span class="hljs-string">                                    &#125;,</span><br><span class="hljs-string">                                    lr_decay=0.95,</span><br><span class="hljs-string">                                    num_epochs=10, batch_size=100,</span><br><span class="hljs-string">                                    print_every=100)</span><br><span class="hljs-string">    Trainer.train()</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, data, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化训练器各项配置。</span><br><span class="hljs-string">        必须参数:</span><br><span class="hljs-string">        - model: 神经网络模型，如：DNN,CNN,RNN</span><br><span class="hljs-string">        - data: 数据字典，其中:</span><br><span class="hljs-string">            &#x27;X_train&#x27;:    形状为(N_train, d_1, ..., d_k)的训练数据</span><br><span class="hljs-string">            &#x27;X_val&#x27;:    形状为(N_val, d_1, ..., d_k)的验证数据</span><br><span class="hljs-string">            &#x27;y_train&#x27;:    形状为(N_train,)的训练数据类标</span><br><span class="hljs-string">            &#x27;y_val&#x27;:    形状为(N_val,)的验证数据类标</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        可选参数:</span><br><span class="hljs-string">        - update_rule: 更新规则，其存放在updater.py文件中，默认选项为&#x27;sgd&#x27;。</span><br><span class="hljs-string">        - updater_config:更新规则所对应的超参数配置，同见updater.py文件。</span><br><span class="hljs-string">        - lr_decay: 学习率衰减系数。</span><br><span class="hljs-string">        - batch_size: 批量数据大小。</span><br><span class="hljs-string">        - num_epochs: 训练周期。</span><br><span class="hljs-string">        - print_every: 整数型，迭代训练print_every次模型，打印一次中间结果。</span><br><span class="hljs-string">        - verbose: 布尔型; 是否在训练期间打印中间结果</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.model = model<br>        self.X_train = data[ <span class="hljs-string">&#x27;X_train&#x27;</span>]<br>        self.y_train = data[ <span class="hljs-string">&#x27;y_train&#x27;</span>]<br>        self.X_val = data[ <span class="hljs-string">&#x27;X_val&#x27;</span>]<br>        self.y_val = data[ <span class="hljs-string">&#x27;y_val&#x27;</span>]<br>        <br>        <span class="hljs-comment"># 弹出可选参数，进行相关配置。</span><br>        self.update_rule = kwargs.pop(<span class="hljs-string">&#x27;update_rule&#x27;</span>, <span class="hljs-string">&#x27;sgd&#x27;</span>)<br>        self.updater_config = kwargs.pop(<span class="hljs-string">&#x27;updater_config&#x27;</span>, &#123;&#125;)<br>        <span class="hljs-comment"># 默认不采用学习衰减</span><br>        self.lr_decay = kwargs.pop(<span class="hljs-string">&#x27;lr_decay&#x27;</span>, <span class="hljs-number">1.0</span>)<br>        self.batch_size = kwargs.pop(<span class="hljs-string">&#x27;batch_size&#x27;</span>, <span class="hljs-number">100</span>)<br>        self.num_epochs = kwargs.pop(<span class="hljs-string">&#x27;num_epochs&#x27;</span>, <span class="hljs-number">10</span>)<br><br>        self.print_every = kwargs.pop(<span class="hljs-string">&#x27;print_every&#x27;</span>, <span class="hljs-number">10</span>)<br>        self.verbose = kwargs.pop(<span class="hljs-string">&#x27;verbose&#x27;</span>, <span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 若可选参数错误，抛出异常</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(kwargs) &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 获取不符合规定的key</span><br>            extra = <span class="hljs-string">&#x27;, &#x27;</span>.join(<span class="hljs-string">&#x27;&quot;%s&quot;&#x27;</span> % k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> kwargs.keys())<br>            <span class="hljs-comment"># 报错</span><br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Unrecognized arguments %s&#x27;</span> % extra)<br><br><br>        <span class="hljs-comment">#确认updater中含有更新规则</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(updater, self.update_rule):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Invalid update_rule &quot;%s&quot;&#x27;</span> % self.update_rule)<br>        self.update_rule = <span class="hljs-built_in">getattr</span>(updater, self.update_rule)<br><br>        <span class="hljs-comment"># 初始化相关变量</span><br>        self.epoch = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 最佳的验证准确率</span><br>        self.best_val_acc = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 最佳参数</span><br>        self.best_params = &#123;&#125;<br>        <span class="hljs-comment"># 训练过程中的中间数据</span><br>        self.loss_history = []<br>        self.train_acc_history = []<br>        self.val_acc_history = []<br><br>        <span class="hljs-comment"># 对updater_config中的参数进行深拷贝</span><br>        self.updater_configs = &#123;&#125;<br>        <span class="hljs-comment"># 获取各层的权重</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.model.params:<br>            d = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.updater_config.items()&#125;<br>            self.updater_configs[p] = d<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        执行单步梯度更新</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 采样批量数据</span><br>        num_train = self.X_train.shape[<span class="hljs-number">0</span>]<br>        batch_mask = np.random.choice(num_train, self.batch_size)<br>        X_batch = self.X_train[batch_mask]<br>        y_batch = self.y_train[batch_mask]<br><br>        <span class="hljs-comment"># 计算损失及梯度</span><br>        loss, grads = self.model.loss(X_batch, y_batch)<br>        self.loss_history.append(loss)<br><br>        <span class="hljs-comment"># 更新参数</span><br>        <span class="hljs-keyword">for</span> p, w <span class="hljs-keyword">in</span> self.model.params.items():<br>            dw = grads[p]<br>            config = self.updater_configs[p]<br>            <span class="hljs-comment"># 计算出更新后的值</span><br>            next_w, next_config = self.update_rule(w, dw, config)<br>            <span class="hljs-comment"># 参数更新</span><br>            self.model.params[p] = next_w<br>            self.updater_configs[p] = next_config<br><br>    <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_accuracy</span>(<span class="hljs-params">self, X, y, num_samples=<span class="hljs-literal">None</span>, batch_size=<span class="hljs-number">100</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        根据提供的数据检验精度，若数据集过大，可进行采样测试。</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 形状为(N, d_1, ..., d_k)的数据</span><br><span class="hljs-string">        - y: 形状为 (N,)的数据类标</span><br><span class="hljs-string">        - num_samples: 采样次数</span><br><span class="hljs-string">        - batch_size:批量数据大小</span><br><span class="hljs-string">            </span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - acc: 测试数据正确率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        <span class="hljs-comment"># 对数据进行采样</span><br>        N = X.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment"># 采样次数为空或者训练数据大于采样数据</span><br>        <span class="hljs-keyword">if</span> num_samples <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> N &gt; num_samples:<br>            <span class="hljs-comment"># 随机选择训练数据</span><br>            mask = np.random.choice(N, num_samples)<br>            N = num_samples<br>            X = X[mask]<br>            y = y[mask]<br><br>        <span class="hljs-comment"># 计算精度</span><br>        num_batches = N / batch_size<br>        <span class="hljs-keyword">if</span> N % batch_size != <span class="hljs-number">0</span>:<br>            num_batches += <span class="hljs-number">1</span><br>        y_pred = []<br>        <span class="hljs-comment"># 预测所有的数据</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_batches)):<br>            start = i * batch_size<br>            end = (i + <span class="hljs-number">1</span>) * batch_size<br>            scores = self.model.loss(X[start:end])<br>            y_pred.append(np.argmax(scores, axis=<span class="hljs-number">1</span>))<br>        <span class="hljs-comment"># 水平方向进行叠加</span><br>        y_pred = np.hstack(y_pred)<br>        <span class="hljs-comment"># 计算准确率</span><br>        acc = np.mean(y_pred == y)<br>        <span class="hljs-keyword">return</span> acc<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        根据配置训练模型</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train = self.X_train.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment">#训练集大小</span><br>        <span class="hljs-comment"># 每个周期的迭代次数</span><br>        iterations_per_epoch = <span class="hljs-built_in">max</span>(num_train / self.batch_size, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 总的迭代数</span><br>        num_iterations = self.num_epochs * iterations_per_epoch<br><br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(num_iterations)):<br>            self._step()<br><br>            <span class="hljs-comment"># 打印损失值</span><br>            <span class="hljs-keyword">if</span> self.verbose <span class="hljs-keyword">and</span> t % self.print_every == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(迭代 %d / %d) 损失值: %f&#x27;</span> % (t + <span class="hljs-number">1</span>, num_iterations, self.loss_history[-<span class="hljs-number">1</span>]))<br><br>            <span class="hljs-comment"># 更新学习率</span><br>            epoch_end = (t + <span class="hljs-number">1</span>) % iterations_per_epoch == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> epoch_end:<br>                self.epoch += <span class="hljs-number">1</span><br>                <span class="hljs-comment"># 学习率衰减</span><br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> self.updater_configs:<br>                    self.updater_configs[k][<span class="hljs-string">&#x27;learning_rate&#x27;</span>] *= self.lr_decay<br><br><br>            <span class="hljs-comment">#在训练的开始，末尾，每一轮训练周期检验精度</span><br>            first_it = (t == <span class="hljs-number">0</span>)<br>            last_it = (t == num_iterations + <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> first_it <span class="hljs-keyword">or</span> last_it <span class="hljs-keyword">or</span> epoch_end:<br>                train_acc = self.check_accuracy(self.X_train, self.y_train,<br>                                                                                num_samples=<span class="hljs-number">1000</span>)<br>                val_acc = self.check_accuracy(self.X_val, self.y_val)<br>                self.train_acc_history.append(train_acc)<br>                self.val_acc_history.append(val_acc)<br><br>                <span class="hljs-keyword">if</span> self.verbose:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;(周期 %d / %d) 训练精度: %f; 验证精度: %f&#x27;</span> % (<br>                                 self.epoch, self.num_epochs, train_acc, val_acc))<br><br>                <span class="hljs-comment"># 记录最佳模型</span><br>                <span class="hljs-keyword">if</span> val_acc &gt; self.best_val_acc:<br>                    self.best_val_acc = val_acc<br>                    self.best_params = &#123;&#125;<br>                    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.model.params.items():<br>                        self.best_params[k] = v.copy()<br><br>        <span class="hljs-comment"># 训练结束后返回最佳模型</span><br>        self.model.params = self.best_params<br><br><br></code></pre></td></tr></table></figure><h2 id="updaterpy"><a class="markdownIt-Anchor" href="#updaterpy"></a> <a target="_blank" rel="noopener" href="http://updater.py">updater.py</a></h2><p>解耦更新器，主要负责更新神经网络的权重，其传入参数有神经网络的权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span>、当前权重的梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">dw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span>及相应的更新配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">频繁使用的神经网络一阶梯度更新规则。每次更新接收：当前的网络权重，</span><br><span class="hljs-string">训练获得的梯度，以及相关配置进行权重更新。</span><br><span class="hljs-string">def update(w, dw, config = None):</span><br><span class="hljs-string">Inputs:</span><br><span class="hljs-string">  - w:当前权重.</span><br><span class="hljs-string">  - dw: 和权重形状相同的梯度.</span><br><span class="hljs-string">  - config: 字典型超参数配置，比如学习率，动量值等。如果更新规则需要用到缓存，</span><br><span class="hljs-string">    在配置中需要保存相应的缓存。</span><br><span class="hljs-string">Returns:</span><br><span class="hljs-string">  - next_w: 更新后的权重.</span><br><span class="hljs-string">  - config: 更新规则相应的配置.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">w, dw, config=<span class="hljs-literal">None</span></span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  随机梯度下降更新规则.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  config :</span><br><span class="hljs-string">  - learning_rate: 学习率.</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: <br>      config = &#123;&#125;<br>  config.setdefault(<span class="hljs-string">&#x27;learning_rate&#x27;</span>, <span class="hljs-number">1e-2</span>)<br><br>  w -= config[<span class="hljs-string">&#x27;learning_rate&#x27;</span>] * dw<br>  <span class="hljs-keyword">return</span> w, config<br><br><br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>编码实现Dropout正则化编码</div><div>https://fulequn.github.io/2020/11/Article202011305/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2020年11月30日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2020/12/Article202012031/" title="深度学习实战 第5章深度学习优化笔记"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">深度学习实战 第5章深度学习优化笔记</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/11/Article202011304/" title="深度学习实战 第4章深度学习正则化笔记"><span class="hidden-mobile">深度学习实战 第4章深度学习正则化笔记</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>