<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="参考文献： hbase-site.xml 配置详解_ningxuezhu的专栏-CSDN博客  该文档是用hbase默认配置文件生成的，文件源是 hbase-default.xml  hbase.rootdir 这个目录是region server的共享目录，用来持久化HBase。URL需要是完全正确的，还要包含文件系统的scheme。例如，要表示hdfs中的’&#x2F;hbase’目录，namenod"><meta property="og:type" content="article"><meta property="og:title" content="HBase的hbase-site.xml文件配置详解"><meta property="og:url" content="https://fulequn.github.io/2020/11/Article202011094/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="参考文献： hbase-site.xml 配置详解_ningxuezhu的专栏-CSDN博客  该文档是用hbase默认配置文件生成的，文件源是 hbase-default.xml  hbase.rootdir 这个目录是region server的共享目录，用来持久化HBase。URL需要是完全正确的，还要包含文件系统的scheme。例如，要表示hdfs中的’&#x2F;hbase’目录，namenod"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-11-09T15:35:37.000Z"><meta property="article:modified_time" content="2024-05-30T00:16:32.000Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="Hadoop"><meta property="article:tag" content="HBase"><meta name="twitter:card" content="summary_large_image"><title>HBase的hbase-site.xml文件配置详解 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:60,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="HBase的hbase-site.xml文件配置详解"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-11-09 23:35" pubdate>2020年11月9日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>4.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>39 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">HBase的hbase-site.xml文件配置详解</h1><div class="markdown-body"><blockquote><p>参考文献：</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ningxuezhu/article/details/50547970">hbase-site.xml 配置详解_ningxuezhu的专栏-CSDN博客</a></p></blockquote><p>该文档是用hbase默认配置文件生成的，文件源是 <code>hbase-default.xml</code></p><h4 id="hbaserootdir"><a class="markdownIt-Anchor" href="#hbaserootdir"></a> hbase.rootdir</h4><p>这个目录是region server的共享目录，用来持久化HBase。URL需要是<strong>完全正确</strong>的，还要包含文件系统的scheme。例如，要表示hdfs中的’/hbase’目录，namenode 运行在namenode.example.org的9090端口。则需要设置为hdfs://namenode.example.org:9000/hbase。默认情况下HBase是写到/tmp的。不改这个配置，数据会在重启的时候丢失。</p><p>默认: <code>file:///tmp/hbase-$&#123;user.name&#125;/hbase</code></p><h4 id="hbasemasterport"><a class="markdownIt-Anchor" href="#hbasemasterport"></a> hbase.master.port</h4><p>HBase的Master的端口.</p><p>默认: <code>60000</code></p><h4 id="hbaseclusterdistributed"><a class="markdownIt-Anchor" href="#hbaseclusterdistributed"></a> hbase.cluster.distributed</h4><p>HBase的运行模式。false是单机模式，true是分布式模式。若为false,HBase和Zookeeper会运行在同一个JVM里面。</p><p>默认: <code>false</code></p><h4 id="hbasetmpdir"><a class="markdownIt-Anchor" href="#hbasetmpdir"></a> hbase.tmp.dir</h4><p>本地文件系统的临时文件夹。可以修改到一个更为持久的目录上。(/tmp会在重启时清楚)</p><p>默认:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>j</mi><mi>a</mi><mi>v</mi><mi>a</mi><mi mathvariant="normal">.</mi><mi>i</mi><mi>o</mi><mi mathvariant="normal">.</mi><mi>t</mi><mi>m</mi><mi>p</mi><mi>d</mi><mi>i</mi><mi>r</mi></mrow><mi mathvariant="normal">/</mi><mi>h</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mo>−</mo></mrow><annotation encoding="application/x-tex">{java.io.tmpdir}/hbase-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="mord mathnormal">a</span><span class="mord">.</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord">.</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">p</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span><span class="mord">/</span><span class="mord mathnormal">h</span><span class="mord mathnormal">b</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord">−</span></span></span></span>{<a target="_blank" rel="noopener" href="http://user.name">user.name</a>}</p><h4 id="hbaselocaldir"><a class="markdownIt-Anchor" href="#hbaselocaldir"></a> hbase.local.dir</h4><p>作为本地存储，位于本地文件系统的路径。</p><p>默认: ${hbase.tmp.dir}/local/</p><h4 id="hbasemasterinfoport"><a class="markdownIt-Anchor" href="#hbasemasterinfoport"></a> hbase.master.info.port</h4><p>HBase Master web 界面端口. 设置为-1 意味着你不想让他运行。</p><p>0.98 版本以后默认: 16010 以前是 60010</p><h4 id="hbasemasterinfobindaddress"><a class="markdownIt-Anchor" href="#hbasemasterinfobindaddress"></a> hbase.master.info.bindAddress</h4><p>HBase Master web 界面绑定的端口</p><p>默认: <code>0.0.0.0</code></p><h4 id="hbaseclientwritebuffer"><a class="markdownIt-Anchor" href="#hbaseclientwritebuffer"></a> hbase.client.write.buffer</h4><p>HTable客户端的写缓冲的默认大小。这个值越大，需要消耗的内存越大。因为缓冲在客户端和服务端都有实例，所以需要消耗客户端和服务端两个地方的内存。得到的好处是，可以减少RPC的次数。可以这样估算服务器端被占用的内存： hbase.client.write.buffer * hbase.regionserver.handler.count</p><p>默认: <code>2097152</code></p><h4 id="hbaseregionserverport"><a class="markdownIt-Anchor" href="#hbaseregionserverport"></a> hbase.regionserver.port</h4><p>HBase RegionServer绑定的端口</p><p>0.98 以前默认: <code>60020</code>以后默认是：16020</p><h4 id="hbaseregionserverinfoport"><a class="markdownIt-Anchor" href="#hbaseregionserverinfoport"></a> hbase.regionserver.info.port</h4><p>HBase RegionServer web 界面绑定的端口 设置为 -1 意味这你不想与运行 RegionServer 界面.</p><p>0.98 以前默认: <code>60030 以后默认是：16030</code></p><h4 id="hbaseregionserverinfoportauto"><a class="markdownIt-Anchor" href="#hbaseregionserverinfoportauto"></a> hbase.regionserver.info.port.auto</h4><p>Master或RegionServer是否要动态搜一个可以用的端口来绑定界面。当hbase.regionserver.info.port已经被占用的时候，可以搜一个空闲的端口绑定。这个功能在测试的时候很有用。默认关闭。</p><p>默认: <code>false</code></p><h4 id="hbaseregionserverinfobindaddress"><a class="markdownIt-Anchor" href="#hbaseregionserverinfobindaddress"></a> hbase.regionserver.info.bindAddress</h4><p>HBase RegionServer web 界面的IP地址</p><p>默认: <code>0.0.0.0</code></p><h4 id="hbaseregionserverclass"><a class="markdownIt-Anchor" href="#hbaseregionserverclass"></a> hbase.regionserver.class</h4><p>RegionServer 使用的接口。客户端打开代理来连接region server的时候会使用到。</p><p>默认: <code>org.apache.hadoop.hbase.ipc.HRegionInterface</code></p><h4 id="hbaseclientpause"><a class="markdownIt-Anchor" href="#hbaseclientpause"></a> hbase.client.pause</h4><p>通常的客户端暂停时间。最多的用法是客户端在重试前的等待时间。比如失败的get操作和region查询操作等都很可能用到。</p><p>默认: <code>1000</code></p><h4 id="hbaseclientretriesnumber"><a class="markdownIt-Anchor" href="#hbaseclientretriesnumber"></a> hbase.client.retries.number</h4><p>最大重试次数。所有需重试操作的最大值。例如从root region服务器获取root region，Get单元值，行Update操作等等。这是最大重试错误的值。 Default: 10.</p><p>0.98 以前默认: <code>10 以后默认是：35</code></p><h4 id="hbasebulkloadretriesnumber"><a class="markdownIt-Anchor" href="#hbasebulkloadretriesnumber"></a> hbase.bulkload.retries.number</h4><p>最大重试次数。 原子批加载尝试的迭代最大次数。 0 永不放弃。默认: 0.</p><p>默认: 0</p><h4 id="hbaseclientscannercaching"><a class="markdownIt-Anchor" href="#hbaseclientscannercaching"></a> hbase.client.scanner.caching</h4><p>当调用Scanner的next方法，而值又不在缓存里的时候，从服务端一次获取的行数。越大的值意味着Scanner会快一些，但是会占用更多的内存。当缓冲被占满的时候，next方法调用会越来越慢。慢到一定程度，可能会导致超时。例如超过了hbase.regionserver.lease.period。</p><p>默认: 100</p><h4 id="hbaseclientkeyvaluemaxsize"><a class="markdownIt-Anchor" href="#hbaseclientkeyvaluemaxsize"></a> hbase.client.keyvalue.maxsize</h4><p>一个KeyValue实例的最大size.这个是用来设置存储文件中的单个entry的大小上界。因为一个KeyValue是不能分割的，所以可以避免因为数据过大导致region不可分割。明智的做法是把它设为可以被最大region size整除的数。如果设置为0或者更小，就会禁用这个检查。默认10MB。</p><p>默认: <code>10485760</code></p><h4 id="hbaseregionserverleaseperiod"><a class="markdownIt-Anchor" href="#hbaseregionserverleaseperiod"></a> hbase.regionserver.lease.period</h4><p>客户端租用HRegion server 期限，即超时阀值。单位是毫秒。默认情况下，客户端必须在这个时间内发一条信息，否则视为死掉。</p><p>默认: <code>60000</code></p><h4 id="hbaseregionserverhandlercount"><a class="markdownIt-Anchor" href="#hbaseregionserverhandlercount"></a> hbase.regionserver.handler.count</h4><p>RegionServers受理的RPC Server实例数量。对于Master来说，这个属性是Master受理的handler数量</p><p>默认: <code>10</code></p><h4 id="hbaseregionservermsginterval"><a class="markdownIt-Anchor" href="#hbaseregionservermsginterval"></a> hbase.regionserver.msginterval</h4><p>RegionServer 发消息给 Master 时间间隔，单位是毫秒</p><p>默认: <code>3000</code></p><h4 id="hbaseregionserveroptionallogflushinterval"><a class="markdownIt-Anchor" href="#hbaseregionserveroptionallogflushinterval"></a> hbase.regionserver.optionallogflushinterval</h4><p>将Hlog同步到HDFS的间隔。如果Hlog没有积累到一定的数量，到了时间，也会触发同步。默认是1秒，单位毫秒。</p><p>默认: <code>1000</code></p><h4 id="hbaseregionserverregionsplitlimit"><a class="markdownIt-Anchor" href="#hbaseregionserverregionsplitlimit"></a> hbase.regionserver.regionSplitLimit</h4><p>region的数量到了这个值后就不会在分裂了。这不是一个region数量的硬性限制。但是起到了一定指导性的作用，到了这个值就该停止分裂了。默认是MAX_INT.就是说不阻止分裂。</p><p>默认: <code>2147483647</code></p><h4 id="hbaseregionserverlogrollperiod"><a class="markdownIt-Anchor" href="#hbaseregionserverlogrollperiod"></a> hbase.regionserver.logroll.period</h4><p>提交commit log的间隔，不管有没有写足够的值。</p><p>默认: <code>3600000</code></p><h4 id="hbaseregionserverhlogreaderimpl"><a class="markdownIt-Anchor" href="#hbaseregionserverhlogreaderimpl"></a> hbase.regionserver.hlog.reader.impl</h4><p>HLog file reader 的实现.</p><p>默认: <code>org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader</code></p><h4 id="hbaseregionserverhlogwriterimpl"><a class="markdownIt-Anchor" href="#hbaseregionserverhlogwriterimpl"></a> hbase.regionserver.hlog.writer.impl</h4><p>HLog file writer 的实现.</p><p>默认: <code>org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter</code></p><h4 id="hbaseregionservernbreservationblocks"><a class="markdownIt-Anchor" href="#hbaseregionservernbreservationblocks"></a> hbase.regionserver.nbreservationblocks</h4><p>储备的内存block的数量(译者注:就像石油储备一样)。当发生out of memory 异常的时候，我们可以用这些内存在RegionServer停止之前做清理操作。</p><p>默认: <code>4</code></p><h4 id="hbasezookeeperdnsinterface"><a class="markdownIt-Anchor" href="#hbasezookeeperdnsinterface"></a> hbase.zookeeper.dns.interface</h4><p>当使用DNS的时候，Zookeeper用来上报的IP地址的网络接口名字。</p><p>默认: <code>default</code></p><h4 id="hbasezookeeperdnsnameserver"><a class="markdownIt-Anchor" href="#hbasezookeeperdnsnameserver"></a> hbase.zookeeper.dns.nameserver</h4><p>当使用DNS的时候，Zookeepr使用的DNS的域名或者IP 地址，Zookeeper用它来确定和master用来进行通讯的域名.</p><p>默认: <code>default</code></p><h4 id="hbaseregionserverdnsinterface"><a class="markdownIt-Anchor" href="#hbaseregionserverdnsinterface"></a> hbase.regionserver.dns.interface</h4><p>当使用DNS的时候，RegionServer用来上报的IP地址的网络接口名字。</p><p>默认: <code>default</code></p><h4 id="hbaseregionserverdnsnameserver"><a class="markdownIt-Anchor" href="#hbaseregionserverdnsnameserver"></a> hbase.regionserver.dns.nameserver</h4><p>当使用DNS的时候，RegionServer使用的DNS的域名或者IP 地址，RegionServer用它来确定和master用来进行通讯的域名.</p><p>默认: <code>default</code></p><h4 id="hbasemasterdnsinterface"><a class="markdownIt-Anchor" href="#hbasemasterdnsinterface"></a> hbase.master.dns.interface</h4><p>当使用DNS的时候，Master用来上报的IP地址的网络接口名字。</p><p>默认: <code>default</code></p><h4 id="hbasemasterdnsnameserver"><a class="markdownIt-Anchor" href="#hbasemasterdnsnameserver"></a> hbase.master.dns.nameserver</h4><p>当使用DNS的时候，RegionServer使用的DNS的域名或者IP 地址，Master用它来确定用来进行通讯的域名.</p><p>默认: <code>default</code></p><h4 id="hbasebalancerperiod"><a class="markdownIt-Anchor" href="#hbasebalancerperiod"></a> hbase.balancer.period</h4><p>Master执行region balancer的间隔。</p><p>默认: <code>300000</code></p><h4 id="hbaseregionsslop"><a class="markdownIt-Anchor" href="#hbaseregionsslop"></a> hbase.regions.slop</h4><p>当任一区域服务器有average + (average * slop)个分区，将会执行重新均衡。默认 20% slop .</p><p>默认:0.2</p><h4 id="hbasemasterlogcleanerttl"><a class="markdownIt-Anchor" href="#hbasemasterlogcleanerttl"></a> hbase.master.logcleaner.ttl</h4><p>Hlog存在于.oldlogdir 文件夹的最长时间, 超过了就会被 Master 的线程清理掉.</p><p>默认: <code>600000</code></p><h4 id="hbasemasterlogcleanerplugins"><a class="markdownIt-Anchor" href="#hbasemasterlogcleanerplugins"></a> hbase.master.logcleaner.plugins</h4><p>LogsCleaner服务会执行的一组LogCleanerDelegat。值用逗号间隔的文本表示。这些WAL/HLog cleaners会按顺序调用。可以把先调用的放在前面。你可以实现自己的LogCleanerDelegat，加到Classpath下，然后在这里写下类的全称。一般都是加在默认值的前面。</p><p>默认: <code>org.apache.hadoop.hbase.master.TimeToLiveLogCleaner</code></p><h4 id="hbaseregionserverglobalmemstoreupperlimit"><a class="markdownIt-Anchor" href="#hbaseregionserverglobalmemstoreupperlimit"></a> hbase.regionserver.global.memstore.upperLimit</h4><p>单个region server的全部memtores的最大值。超过这个值，一个新的update操作会被挂起，强制执行flush操作。</p><p>默认: <code>0.4</code></p><h4 id="hbaseregionserverglobalmemstorelowerlimit"><a class="markdownIt-Anchor" href="#hbaseregionserverglobalmemstorelowerlimit"></a> hbase.regionserver.global.memstore.lowerLimit</h4><p>当强制执行flush操作的时候，当低于这个值的时候，flush会停止。默认是堆大小的 35% . 如果这个值和 hbase.regionserver.global.memstore.upperLimit 相同就意味着当update操作因为内存限制被挂起时，会尽量少的执行flush(译者注:一旦执行flush，值就会比下限要低，不再执行)</p><p>默认: <code>0.35</code></p><h4 id="hbaseserverthreadwakefrequency"><a class="markdownIt-Anchor" href="#hbaseserverthreadwakefrequency"></a> hbase.server.thread.wakefrequency</h4><p>service工作的sleep间隔，单位毫秒。 可以作为service线程的sleep间隔，比如log roller.</p><p>默认: <code>10000</code></p><h4 id="hbaseserverversionfilewriteattempts"><a class="markdownIt-Anchor" href="#hbaseserverversionfilewriteattempts"></a> hbase.server.versionfile.writeattempts</h4><p>退出前尝试写版本文件的次数。每次尝试由 hbase.server.thread.wakefrequency 毫秒数间隔。</p><p>默认: 3</p><h4 id="hbasehregionmemstoreflushsize"><a class="markdownIt-Anchor" href="#hbasehregionmemstoreflushsize"></a> hbase.hregion.memstore.flush.size</h4><p>当memstore的大小超过这个值的时候，会flush到磁盘。这个值被一个线程每隔hbase.server.thread.wakefrequency检查一下。</p><p>默认:134217728</p><h4 id="hbasehregionprecloseflushsize"><a class="markdownIt-Anchor" href="#hbasehregionprecloseflushsize"></a> hbase.hregion.preclose.flush.size</h4><p>当一个region中的memstore的大小大于这个值的时候，我们又触发了close.会先运行“pre-flush”操作，清理这个需要关闭的memstore，然后将这个region下线。当一个region下线了，我们无法再进行任何写操作。如果一个memstore很大的时候，flush操作会消耗很多时间。&quot;pre-flush&quot;操作意味着在region下线之前，会先把memstore清空。这样在最终执行close操作的时候，flush操作会很快。</p><p>默认: <code>5242880</code></p><h4 id="hbasehregionmemstoreblockmultiplier"><a class="markdownIt-Anchor" href="#hbasehregionmemstoreblockmultiplier"></a> hbase.hregion.memstore.block.multiplier</h4><p>如果memstore有hbase.hregion.memstore.block.multiplier倍数的hbase.hregion.flush.size的大小，就会阻塞update操作。这是为了预防在update高峰期会导致的失控。如果不设上界，flush的时候会花很长的时间来合并或者分割，最坏的情况就是引发out of memory异常。(译者注:内存操作的速度和磁盘不匹配，需要等一等。原文似乎有误)</p><p>默认: <code>2</code></p><h4 id="hbasehregionmemstoremslabenabled"><a class="markdownIt-Anchor" href="#hbasehregionmemstoremslabenabled"></a> hbase.hregion.memstore.mslab.enabled</h4><p>体验特性：启用memStore分配本地缓冲区。这个特性是为了防止在大量写负载的时候堆的碎片过多。这可以减少GC操作的频率。(GC有可能会Stop the world)(译者注：实现的原理相当于预分配内存，而不是每一个值都要从堆里分配)</p><p>默认: <code>true</code></p><h4 id="hbasehregionmaxfilesize"><a class="markdownIt-Anchor" href="#hbasehregionmaxfilesize"></a> hbase.hregion.max.filesize</h4><p>最大HStoreFile大小。若某个列族的HStoreFile增长达到这个值，这个Hegion会被切割成两个。 默认: 10G.</p><p>默认:10737418240</p><h4 id="hbasehstorecompactionthreshold"><a class="markdownIt-Anchor" href="#hbasehstorecompactionthreshold"></a> hbase.hstore.compactionThreshold</h4><p>当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，把这HStoreFiles写成一个。这个值越大，需要合并的时间就越长。</p><p>默认: <code>3</code></p><h4 id="hbasehstoreblockingstorefiles"><a class="markdownIt-Anchor" href="#hbasehstoreblockingstorefiles"></a> hbase.hstore.blockingStoreFiles</h4><p>当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，update会阻塞直到合并完成，直到超过了hbase.hstore.blockingWaitTime的值</p><p>默认: <code>7</code></p><h4 id="hbasehstoreblockingwaittime"><a class="markdownIt-Anchor" href="#hbasehstoreblockingwaittime"></a> hbase.hstore.blockingWaitTime</h4><p>hbase.hstore.blockingStoreFiles所限制的StoreFile数量会导致update阻塞，这个时间是来限制阻塞时间的。当超过了这个时间，HRegion会停止阻塞update操作，不过合并还有没有完成。默认为90s.</p><p>默认: <code>90000</code></p><h4 id="hbasehstorecompactionmax"><a class="markdownIt-Anchor" href="#hbasehstorecompactionmax"></a> hbase.hstore.compaction.max</h4><p>每个“小”合并的HStoreFiles最大数量。</p><p>默认: <code>10</code></p><h4 id="hbasehregionmajorcompaction"><a class="markdownIt-Anchor" href="#hbasehregionmajorcompaction"></a> hbase.hregion.majorcompaction</h4><p>一个Region中的所有HStoreFile的major compactions的时间间隔。默认是1天。 设置为0就是禁用这个功能。</p><p>默认: <code>86400000</code></p><h4 id="hbasestorescannerparallelseekenable"><a class="markdownIt-Anchor" href="#hbasestorescannerparallelseekenable"></a> hbase.storescanner.parallel.seek.enable</h4><p>允许 StoreFileScanner 并行搜索 StoreScanner, 一个在特定条件下降低延迟的特性。</p><p>默认: false</p><h4 id="hbasestorescannerparallelseekthreads"><a class="markdownIt-Anchor" href="#hbasestorescannerparallelseekthreads"></a> hbase.storescanner.parallel.seek.threads</h4><p>并行搜索特性打开后，默认线程池大小。</p><p>默认: 10</p><h4 id="hbasemapreducehfileoutputformatblocksize"><a class="markdownIt-Anchor" href="#hbasemapreducehfileoutputformatblocksize"></a> hbase.mapreduce.hfileoutputformat.blocksize</h4><p>MapReduce中HFileOutputFormat可以写 storefiles/hfiles. 这个值是hfile的blocksize的最小值。通常在HBase写Hfile的时候，bloocksize是由table schema(HColumnDescriptor)决定的，但是在mapreduce写的时候，我们无法获取schema中blocksize。这个值越小，你的索引就越大，你随机访问需要获取的数据就越小。如果你的cell都很小，而且你需要更快的随机访问，可以把这个值调低。</p><p>默认: <code>65536</code></p><h4 id="hfileblockcachesize"><a class="markdownIt-Anchor" href="#hfileblockcachesize"></a> hfile.block.cache.size</h4><p>分配给HFile/StoreFile的block cache占最大堆(-Xmx setting)的比例。默认0.25意思是分配25%，设置为0就是禁用，但不推荐。</p><p>默认:0.25</p><h4 id="hbasehashtype"><a class="markdownIt-Anchor" href="#hbasehashtype"></a> hbase.hash.type</h4><p>哈希函数使用的哈希算法。可以选择两个值:: murmur (MurmurHash) 和 jenkins (JenkinsHash). 这个哈希是给 bloom filters用的.</p><p>默认: <code>murmur</code></p><h4 id="hfileblockindexcacheonwrite"><a class="markdownIt-Anchor" href="#hfileblockindexcacheonwrite"></a> hfile.block.index.cacheonwrite</h4><p>在index写入的时候允许put无根（non-root）的多级索引块到block cache里，默认是false；</p><p>hfile.index.block.max.size：在多级索引的树形结构里，如果任何一层的block index达到这个配置大小，则block写出，同时替换上新的block，默认是131072；</p><h4 id="hfileformatversion"><a class="markdownIt-Anchor" href="#hfileformatversion"></a> hfile.format.version</h4><p>新文件的HFile 格式版本，设置为1来测试向后兼容，默认是2；</p><h4 id="iostorefilebloomblocksize"><a class="markdownIt-Anchor" href="#iostorefilebloomblocksize"></a> io.storefile.bloom.block.size</h4><p>一个联合布隆过滤器的单一块（chunk）的大小，这个值是一个逼近值，默认是131072；</p><h4 id="hfileblockbloomcacheonwrite"><a class="markdownIt-Anchor" href="#hfileblockbloomcacheonwrite"></a> hfile.block.bloom.cacheonwrite</h4><p>对于组合布隆过滤器的内联block开启cache-on-write，默认是false</p><h4 id="hbaserscacheblocksonwrite"><a class="markdownIt-Anchor" href="#hbaserscacheblocksonwrite"></a> hbase.rs.cacheblocksonwrite</h4><p>当一个HFile block完成时是否写入block cache，默认是false</p><h4 id="hbaserpcserverengine"><a class="markdownIt-Anchor" href="#hbaserpcserverengine"></a> hbase.rpc.server.engine</h4><p>hbase 做rpc server的调度管理类，实现自org.apache.hadoop.ipc.RpcServerEngine，默认是org.apache.hadoop.hbase.ipc.ProtobufRpcServerEngine</p><h4 id="hbaseipcclienttcpnodelay"><a class="markdownIt-Anchor" href="#hbaseipcclienttcpnodelay"></a> hbase.ipc.client.tcpnodelay</h4><p>默认是true，具体就是在tcp socket连接时设置 no delay</p><h4 id="hbasemasterkeytabfile"><a class="markdownIt-Anchor" href="#hbasemasterkeytabfile"></a> hbase.master.keytab.file</h4><p>HMaster server验证登录使用的kerberos keytab 文件路径。(译者注：HBase使用Kerberos实现安全)</p><p>默认:</p><h4 id="hbasemasterkerberosprincipal"><a class="markdownIt-Anchor" href="#hbasemasterkerberosprincipal"></a> hbase.master.kerberos.principal</h4><p>例如. “hbase/_HOST@EXAMPLE.COM”. HMaster运行需要使用 kerberos principal name. principal name 可以在: user/hostname@DOMAIN 中获取. 如果 “_HOST” 被用做hostname portion，需要使用实际运行的hostname来替代它。</p><p>默认:</p><h4 id="hbaseregionserverkeytabfile"><a class="markdownIt-Anchor" href="#hbaseregionserverkeytabfile"></a> hbase.regionserver.keytab.file</h4><p>HRegionServer验证登录使用的kerberos keytab 文件路径。</p><p>默认:</p><h4 id="hbaseregionserverkerberosprincipal"><a class="markdownIt-Anchor" href="#hbaseregionserverkerberosprincipal"></a> hbase.regionserver.kerberos.principal</h4><p>例如. “hbase/_HOST@EXAMPLE.COM”. HRegionServer运行需要使用 kerberos principal name. principal name 可以在: user/hostname@DOMAIN 中获取. 如果 “_HOST” 被用做hostname portion，需要使用实际运行的hostname来替代它。在这个文件中必须要有一个entry来描述 hbase.regionserver.keytab.file</p><p>默认:</p><h4 id="hadooppolicyfile"><a class="markdownIt-Anchor" href="#hadooppolicyfile"></a> hadoop.policy.file</h4><p>RPC服务器做权限认证时需要的安全策略配置文件，在Hbase security开启后使用，默认是habse-policy.xml；</p><h4 id="hbasesuperuser"><a class="markdownIt-Anchor" href="#hbasesuperuser"></a> hbase.superuser</h4><p>Hbase security 开启后的超级用户配置，一系列由逗号隔开的user或者group；</p><h4 id="hbaseauthkeyupdateinterval"><a class="markdownIt-Anchor" href="#hbaseauthkeyupdateinterval"></a> hbase.auth.key.update.interval</h4><p>Hbase security开启后服务端更新认证key的间隔时间：默认是86400000毫秒；</p><h4 id="hbaseauthtokenmaxlifetime"><a class="markdownIt-Anchor" href="#hbaseauthtokenmaxlifetime"></a> hbase.auth.token.max.lifetime</h4><p>Hbase security开启后，认证token下发后的生存周期，默认是604800000毫秒</p><h4 id="zookeepersessiontimeout"><a class="markdownIt-Anchor" href="#zookeepersessiontimeout"></a> zookeeper.session.timeout</h4><p>ZooKeeper 会话超时.HBase把这个值传递改zk集群，向他推荐一个会话的最大超时时间。详见<a target="_blank" rel="noopener" href="http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#ch_zkSessions">http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#ch_zkSessions</a> &quot;The client sends a requested timeout, the server responds with the timeout that it can give the client. &quot;。 单位是毫秒</p><p>默认: <code>180000</code></p><h4 id="zookeeperznodeparent"><a class="markdownIt-Anchor" href="#zookeeperznodeparent"></a> zookeeper.znode.parent</h4><p>ZooKeeper中的HBase的根ZNode。所有的HBase的ZooKeeper会用这个目录配置相对路径。默认情况下，所有的HBase的ZooKeeper文件路径是用相对路径，所以他们会都去这个目录下面。</p><p>默认: <code>/hbase</code></p><h4 id="zookeeperznoderootserver"><a class="markdownIt-Anchor" href="#zookeeperznoderootserver"></a> zookeeper.znode.rootserver</h4><p>ZNode 保存的 根region的路径. 这个值是由Master来写，client和regionserver 来读的。如果设为一个相对地址，父目录就是 ${zookeeper.znode.parent}.默认情形下，意味着根region的路径存储在/hbase/root-region-server.</p><p>默认: <code>root-region-server</code></p><h4 id="zookeeperznodeaclparent"><a class="markdownIt-Anchor" href="#zookeeperznodeaclparent"></a> zookeeper.znode.acl.parent</h4><p>root znode的acl，默认acl；</p><h4 id="hbasecoprocessorregionclasses"><a class="markdownIt-Anchor" href="#hbasecoprocessorregionclasses"></a> hbase.coprocessor.region.classes</h4><p>逗号分隔的Coprocessores列表，会被加载到默认所有表上。在自己实现了一个Coprocessor后，将其添加到Hbase的classpath并加入全限定名。也可以延迟加载，由HTableDescriptor指定；</p><h4 id="hbasecoprocessormasterclasses"><a class="markdownIt-Anchor" href="#hbasecoprocessormasterclasses"></a> hbase.coprocessor.master.classes</h4><p>由HMaster进程加载的coprocessors，逗号分隔，全部实现org.apache.hadoop.hbase.coprocessor.MasterObserver，同coprocessor类似，加入classpath及全限定名；</p><h4 id="hbasezookeeperquorum"><a class="markdownIt-Anchor" href="#hbasezookeeperquorum"></a> hbase.zookeeper.quorum</h4><p>Zookeeper集群的地址列表，用逗号分割。例如：“<a target="_blank" rel="noopener" href="http://host1.mydomain.com">host1.mydomain.com</a>,<a target="_blank" rel="noopener" href="http://host2.mydomain.com">host2.mydomain.com</a>,<a target="_blank" rel="noopener" href="http://host3.mydomain.com">host3.mydomain.com</a>”.默认是localhost,是给伪分布式用的。要修改才能在完全分布式的情况下使用。如果在hbase-env.sh设置了HBASE_MANAGES_ZK，这些ZooKeeper节点就会和HBase一起启动。</p><p>默认: <code>localhost</code></p><h4 id="hbasezookeeperpeerport"><a class="markdownIt-Anchor" href="#hbasezookeeperpeerport"></a> hbase.zookeeper.peerport</h4><p>ZooKeeper节点使用的端口。详细参见：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper">http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper</a></p><p>默认: <code>2888</code></p><h4 id="hbasezookeeperleaderport"><a class="markdownIt-Anchor" href="#hbasezookeeperleaderport"></a> hbase.zookeeper.leaderport</h4><p>ZooKeeper用来选择Leader的端口，详细参见：<a target="_blank" rel="noopener" href="http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper">http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper</a></p><p>默认: <code>3888</code></p><h4 id="hbasezookeeperusemulti"><a class="markdownIt-Anchor" href="#hbasezookeeperusemulti"></a> hbase.zookeeper.useMulti</h4><p>Instructs HBase to make use of ZooKeeper’s multi-update functionality. This allows certain ZooKeeper operations to complete more quickly and prevents some issues with rare Replication failure scenarios (see the release note of HBASE-2611 for an example). IMPORTANT: only set this to true if all ZooKeeper servers in the cluster are on version 3.4+ and will not be downgraded. ZooKeeper versions before 3.4 do not support multi-update and will not fail gracefully if multi-update is invoked (see ZOOKEEPER-1495).</p><p>Default: false</p><h4 id="hbasezookeeperpropertyinitlimit"><a class="markdownIt-Anchor" href="#hbasezookeeperpropertyinitlimit"></a> hbase.zookeeper.property.initLimit</h4><p>ZooKeeper的zoo.conf中的配置。 初始化synchronization阶段的ticks数量限制</p><p>默认: <code>10</code></p><h4 id="hbasezookeeperpropertysynclimit"><a class="markdownIt-Anchor" href="#hbasezookeeperpropertysynclimit"></a> hbase.zookeeper.property.syncLimit</h4><p>ZooKeeper的zoo.conf中的配置。 发送一个请求到获得承认之间的ticks的数量限制</p><p>默认: <code>5</code></p><h4 id="hbasezookeeperpropertydatadir"><a class="markdownIt-Anchor" href="#hbasezookeeperpropertydatadir"></a> hbase.zookeeper.property.dataDir</h4><p>ZooKeeper的zoo.conf中的配置。 快照的存储位置</p><p>默认: <code>$&#123;hbase.tmp.dir&#125;/zookeeper</code></p><h4 id="hbasezookeeperpropertyclientport"><a class="markdownIt-Anchor" href="#hbasezookeeperpropertyclientport"></a> hbase.zookeeper.property.clientPort</h4><p>ZooKeeper的zoo.conf中的配置。 客户端连接的端口</p><p>默认: <code>2181</code></p><h4 id="hbasezookeeperpropertymaxclientcnxns"><a class="markdownIt-Anchor" href="#hbasezookeeperpropertymaxclientcnxns"></a> hbase.zookeeper.property.maxClientCnxns</h4><p>ZooKeeper的zoo.conf中的配置。 ZooKeeper集群中的单个节点接受的单个Client(以IP区分)的请求的并发数。这个值可以调高一点，防止在单机和伪分布式模式中出问题。</p><p>默认: <code>300</code></p><h4 id="hbaserestport"><a class="markdownIt-Anchor" href="#hbaserestport"></a> hbase.rest.port</h4><p>HBase REST server的端口</p><p>默认: <code>8080</code></p><h4 id="hbaserestreadonly"><a class="markdownIt-Anchor" href="#hbaserestreadonly"></a> hbase.rest.readonly</h4><p>定义REST server的运行模式。可以设置成如下的值： false: 所有的HTTP请求都是被允许的 - GET/PUT/POST/DELETE. true:只有GET请求是被允许的</p><p>默认: <code>false</code></p><h4 id="hbasedefaultsforversionskip"><a class="markdownIt-Anchor" href="#hbasedefaultsforversionskip"></a> hbase.defaults.for.version.skip</h4><p>Set to true to skip the ‘hbase.defaults.for.version’ check. Setting this to true can be useful in contexts other than the other side of a maven generation; i.e. running in an ide. You’ll want to set this boolean to true to avoid seeing the RuntimException complaint: “hbase-default.xml file seems to be for and old version of HBase (${hbase.version}), this version is X.X.X-SNAPSHOT”</p><p>Default: false</p><p>是否跳过hbase.defaults.for.version的检查，默认是false；</p><h4 id="hbasecoprocessorabortonerror"><a class="markdownIt-Anchor" href="#hbasecoprocessorabortonerror"></a> hbase.coprocessor.abortonerror</h4><p>Set to true to cause the hosting server (master or regionserver) to abort if a coprocessor throws a Throwable object that is not IOException or a subclass of IOException. Setting it to true might be useful in development environments where one wants to terminate the server as soon as possible to simplify coprocessor failure analysis.</p><p>Default: false</p><p>如果coprocessor加载失败或者初始化失败或者抛出Throwable对象，则主机退出。设置为false会让系统继续运行，但是coprocessor的状态会不一致，所以一般debug时才会设置为false，默认是true；</p><h4 id="hbaseonlineschemaupdateenable"><a class="markdownIt-Anchor" href="#hbaseonlineschemaupdateenable"></a> hbase.online.schema.update.enable</h4><p>Set true to enable online schema changes. This is an experimental feature. There are known issues modifying table schemas at the same time a region split is happening so your table needs to be quiescent or else you have to be running with splits disabled.</p><p>Default: false</p><p>设置true来允许在线schema变更，默认是true；</p><h4 id="hbasetablelockenable"><a class="markdownIt-Anchor" href="#hbasetablelockenable"></a> hbase.table.lock.enable</h4><p>Set to true to enable locking the table in zookeeper for schema change operations. Table locking from master prevents concurrent schema modifications to corrupt table state.</p><p>Default: true</p><p>设置为true来允许在schema变更时zk锁表，锁表可以组织并发的schema变更导致的表状态不一致，默认是true；</p><h4 id="dfssupportappend"><a class="markdownIt-Anchor" href="#dfssupportappend"></a> dfs.support.append</h4><p>Does HDFS allow appends to files? This is an hdfs config. set in here so the hdfs client will do append support. You must ensure that this config. is true serverside too when running hbase (You will have to restart your cluster after setting it).</p><p>Default: true</p><h4 id="hbasethriftminworkerthreads"><a class="markdownIt-Anchor" href="#hbasethriftminworkerthreads"></a> hbase.thrift.minWorkerThreads</h4><p>The “core size” of the thread pool. New threads are created on every connection until this many threads are created.</p><p>Default: 16</p><p>线程池的core size，在达到这里配置的量级后，新线程才会再新的连接创立时创建，默认是16；</p><h4 id="hbasethriftmaxworkerthreads"><a class="markdownIt-Anchor" href="#hbasethriftmaxworkerthreads"></a> hbase.thrift.maxWorkerThreads</h4><p>The maximum size of the thread pool. When the pending request queue overflows, new threads are created until their number reaches this number. After that, the server starts dropping connections.</p><p>Default: 1000</p><p>顾名思义，最大线程数，达到这个数字后，服务器开始drop连接，默认是1000；</p><h4 id="hbasethriftmaxqueuedrequests"><a class="markdownIt-Anchor" href="#hbasethriftmaxqueuedrequests"></a> hbase.thrift.maxQueuedRequests</h4><p>The maximum number of pending Thrift connections waiting in the queue. If there are no idle threads in the pool, the server queues requests. Only when the queue overflows, new threads are added, up to hbase.thrift.maxQueuedRequests threads.</p><p>Default: 1000</p><p>Thrift连接队列的最大数，如果线程池满，会先在这个队列中缓存请求，缓存上限就是该配置，默认是1000；</p><h4 id="hbaseoffheapcachepercentage"><a class="markdownIt-Anchor" href="#hbaseoffheapcachepercentage"></a> hbase.offheapcache.percentage</h4><p>The amount of off heap space to be allocated towards the experimental off heap cache. If you desire the cache to be disabled, simply set this value to 0.</p><p>Default: 0</p><p>JVM参数-XX:MaxDirectMemorySize的百分比值，默认是0，即不开启堆外分配；</p><h4 id="hbasedataumaskenable"><a class="markdownIt-Anchor" href="#hbasedataumaskenable"></a> hbase.data.umask.enable</h4><p>Enable, if true, that file permissions should be assigned to the files written by the regionserver</p><p>Default: false</p><p>开启后，文件在regionserver写入时会 有权限相关设定，默认是false不开启；</p><h4 id="hbasedataumask"><a class="markdownIt-Anchor" href="#hbasedataumask"></a> hbase.data.umask</h4><p>File permissions that should be used to write data files when hbase.data.umask.enable is true</p><p>Default: 000</p><p>开启上面一项配置后，文件的权限umask，默认是000</p><h4 id="hbasemetricsshowtablename"><a class="markdownIt-Anchor" href="#hbasemetricsshowtablename"></a> hbase.metrics.showTableName</h4><p>Whether to include the prefix “tbl.tablename” in per-column family metrics. If true, for each metric M, per-cf metrics will be reported for tbl.T.cf.CF.M, if false, per-cf metrics will be aggregated by column-family across tables, and reported for cf.CF.M. In both cases, the aggregated metric M across tables and cfs will be reported.</p><p>Default: true</p><p>是否为每个指标显示表名前缀，默认是true；</p><h4 id="hbasemetricsexposeoperationtimes"><a class="markdownIt-Anchor" href="#hbasemetricsexposeoperationtimes"></a> hbase.metrics.exposeOperationTimes</h4><p>Whether to report metrics about time taken performing an operation on the region server. Get, Put, Delete, Increment, and Append can all have their times exposed through Hadoop metrics per CF and per region.</p><p>Default: true</p><p>是否进行关于操作在使用时间维度的指标报告，比如GET PUT DELETE INCREMENT等，默认是true；</p><h4 id="hbasemasterhfilecleanerplugins"><a class="markdownIt-Anchor" href="#hbasemasterhfilecleanerplugins"></a> hbase.master.hfilecleaner.plugins</h4><p>A comma-separated list of HFileCleanerDelegate invoked by the HFileCleaner service. These HFiles cleaners are called in order, so put the cleaner that prunes the most files in front. To implement your own HFileCleanerDelegate, just put it in HBase’s classpath and add the fully qualified class name here. Always add the above default log cleaners in the list as they will be overwritten in hbase-site.xml.</p><p>Default: org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner</p><p>HFile的清理插件列表，逗号分隔，被HFileService调用，可以自定义，默认org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner</p><h4 id="hbaseregionservercatalogtimeout"><a class="markdownIt-Anchor" href="#hbaseregionservercatalogtimeout"></a> hbase.regionserver.catalog.timeout</h4><p>Timeout value for the Catalog Janitor from the regionserver to META.</p><p>Default: 600000</p><p>regionserver的Catalog Janitor访问META的超时时间，默认是600000；</p><h4 id="hbasemastercatalogtimeout"><a class="markdownIt-Anchor" href="#hbasemastercatalogtimeout"></a> hbase.master.catalog.timeout</h4><p>Timeout value for the Catalog Janitor from the master to META.</p><p>Default: 600000</p><p>Catalog Janitor从master到META的超时时间，我们知道这个Janitor是定时的去META扫描表目录，来决定回收无用的regions，默认是600000；</p><h4 id="hbaseconfigreadzookeeperconfig"><a class="markdownIt-Anchor" href="#hbaseconfigreadzookeeperconfig"></a> hbase.config.read.zookeeper.config</h4><p>Set to true to allow HBaseConfiguration to read the zoo.cfg file for ZooKeeper properties. Switching this to true is not recommended, since the functionality of reading ZK properties from a zoo.cfg file has been deprecated.</p><p>Default: false</p><p>让hbaseconfig去读zk的config，默认false，也不支持开启，这个功能很搞笑~~个人观点；</p><h4 id="hbasesnapshotenabled"><a class="markdownIt-Anchor" href="#hbasesnapshotenabled"></a> hbase.snapshot.enabled</h4><p>Set to true to allow snapshots to be taken / restored / cloned.</p><p>Default: true</p><p>是否允许snapshot被使用、存储和克隆，默认是true；</p><h4 id="hbaserestthreadsmax"><a class="markdownIt-Anchor" href="#hbaserestthreadsmax"></a> hbase.rest.threads.max</h4><p>The maximum number of threads of the REST server thread pool. Threads in the pool are reused to process REST requests. This controls the maximum number of requests processed concurrently. It may help to control the memory used by the REST server to avoid OOM issues. If the thread pool is full, incoming requests will be queued up and wait for some free threads. The default is 100.</p><p>Default: 100</p><p>REST服务器线程池的最大线程数，池满的话新请求会自动排队，限制这个配置可以控制服务器的内存量，预防OOM，默认是100；</p><h4 id="hbaserestthreadsmin"><a class="markdownIt-Anchor" href="#hbaserestthreadsmin"></a> hbase.rest.threads.min</h4><p>The minimum number of threads of the REST server thread pool. The thread pool always has at least these number of threads so the REST server is ready to serve incoming requests. The default is 2.</p><p>Default: 2</p><p>同上类似，最小线程数，为了确保服务器的服务状态，默认是2；</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%8A%80%E6%9C%AF%E5%B7%A5%E5%85%B7/" class="category-chain-item">技术工具</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Hadoop/" class="print-no-link">#Hadoop</a> <a href="/tags/HBase/" class="print-no-link">#HBase</a></div></div><div class="license-box my-3"><div class="license-title"><div>HBase的hbase-site.xml文件配置详解</div><div>https://fulequn.github.io/2020/11/Article202011094/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2020年11月9日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2020/11/Article202011181/" title="“Couldn&#39;t find field google.protobuf.DescriptorProto.ExtensionRange.options”问题"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">“Couldn&#39;t find field google.protobuf.DescriptorProto.ExtensionRange.options”问题</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/11/Article202011093/" title="HBase Java API基本概念与使用"><span class="hidden-mobile">HBase Java API基本概念与使用</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>