<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Fulequn"><meta name="keywords" content=""><meta name="description" content="layers.py 首先实现神经网络中仿射层、ReLU层以及组合单层神经元。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848"><meta property="og:type" content="article"><meta property="og:title" content="编码实现全连接神经网络"><meta property="og:url" content="https://fulequn.github.io/2020/11/Article202011303/index.html"><meta property="og:site_name" content="FuLeQun&#39;s Blog"><meta property="og:description" content="layers.py 首先实现神经网络中仿射层、ReLU层以及组合单层神经元。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-11-30T04:48:58.000Z"><meta property="article:modified_time" content="2024-05-30T00:16:34.000Z"><meta property="article:author" content="Fulequn"><meta property="article:tag" content="Python"><meta property="article:tag" content="DeepLearning"><meta name="twitter:card" content="summary_large_image"><title>编码实现全连接神经网络 - FuLeQun&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"fulequn.github.io",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:60,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null,tajs:null},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.2.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>FuLeQun&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="编码实现全连接神经网络"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-11-30 12:48" pubdate>2020年11月30日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i>3.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i>28 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">编码实现全连接神经网络</h1><div class="markdown-body"><h2 id="layerspy"><a class="markdownIt-Anchor" href="#layerspy"></a> <a target="_blank" rel="noopener" href="http://layers.py">layers.py</a></h2><p>首先实现神经网络中仿射层、ReLU层以及组合单层神经元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算神经网络当前层的前馈传播，该方法计算在全连接情况下的得分函数。</span><br><span class="hljs-string">    注：如果不理解affine仿射变换，简单的理解为在全连接情况下的得分函数即可。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    输入数据x的形状为(N, d_1, ..., d_k)，其中N表示数据量，(d_1, ..., d_k)表示</span><br><span class="hljs-string">    每一通道的数据维度，如果是图片数据就为(长，宽，色道)。数据的总维度为</span><br><span class="hljs-string">    D = d_1 * ... * d_k，因此我们需要将数据重塑成形状为(N,D)的数组再进行仿射变换。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入数据，其形状为(N, d_1, ..., d_k)的numpy数组。</span><br><span class="hljs-string">    - w: 权重矩阵，其形状为(D,M)的numpy数组，D表示输入数据维度，M表示输出数据维度</span><br><span class="hljs-string">             可以将D看成输入的神经元个数，M看成输出神经元个数。</span><br><span class="hljs-string">    - b: 偏置向量，其形状为(M,)的numpy数组。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 形状为(N, M)的输出结果。</span><br><span class="hljs-string">    - cache: 将输入进行缓存(x, w, b)。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                    任务: 实现全连接前向传播                                                         #</span><br>    <span class="hljs-comment">#                    注：首先你需要将输入数据重塑成行。                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 获取样本数</span><br>    N = x.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># 整合数据</span><br>    x_new = x.reshape(N, -<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 计算仿射的得分</span><br>    out = np.dot(x_new, w)+b<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = (x, w, b)<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     计算仿射层的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: 形状为(N, M)的上层梯度</span><br><span class="hljs-string">    - cache: 元组:</span><br><span class="hljs-string">        - x: (N, d_1, ... d_k)的输入数据</span><br><span class="hljs-string">        - w: 形状为(D, M)的权重矩阵</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度，其形状为(N, d1, ..., d_k)</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度，其形状为(D,M)</span><br><span class="hljs-string">    - db: 偏置项b的梯度，其形状为(M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b = cache<br>    dx, dw, db = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                 任务: 实现仿射层反向传播                                                                 #</span><br>    <span class="hljs-comment">#                 注意：你需要将x重塑成(N,D)后才能计算各梯度，                                            #</span><br>    <span class="hljs-comment">#                 求完梯度后你需要将dx的形状与x重塑成一样                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment"># 计算b的梯度</span><br>    db = np.<span class="hljs-built_in">sum</span>(dout, axis=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># 将x重塑成(N, D)</span><br>    xx = x.reshape(x.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>    dw = np.dot(xx.T, dout)<br>    dx = np.dot(dout, w.T)<br>    <span class="hljs-comment"># 将dx的形状与x重塑成一样</span><br>    dx = np.reshape(dx, x.shape)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                                         结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_forward</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算rectified linear units (ReLUs)激活函数的前向传播，并保存相应缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - x: 输入数据</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out: 和输入数据x形状相同</span><br><span class="hljs-string">    - cache: x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    out = <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                         任务: 实现ReLU 的前向传播.                                                                        #</span><br>    <span class="hljs-comment">#                        注意：你只需要1行代码即可完成                                                                    #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    out = np.maximum(<span class="hljs-number">0</span>, x)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                        结束编码                                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    cache = x<br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 rectified linear units (ReLUs)激活函数的反向传播.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: 输入 x,其形状应该和dout相同</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: x的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dx, x = <span class="hljs-literal">None</span>, cache<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU 反向传播.                                                                     #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    dx = dout<br>    dx[x&lt;=<span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                            结束编码                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_forward</span>(<span class="hljs-params">x, w, b</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元前向传播</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - x: 输入到 affine层的数据</span><br><span class="hljs-string">    - w, b:    affine层的权重矩阵和偏置向量</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns 元组:</span><br><span class="hljs-string">    - out:    ReLU的输出结果</span><br><span class="hljs-string">    - cache: 前向传播的缓存</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">######################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU神经元前向传播.                                             #</span><br>    <span class="hljs-comment">#                注意：你需要调用affine_forward以及relu_forward函数，                #</span><br>    <span class="hljs-comment">#                            并将各自的缓存保存在cache中                                                     #</span><br>    <span class="hljs-comment">######################################################################</span><br>    a, fc_cache = affine_forward(x, w, b)<br>    out, relu_cache = relu_forward(a)<br>    cache = (fc_cache, relu_cache)<br>    <span class="hljs-comment">######################################################################</span><br>    <span class="hljs-comment">#                                         结束编码                                                                             #</span><br>    <span class="hljs-comment">######################################################################</span><br>    <span class="hljs-keyword">return</span> out, cache<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">affine_relu_backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">     ReLU神经元的反向传播</span><br><span class="hljs-string">     </span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: 上层误差梯度</span><br><span class="hljs-string">    - cache: affine缓存，以及relu缓存</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: 输入数据x的梯度</span><br><span class="hljs-string">    - dw: 权重矩阵w的梯度</span><br><span class="hljs-string">    - db: 偏置向量b的梯度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                             任务: 实现 ReLU神经元反向传播.                                                            #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    fc_cache, relu_cache = cache<br>    da = relu_backward(dout, relu_cache)<br>    dx, dw, db = affine_backward(da, fc_cache)<br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-comment">#                                     结束编码                                                                                             #</span><br>    <span class="hljs-comment">#############################################################################</span><br>    <span class="hljs-keyword">return</span> dx, dw, db<br><br><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_loss</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    计算Softmax损失函数</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    x : numpy数组</span><br><span class="hljs-string">        表示训练数据.</span><br><span class="hljs-string">    y : numpy数组</span><br><span class="hljs-string">        表示数据类标。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    loss : TYPE</span><br><span class="hljs-string">        数据损失值</span><br><span class="hljs-string">    dx : TYPE</span><br><span class="hljs-string">        梯度</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    probs = np.exp(x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>))<br>    probs /= np.<span class="hljs-built_in">sum</span>(probs, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 数据量</span><br>    N = x.shape[<span class="hljs-number">0</span>]<br>    loss = -np.<span class="hljs-built_in">sum</span>(np.log(probs[np.arange(N), y])) / N<br>    dx = probs.copy()<br>    dx[np.arange(N), y] -= <span class="hljs-number">1</span><br>    dx /= N<br><br>    <span class="hljs-keyword">return</span> loss, dx<br></code></pre></td></tr></table></figure><h2 id="shallow_layer_netpy"><a class="markdownIt-Anchor" href="#shallow_layer_netpy"></a> shallow_layer_net.py</h2><p>实现浅层神经网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> .layers <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShallowLayerNet</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    浅层全连接神经网络，其中隐藏层使用ReLU作为激活函数，输出层使用softmax作为分类器</span><br><span class="hljs-string">    该网络结构应该为     affine - relu -affine - softmax</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim=<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>, hidden_dim=<span class="hljs-number">100</span>, num_classes=<span class="hljs-number">10</span>,</span><br><span class="hljs-params">               weight_scale=<span class="hljs-number">1e-3</span>, reg=<span class="hljs-number">0.0</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化网络.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - input_dim: 输入数据维度</span><br><span class="hljs-string">        - hidden_dim: 隐藏层维度</span><br><span class="hljs-string">        - num_classes: 分类数量</span><br><span class="hljs-string">        - weight_scale: 权重范围，给予初始化权重的标准差</span><br><span class="hljs-string">        - reg: L2正则化的权重衰减系数.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.params = &#123;&#125;<br>        self.reg = reg<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                      任务：初始化权重以及偏置项                          #</span><br>        <span class="hljs-comment">#      权重应该服从标准差为weight_scale的高斯分布，偏置项应该初始化为0,    #</span><br>        <span class="hljs-comment">#        所有权重矩阵和偏置向量应该存放在self.params字典中。               #</span><br>        <span class="hljs-comment">#     第一层的权重和偏置使用键值 &#x27;W1&#x27;以及&#x27;b1&#x27;，第二层使用&#x27;W2&#x27;以及&#x27;b2&#x27;      #</span><br>        <span class="hljs-comment">############################################################################</span><br>        self.params[<span class="hljs-string">&#x27;W1&#x27;</span>] = weight_scale * np.random.randn(input_dim, hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.zeros(hidden_dim)<br>        self.params[<span class="hljs-string">&#x27;W2&#x27;</span>] = weight_scale * np.random.randn(hidden_dim, num_classes)<br>        self.params[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.zeros(num_classes)<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                           结束编码                                       #</span><br>        <span class="hljs-comment">############################################################################</span><br>    <br>    <br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, X, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        计算数据X的损失值以及梯度.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 输入数据，形状为(N, d_1, ..., d_k)的numpy数组。</span><br><span class="hljs-string">        - y: 数据类标，形状为(N,)的numpy数组。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        如果y为 None, 表明网络处于测试阶段直接返回输出层的得分即可:</span><br><span class="hljs-string">        - scores:形状为 (N, C)，其中scores[i, c] 是数据 X[i] 在第c类上的得分.</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        如果y为 not None, 表明网络处于训练阶段，返回一个元组:</span><br><span class="hljs-string">        - loss:数据的损失值</span><br><span class="hljs-string">        - grads: 与参数字典相同的梯度字典，键值和参数字典的键值要相同</span><br><span class="hljs-string">        &quot;&quot;&quot;</span>  <br>        scores = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#              任务: 实现浅层网络的前向传播过程，                          #</span><br>        <span class="hljs-comment">#                       计算各数据的分类得分                               #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment"># 第1层是affine和relu的组合层</span><br>        out1, cache1 = affine_relu_forward(X, self.params[<span class="hljs-string">&#x27;W1&#x27;</span>], self.params[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>		<span class="hljs-comment"># 第2层是affine层</span><br>        scores, cache2 = affine_forward(out1, self.params[<span class="hljs-string">&#x27;W2&#x27;</span>], self.params[<span class="hljs-string">&#x27;b2&#x27;</span>])<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                            结束编码                                     #</span><br>        <span class="hljs-comment">############################################################################</span><br><br>        <span class="hljs-comment"># 如果y为 None 直接返回得分</span><br>        <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>          <span class="hljs-keyword">return</span> scores<br><br>        loss, grads = <span class="hljs-number">0</span>, &#123;&#125;<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                    任务：实现浅层网络的反向传播过程，                    #</span><br>        <span class="hljs-comment">#            将损失值存储在loss中,将各层梯度储存在grads字典中。            #</span><br>        <span class="hljs-comment">#                           注意：别忘了还要计算权重衰减哟。               #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment"># 计算最后的分类</span><br>        loss, dy = softmax_loss(scores, y)<br>        loss += <span class="hljs-number">0.5</span>*self.reg*(np.<span class="hljs-built_in">sum</span>(self.params[<span class="hljs-string">&#x27;W1&#x27;</span>]*self.params[<span class="hljs-string">&#x27;W1&#x27;</span>])<br>                   +np.<span class="hljs-built_in">sum</span>(self.params[<span class="hljs-string">&#x27;W2&#x27;</span>]*self.params[<span class="hljs-string">&#x27;W2&#x27;</span>]))<br>        dx2, dw2, grads[<span class="hljs-string">&#x27;b2&#x27;</span>] = affine_backward(dy, cache2) <br>        grads[<span class="hljs-string">&#x27;W2&#x27;</span>] = dw2 + self.reg*self.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        dx, dw1, grads[<span class="hljs-string">&#x27;b1&#x27;</span>] = affine_relu_backward(dx2, cache1) <br>        grads[<span class="hljs-string">&#x27;W1&#x27;</span>] = dw1 + self.reg*self.params[<span class="hljs-string">&#x27;W1&#x27;</span>]<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                             结束编码                                     #</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-keyword">return</span> loss, grads<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X, y, X_val, y_val,</span><br><span class="hljs-params">            learning_rate=<span class="hljs-number">1e-3</span>, learning_rate_decay=<span class="hljs-number">0.95</span>,</span><br><span class="hljs-params">            reg=<span class="hljs-number">1e-5</span>, num_iters=<span class="hljs-number">100</span>,</span><br><span class="hljs-params">            batch_size=<span class="hljs-number">200</span>, verbose=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        使用随机梯度下贱训练神经网络</span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 训练数据</span><br><span class="hljs-string">        - y: 训练类标.</span><br><span class="hljs-string">        - X_val: 验证数据.</span><br><span class="hljs-string">        - y_val:验证类标.</span><br><span class="hljs-string">        - learning_rate: 学习率.</span><br><span class="hljs-string">        - learning_rate_decay: 学习率衰减系数</span><br><span class="hljs-string">        - reg: 权重衰减系数.</span><br><span class="hljs-string">        - num_iters: 迭代次数.</span><br><span class="hljs-string">        - batch_size: 批量大小.</span><br><span class="hljs-string">        - verbose:是否在训练过程中打印结果.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train = X.shape[<span class="hljs-number">0</span>]<br>        self.reg =reg<br>        <span class="hljs-comment">#打印以及学习率衰减周期</span><br>        iterations_per_epoch = <span class="hljs-built_in">max</span>(num_train / batch_size, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 存储训练中的数据</span><br>        loss_history = []  <span class="hljs-comment">#loss值</span><br>        train_acc_history = []  <span class="hljs-comment">#训练精度</span><br>        val_acc_history = []  <span class="hljs-comment">#记录精度</span><br>        best_val=-<span class="hljs-number">1</span>  <span class="hljs-comment">#记录最佳精度</span><br>        <br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br>          X_batch = <span class="hljs-literal">None</span><br>          y_batch = <span class="hljs-literal">None</span><br><br>          sample_index = np.random.choice(num_train, batch_size, replace=<span class="hljs-literal">True</span>)<br>          X_batch = X[sample_index, :]  <span class="hljs-comment"># (batch_size,D)</span><br>          y_batch = y[sample_index]  <span class="hljs-comment"># (1,batch_size)</span><br><br>          <span class="hljs-comment">#计算损失及梯度</span><br>          loss, grads = self.loss(X_batch, y=y_batch)<br>          loss_history.append(loss)<br><br>          <span class="hljs-comment">#修改权重</span><br>          self.params[<span class="hljs-string">&#x27;W1&#x27;</span>] += -learning_rate*grads[<span class="hljs-string">&#x27;W1&#x27;</span>]<br>          self.params[<span class="hljs-string">&#x27;W2&#x27;</span>] += -learning_rate*grads[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>          self.params[<span class="hljs-string">&#x27;b1&#x27;</span>] += -learning_rate*grads[<span class="hljs-string">&#x27;b1&#x27;</span>]<br>          self.params[<span class="hljs-string">&#x27;b2&#x27;</span>] += -learning_rate*grads[<span class="hljs-string">&#x27;b2&#x27;</span>]<br><br>          <span class="hljs-keyword">if</span> verbose <span class="hljs-keyword">and</span> it % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;迭代次数 %d / %d: 损失值 %f&#x27;</span> % (it, num_iters, loss))<br><br>          <span class="hljs-keyword">if</span> it % iterations_per_epoch == <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 测试精度</span><br>            train_acc = (self.predict(X_batch) == y_batch).mean()<br>            val_acc = (self.predict(X_val) == y_val).mean()<br>            <span class="hljs-comment"># 记录数据</span><br>            train_acc_history.append(train_acc)<br>            val_acc_history.append(val_acc)<br>            <span class="hljs-keyword">if</span> (best_val &lt; val_acc):<br>                best_val = val_acc<br>            <span class="hljs-comment"># 学习率衰减</span><br>            learning_rate *= learning_rate_decay<br>        <span class="hljs-comment"># 以字典的形式返回结果</span><br>        <span class="hljs-keyword">return</span> &#123;<br>          <span class="hljs-string">&#x27;loss_history&#x27;</span>: loss_history,<br>          <span class="hljs-string">&#x27;train_acc_history&#x27;</span>: train_acc_history,<br>          <span class="hljs-string">&#x27;val_acc_history&#x27;</span>: val_acc_history,<br>          <span class="hljs-string">&#x27;best_val_acc&#x27;</span>:best_val<br>        &#125;<br>    <br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 输入数据</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - y_pred: 预测类别</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        y_pred = <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># 使用神经网络计算各类的可能性结果</span><br>        out1, cache1 = affine_relu_forward(X,self.params[<span class="hljs-string">&#x27;W1&#x27;</span>],self.params[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>        scores, cache2 = affine_forward(out1,self.params[<span class="hljs-string">&#x27;W2&#x27;</span>],self.params[<span class="hljs-string">&#x27;b2&#x27;</span>])<br>        <span class="hljs-comment"># 选择可能性最大的类别作为该样本的分类</span><br>        y_pred = np.argmax(scores, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure><h2 id="fc_netpy"><a class="markdownIt-Anchor" href="#fc_netpy"></a> fc_net.py</h2><p>实现深层全连接网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#-*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> .layers <span class="hljs-keyword">import</span> *<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyConnectedNet</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    深层全连接神经网络，其中隐藏层使用ReLU作为激活函数，输出层使用softmax作为分类器</span><br><span class="hljs-string">    该网络结构应该为     &#123;affine - relu&#125;x(L -1) -affine - softmax</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim=<span class="hljs-number">3</span>*<span class="hljs-number">32</span>*<span class="hljs-number">32</span>, hidden_dims=[<span class="hljs-number">50</span>,<span class="hljs-number">50</span>], num_classes=<span class="hljs-number">10</span>,</span><br><span class="hljs-params">                 reg=<span class="hljs-number">0.0</span>, weight_scale=<span class="hljs-number">1e-3</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化网络.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - input_dim: 输入数据维度</span><br><span class="hljs-string">        - hidden_dim: 隐藏层各层维度</span><br><span class="hljs-string">        - num_classes: 分类数量</span><br><span class="hljs-string">        - weight_scale: 权重范围，给予初始化权重的标准差</span><br><span class="hljs-string">        - reg: L2正则化的权重衰减系数.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.reg = reg<br>        self.num_layers = <span class="hljs-number">1</span> + <span class="hljs-built_in">len</span>(hidden_dims)<br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 这里存储的是每层的神经元数量。 </span><br>        layers_dims = [input_dim] + hidden_dims + [num_classes] <br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                    任务：初始化任意多层权重以及偏置项                    #</span><br>        <span class="hljs-comment">#      权重应该服从标准差为weight_scale的高斯分布，偏置项应该初始化为0,    #</span><br>        <span class="hljs-comment">#        所有权重矩阵和偏置向量应该存放在self.params字典中。               #</span><br>        <span class="hljs-comment">#   第一层的权重和偏置使用键值 &#x27;W1&#x27;以及&#x27;b1&#x27;，第n层使用&#x27;Wn&#x27;以及&#x27;bn&#x27;         #</span><br>        <span class="hljs-comment">############################################################################   </span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>            self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)] = weight_scale*np.random.randn(<br>                layers_dims[i], layers_dims[i+<span class="hljs-number">1</span>])<br>            self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)] = np.zeros((<span class="hljs-number">1</span>, layers_dims[i+<span class="hljs-number">1</span>]))<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                           结束编码                                       #</span><br>        <span class="hljs-comment">############################################################################</span><br><br><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, X, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        计算数据X的损失值以及梯度.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 输入数据，形状为(N, d_1, ..., d_k)的numpy数组。</span><br><span class="hljs-string">        - y: 数据类标，形状为(N,)的numpy数组。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        如果y为 None, 表明网络处于测试阶段直接返回输出层的得分即可:</span><br><span class="hljs-string">        - scores:形状为 (N, C)，其中scores[i, c] 是数据 X[i] 在第c类上的得分.</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        如果y为 not None, 表明网络处于训练阶段，返回一个元组:</span><br><span class="hljs-string">        - loss:数据的损失值</span><br><span class="hljs-string">        - grads: 与参数字典相同的梯度字典，键值和参数字典的键值要相同</span><br><span class="hljs-string">        &quot;&quot;&quot;</span>  <br>        scores = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#              任务: 实现深层网络的前向传播过程，                          #</span><br>        <span class="hljs-comment">#                       计算各数据的分类得分                               #</span><br>        <span class="hljs-comment">############################################################################</span><br>        cache_relu, outs, cache_out = &#123;&#125;, &#123;&#125;, &#123;&#125;<br>        outs[<span class="hljs-number">0</span>] = X<br>        num_h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_h):<br>            outs[i+<span class="hljs-number">1</span>], cache_relu[i+<span class="hljs-number">1</span>] = affine_relu_forward(<br>                outs[i], self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)], self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)])<br>        scores, cache_out = affine_forward(outs[num_h], <br>                                           self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)],<br>                                           self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)])<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                            结束编码                                      #</span><br>        <span class="hljs-comment">############################################################################</span><br>        loss, grads = <span class="hljs-number">0.0</span>, &#123;&#125;<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                    任务：实现深层网络的反向传播过程，                    #</span><br>        <span class="hljs-comment">#            将损失值存储在loss中,将各层梯度储存在grads字典中。            #</span><br>        <span class="hljs-comment">#                           注意：别忘了还要计算权重衰减哟。               #</span><br>        <span class="hljs-comment">############################################################################</span><br>        dout, daffine = &#123;&#125;, &#123;&#125;<br>        loss, dy = softmax_loss(scores, y)<br>        <span class="hljs-comment"># 隐藏层数</span><br>        h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-comment"># 最后一层是affine层，前面的均是组合层</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>            loss += <span class="hljs-number">0.5</span>*self.reg*(np.<span class="hljs-built_in">sum</span>(self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)]))<br>            dout[h], grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)], grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)] = affine_backward(dy, cache_out)<br>            grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)] += self.reg*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h+<span class="hljs-number">1</span>)]<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(h):<br>                dout[h-i-<span class="hljs-number">1</span>], grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)], grads[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] = affine_relu_backward(<br>                    dout[h-i], cache_relu[h-i])<br>                grads[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)] += self.reg*self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(h-i)]<br>        <span class="hljs-comment">############################################################################</span><br>        <span class="hljs-comment">#                             结束编码                                     #</span><br>        <span class="hljs-comment">############################################################################</span><br><br>        <span class="hljs-keyword">return</span> loss, grads<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X, y, X_val, </span><br><span class="hljs-params">              y_val,learning_rate=<span class="hljs-number">1e-3</span>, learning_rate_decay=<span class="hljs-number">0.95</span>,</span><br><span class="hljs-params">              num_iters=<span class="hljs-number">100</span>,batch_size=<span class="hljs-number">200</span>, verbose=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        使用随机梯度下降训练神经网络</span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 训练数据</span><br><span class="hljs-string">        - y: 训练类标.</span><br><span class="hljs-string">        - X_val: 验证数据.</span><br><span class="hljs-string">        - y_val:验证类标.</span><br><span class="hljs-string">        - learning_rate: 学习率.</span><br><span class="hljs-string">        - learning_rate_decay: 学习率衰减系数</span><br><span class="hljs-string">        - reg: 权重衰减系数.</span><br><span class="hljs-string">        - num_iters: 迭代次数.</span><br><span class="hljs-string">        - batch_size: 批量大小.</span><br><span class="hljs-string">        - verbose:是否在训练过程中打印结果.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_train = X.shape[<span class="hljs-number">0</span>]<br>        iterations_per_epoch = <span class="hljs-built_in">max</span>(num_train / batch_size, <span class="hljs-number">1</span>)<br><br>        loss_history = []<br>        train_acc_history = []<br>        val_acc_history = []<br>        best_val=-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iters):<br><br>            X_batch = <span class="hljs-literal">None</span><br>            y_batch = <span class="hljs-literal">None</span><br>            <br>            sample_index = np.random.choice(num_train, batch_size, replace=<span class="hljs-literal">True</span>)<br>            X_batch = X[sample_index, :]  <span class="hljs-comment"># (batch_size,D)</span><br>            y_batch = y[sample_index]  <span class="hljs-comment"># (1,batch_size)</span><br><br>            <span class="hljs-comment">#计算损失以及梯度</span><br>            loss, grads = self.loss(X_batch, y=y_batch)<br>            loss_history.append(loss)<br>            <br>            <span class="hljs-comment">#修改权重</span><br>            <span class="hljs-comment">############################################################################</span><br>            <span class="hljs-comment">#                    任务：修改深层网络的权重                              #</span><br>            <span class="hljs-comment">############################################################################</span><br>            <span class="hljs-keyword">for</span> i,j <span class="hljs-keyword">in</span> self.params.items():<br>                self.params[i] += -learning_rate*grads[i]<br>            <span class="hljs-comment">############################################################################</span><br>            <span class="hljs-comment">#                              结束编码                                    #</span><br>            <span class="hljs-comment">############################################################################</span><br>            <span class="hljs-keyword">if</span> verbose <span class="hljs-keyword">and</span> it % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;iteration %d / %d: loss %f&#x27;</span> % (it, num_iters, loss))<br><br>            <span class="hljs-keyword">if</span> it % iterations_per_epoch == <span class="hljs-number">0</span>:<br>                <span class="hljs-comment"># 检验精度</span><br>                train_acc = (self.predict(X) == y).mean()<br>                val_acc = (self.predict(X_val) == y_val).mean()<br>                <span class="hljs-comment"># 记录数据</span><br>                train_acc_history.append(train_acc)<br>                val_acc_history.append(val_acc)<br>                <span class="hljs-keyword">if</span> (best_val &lt; val_acc):<br>                    best_val = val_acc<br>                <span class="hljs-comment"># 学习率衰减</span><br>                learning_rate *= learning_rate_decay<br>        <span class="hljs-comment"># 以字典的形式返回结果</span><br>        <span class="hljs-keyword">return</span> &#123;<br>          <span class="hljs-string">&#x27;loss_history&#x27;</span>: loss_history,<br>          <span class="hljs-string">&#x27;train_acc_history&#x27;</span>: train_acc_history,<br>          <span class="hljs-string">&#x27;val_acc_history&#x27;</span>: val_acc_history,<br>          <span class="hljs-string">&#x27;best_val_acc&#x27;</span>:best_val<br>        &#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Inputs:</span><br><span class="hljs-string">        - X: 输入数据</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        - y_pred: 预测类别</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        y_pred = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                   任务： 执行深层网络的前向传播，                       #</span><br>        <span class="hljs-comment">#                  然后使用输出层得分函数预测数据类标                     #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        outs = &#123;&#125;<br>        outs[<span class="hljs-number">0</span>] = X<br>        num_h = self.num_layers-<span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_h):<br>            outs[i+<span class="hljs-number">1</span>], _ =affine_relu_forward(outs[i], self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)],<br>                                              self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>)])<br>        scores, _ = affine_forward(outs[num_h], self.params[<span class="hljs-string">&#x27;W&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)], <br>                                   self.params[<span class="hljs-string">&#x27;b&#x27;</span>+<span class="hljs-built_in">str</span>(num_h+<span class="hljs-number">1</span>)])<br>        y_pred = np.argmax(scores, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-comment">#                             结束编码                                    #</span><br>        <span class="hljs-comment">###########################################################################</span><br>        <span class="hljs-keyword">return</span> y_pred<br><br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Python/" class="print-no-link">#Python</a> <a href="/tags/DeepLearning/" class="print-no-link">#DeepLearning</a></div></div><div class="license-box my-3"><div class="license-title"><div>编码实现全连接神经网络</div><div>https://fulequn.github.io/2020/11/Article202011303/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Fulequn</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2020年11月30日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2020/11/Article202011304/" title="深度学习实战 第4章深度学习正则化笔记"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">深度学习实战 第4章深度学习正则化笔记</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/11/Article202011302/" title="Matplotlib使用subplot时标题与刻度重叠"><span class="hidden-mobile">Matplotlib使用subplot时标题与刻度重叠</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>